(DEBUG) 2023-04-10 17:07:42,713 [logger.py:40] logger init.
(INFO) 2023-04-10 17:07:42,713 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-17-07-42-713443.log
(INFO) 2023-04-10 17:07:46,500 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,500 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:46,503 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,503 [api2.py:131] throughput: 0.5780393675050868
(INFO) 2023-04-10 17:07:46,504 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,504 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 53}
(INFO) 2023-04-10 17:07:46,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,506 [api2.py:131] throughput: 4.765427137362659
(INFO) 2023-04-10 17:07:46,507 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,507 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:07:46,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,509 [api2.py:131] throughput: 1.883852530007317
(INFO) 2023-04-10 17:07:46,510 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,510 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 186}
(INFO) 2023-04-10 17:07:46,511 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,512 [api2.py:131] throughput: 369.3805063836227
(INFO) 2023-04-10 17:07:46,516 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,516 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:46,518 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,518 [api2.py:131] throughput: 9.910034015676437
(INFO) 2023-04-10 17:07:46,519 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,519 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:07:46,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,521 [api2.py:131] throughput: 7.205550538186605
(INFO) 2023-04-10 17:07:46,521 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,522 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:46,524 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,524 [api2.py:131] throughput: 1.3666077601407345
(INFO) 2023-04-10 17:07:46,524 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,525 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 194, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,526 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,526 [api2.py:131] throughput: 0.6497453262944477
(INFO) 2023-04-10 17:07:46,527 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,527 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:07:46,529 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,529 [api2.py:131] throughput: 20.730738041533314
(INFO) 2023-04-10 17:07:46,529 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,529 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:07:46,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,532 [api2.py:131] throughput: 0.9913859410782495
(INFO) 2023-04-10 17:07:46,532 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,532 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,534 [api2.py:131] throughput: 2.060550567506617
(INFO) 2023-04-10 17:07:46,535 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,535 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:07:46,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,537 [api2.py:131] throughput: 0.35027363973857406
(INFO) 2023-04-10 17:07:46,538 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,538 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 6, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 17:07:46,540 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,540 [api2.py:131] throughput: 1.2879544614984688
(INFO) 2023-04-10 17:07:46,541 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,541 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,543 [api2.py:131] throughput: 6.671451765040811
(INFO) 2023-04-10 17:07:46,543 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,543 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:07:46,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,545 [api2.py:131] throughput: 1.383474852621674
(INFO) 2023-04-10 17:07:46,546 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,546 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,548 [api2.py:131] throughput: 2.031794455896758
(INFO) 2023-04-10 17:07:46,549 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,549 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-10 17:07:46,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,551 [api2.py:131] throughput: 3.6587991596148646
(INFO) 2023-04-10 17:07:46,552 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,552 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 8, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:46,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,554 [api2.py:131] throughput: 1.1691628143667423
(INFO) 2023-04-10 17:07:46,554 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,554 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:07:46,556 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,556 [api2.py:131] throughput: 0.4143719609363691
(INFO) 2023-04-10 17:07:46,557 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,557 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:07:46,559 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,559 [api2.py:131] throughput: 1.7510615564263718
(INFO) 2023-04-10 17:07:46,560 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,560 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 144, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:07:46,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,562 [api2.py:131] throughput: 1.1366978473252871
(INFO) 2023-04-10 17:07:46,563 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,563 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:07:46,565 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,565 [api2.py:131] throughput: 1.8553750704259897
(INFO) 2023-04-10 17:07:46,566 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,566 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:46,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,568 [api2.py:131] throughput: 1.5309916354188493
(INFO) 2023-04-10 17:07:46,568 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:07:46,568 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 94, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 91}
(INFO) 2023-04-10 17:07:46,570 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,571 [api2.py:131] throughput: 39.40848206366899
(INFO) 2023-04-10 17:07:46,576 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:07:46,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,577 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,641 [api1.py:131] throughput: 0.364292566928713
(INFO) 2023-04-10 17:07:46,642 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:07:46,642 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:07:46,644 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,680 [api1.py:131] throughput: 0.7058068121624179
(INFO) 2023-04-10 17:07:46,690 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,690 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 17:07:46,692 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,692 [api2.py:131] throughput: 86.06528782680327
(INFO) 2023-04-10 17:07:46,693 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,693 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 7, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:07:46,696 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,696 [api2.py:131] throughput: 1.239609641568756
(INFO) 2023-04-10 17:07:46,697 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,697 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:07:46,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,699 [api2.py:131] throughput: 1.4966906297516873
(INFO) 2023-04-10 17:07:46,700 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,700 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 373, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:46,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,702 [api2.py:131] throughput: 0.3490276842616269
(INFO) 2023-04-10 17:07:46,703 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,703 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:46,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,705 [api2.py:131] throughput: 6.641499104300865
(INFO) 2023-04-10 17:07:46,706 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,706 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:07:46,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,709 [api2.py:131] throughput: 6.36528031879768
(INFO) 2023-04-10 17:07:46,710 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,710 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:46,712 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,712 [api2.py:131] throughput: 0.47161463919608737
(INFO) 2023-04-10 17:07:46,713 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,713 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:07:46,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,715 [api2.py:131] throughput: 1.0490313093828652
(INFO) 2023-04-10 17:07:46,716 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,716 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 10, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:07:46,718 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,718 [api2.py:131] throughput: 0.3655038474630921
(INFO) 2023-04-10 17:07:46,719 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,719 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 102}
(INFO) 2023-04-10 17:07:46,722 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,722 [api2.py:131] throughput: 18.65859898700227
(INFO) 2023-04-10 17:07:46,723 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,723 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:46,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,725 [api2.py:131] throughput: 3.1180688447649723
(INFO) 2023-04-10 17:07:46,726 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,726 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:07:46,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,728 [api2.py:131] throughput: 0.9568877515244523
(INFO) 2023-04-10 17:07:46,729 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,729 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 9, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,731 [api2.py:131] throughput: 0.3516100816342972
(INFO) 2023-04-10 17:07:46,732 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,732 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:07:46,734 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,734 [api2.py:131] throughput: 0.5008740532719065
(INFO) 2023-04-10 17:07:46,735 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,735 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:07:46,738 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,738 [api2.py:131] throughput: 1.8811984988885444
(INFO) 2023-04-10 17:07:46,739 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,739 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 8, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:07:46,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,742 [api2.py:131] throughput: 0.7335145595480969
(INFO) 2023-04-10 17:07:46,742 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,743 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 12, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:07:46,744 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,745 [api2.py:131] throughput: 0.7659446800332714
(INFO) 2023-04-10 17:07:46,746 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,746 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 17:07:46,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,748 [api2.py:131] throughput: 23.90353900695403
(INFO) 2023-04-10 17:07:46,749 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,749 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 12, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:46,751 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,752 [api2.py:131] throughput: 1.3541442555792371
(INFO) 2023-04-10 17:07:46,752 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:46,753 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:07:46,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:46,755 [api2.py:131] throughput: 1.8681510374144252
(INFO) 2023-04-10 17:07:46,997 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:46,997 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 146, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:07:47,001 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,002 [api2.py:131] throughput: 0.5356557525634431
(INFO) 2023-04-10 17:07:47,002 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,003 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 210, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 79}
(INFO) 2023-04-10 17:07:47,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,006 [api2.py:131] throughput: 1.4193855596891813
(INFO) 2023-04-10 17:07:47,007 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,007 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:07:47,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,010 [api2.py:131] throughput: 1.1122557870765402
(INFO) 2023-04-10 17:07:47,011 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,011 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 146, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 17:07:47,014 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,014 [api2.py:131] throughput: 44.310330947149275
(INFO) 2023-04-10 17:07:47,015 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,015 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:47,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,018 [api2.py:131] throughput: 2.3042622338701624
(INFO) 2023-04-10 17:07:47,019 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,019 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:07:47,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,023 [api2.py:131] throughput: 0.48010609760825873
(INFO) 2023-04-10 17:07:47,023 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,023 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 84, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:07:47,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,027 [api2.py:131] throughput: 0.8914912982479082
(INFO) 2023-04-10 17:07:47,027 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,027 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 17:07:47,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,031 [api2.py:131] throughput: 1.1423939939360575
(INFO) 2023-04-10 17:07:47,031 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,032 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:47,035 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,035 [api2.py:131] throughput: 8.2770785383326
(INFO) 2023-04-10 17:07:47,036 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,036 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 217, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:07:47,039 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,039 [api2.py:131] throughput: 0.49327769704793795
(INFO) 2023-04-10 17:07:47,040 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,040 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:07:47,043 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,043 [api2.py:131] throughput: 1.9070914505279457
(INFO) 2023-04-10 17:07:47,044 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,044 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 138, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 178}
(INFO) 2023-04-10 17:07:47,047 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,047 [api2.py:131] throughput: 158.5169841347605
(INFO) 2023-04-10 17:07:47,048 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,048 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 124, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:47,052 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,052 [api2.py:131] throughput: 0.6730355717713933
(INFO) 2023-04-10 17:07:47,052 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,052 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 214, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 96}
(INFO) 2023-04-10 17:07:47,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,056 [api2.py:131] throughput: 0.6572213130812617
(INFO) 2023-04-10 17:07:47,057 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,057 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 353}
(INFO) 2023-04-10 17:07:47,060 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,060 [api2.py:131] throughput: 149.0510463607593
(INFO) 2023-04-10 17:07:47,061 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,061 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:07:47,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,064 [api2.py:131] throughput: 32.62425823077311
(INFO) 2023-04-10 17:07:47,065 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,065 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:07:47,068 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,069 [api2.py:131] throughput: 0.3779413815359912
(INFO) 2023-04-10 17:07:47,069 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,069 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 405, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:07:47,073 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,073 [api2.py:131] throughput: 0.616315565259459
(INFO) 2023-04-10 17:07:47,074 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,074 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:47,077 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,077 [api2.py:131] throughput: 1.8033306206624273
(INFO) 2023-04-10 17:07:47,078 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:07:47,078 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 419, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 106}
(INFO) 2023-04-10 17:07:47,081 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,081 [api2.py:131] throughput: 0.44594975991101593
(INFO) 2023-04-10 17:07:47,086 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,086 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 16, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 17:07:47,183 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,298 [api1.py:131] throughput: 4.374933296138974
(INFO) 2023-04-10 17:07:47,300 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,300 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 24, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 127}
(INFO) 2023-04-10 17:07:47,302 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,353 [api1.py:131] throughput: 35.08059551917354
(INFO) 2023-04-10 17:07:47,353 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:47,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,467 [api1.py:131] throughput: 0.6397211610015827
(INFO) 2023-04-10 17:07:47,468 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,468 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:07:47,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,554 [api1.py:131] throughput: 5.094891608966227
(INFO) 2023-04-10 17:07:47,554 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,554 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 195}
(INFO) 2023-04-10 17:07:47,557 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,654 [api1.py:131] throughput: 6.340333481845143
(INFO) 2023-04-10 17:07:47,655 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,655 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 12, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:07:47,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,785 [api1.py:131] throughput: 3.3565492688587155
(INFO) 2023-04-10 17:07:47,787 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 27, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 100}
(INFO) 2023-04-10 17:07:47,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:47,928 [api1.py:131] throughput: 18.496325120530933
(INFO) 2023-04-10 17:07:47,930 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:47,930 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:07:47,933 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,027 [api1.py:131] throughput: 0.7786213151166375
(INFO) 2023-04-10 17:07:48,028 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,029 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 17, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:07:48,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,109 [api1.py:131] throughput: 2.816737848670764
(INFO) 2023-04-10 17:07:48,110 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,110 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:07:48,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,220 [api1.py:131] throughput: 7.4404242256085835
(INFO) 2023-04-10 17:07:48,221 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,221 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:07:48,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,325 [api1.py:131] throughput: 7.416616706009753
(INFO) 2023-04-10 17:07:48,326 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,326 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 15, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:07:48,328 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,396 [api1.py:131] throughput: 8.643274232698515
(INFO) 2023-04-10 17:07:48,397 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 22, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 59}
(INFO) 2023-04-10 17:07:48,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,472 [api1.py:131] throughput: 6.363618533215201
(INFO) 2023-04-10 17:07:48,473 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,473 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 24, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:07:48,475 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,669 [api1.py:131] throughput: 2.3871938242493664
(INFO) 2023-04-10 17:07:48,670 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,670 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 16, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:07:48,672 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,786 [api1.py:131] throughput: 1.5108249520016956
(INFO) 2023-04-10 17:07:48,787 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 14, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:07:48,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,878 [api1.py:131] throughput: 4.136245956405953
(INFO) 2023-04-10 17:07:48,879 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,879 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 17:07:48,881 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:48,973 [api1.py:131] throughput: 0.5169765821377975
(INFO) 2023-04-10 17:07:48,974 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:48,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 31, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:07:48,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:49,077 [api1.py:131] throughput: 3.718638597701006
(INFO) 2023-04-10 17:07:49,078 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:49,078 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 29, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:07:49,080 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:49,170 [api1.py:131] throughput: 2.726520802408529
(INFO) 2023-04-10 17:07:49,171 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-10 17:07:49,171 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 13, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 203}
(INFO) 2023-04-10 17:07:49,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:49,271 [api1.py:131] throughput: 5.747702436343402
(INFO) 2023-04-10 17:07:49,277 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:07:49,277 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:49,278 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:49,323 [api1.py:131] throughput: 0.1940641453365133
(INFO) 2023-04-10 17:07:49,323 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:07:49,324 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:49,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:49,368 [api1.py:131] throughput: 0.05037259832126281
(INFO) 2023-04-10 17:07:52,209 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:07:52,209 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:07:52,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:52,211 [api2.py:131] throughput: 1.6970889769479198
(INFO) 2023-04-10 17:07:54,242 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,242 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 131, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:54,245 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,245 [api2.py:131] throughput: 1.111665238380043
(INFO) 2023-04-10 17:07:54,246 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,246 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:54,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,250 [api2.py:131] throughput: 3.7881027414015644
(INFO) 2023-04-10 17:07:54,250 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,251 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 185, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:07:54,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,254 [api2.py:131] throughput: 0.8029940425509341
(INFO) 2023-04-10 17:07:54,254 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,255 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 111, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 104}
(INFO) 2023-04-10 17:07:54,258 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,258 [api2.py:131] throughput: 121.10351280626315
(INFO) 2023-04-10 17:07:54,259 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,259 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:07:54,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,262 [api2.py:131] throughput: 10.723163742749094
(INFO) 2023-04-10 17:07:54,263 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,263 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:07:54,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,267 [api2.py:131] throughput: 1.8540596048919078
(INFO) 2023-04-10 17:07:54,267 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,267 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 486, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:54,270 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,271 [api2.py:131] throughput: 0.35055664465317854
(INFO) 2023-04-10 17:07:54,271 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,271 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:07:54,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,275 [api2.py:131] throughput: 0.6427430599600931
(INFO) 2023-04-10 17:07:54,276 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,276 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:54,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,279 [api2.py:131] throughput: 2.115099452533007
(INFO) 2023-04-10 17:07:54,280 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,280 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 468, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:07:54,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,284 [api2.py:131] throughput: 0.29177801427714833
(INFO) 2023-04-10 17:07:54,284 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,284 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:54,288 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,288 [api2.py:131] throughput: 1.0442840872522223
(INFO) 2023-04-10 17:07:54,289 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,289 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:07:54,292 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,292 [api2.py:131] throughput: 1.4734432737371588
(INFO) 2023-04-10 17:07:54,293 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,293 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:07:54,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,297 [api2.py:131] throughput: 22.328591615830085
(INFO) 2023-04-10 17:07:54,298 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,298 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:07:54,301 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,301 [api2.py:131] throughput: 2.387153414352021
(INFO) 2023-04-10 17:07:54,302 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,302 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:07:54,305 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,306 [api2.py:131] throughput: 0.7475365362677951
(INFO) 2023-04-10 17:07:54,306 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,307 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 342, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:54,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,310 [api2.py:131] throughput: 2.450448900484906
(INFO) 2023-04-10 17:07:54,311 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,311 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 130, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:07:54,314 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,315 [api2.py:131] throughput: 36.04969160194323
(INFO) 2023-04-10 17:07:54,315 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,315 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 498, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:07:54,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,319 [api2.py:131] throughput: 0.19612984773137415
(INFO) 2023-04-10 17:07:54,320 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,320 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:07:54,323 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,324 [api2.py:131] throughput: 0.8089660471098692
(INFO) 2023-04-10 17:07:54,324 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 17:07:54,324 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:07:54,328 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:54,328 [api2.py:131] throughput: 1.2752216530712175
(INFO) 2023-04-10 17:07:57,332 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,332 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 120}
(INFO) 2023-04-10 17:07:57,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,334 [api2.py:131] throughput: 327.53723443281035
(INFO) 2023-04-10 17:07:57,335 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,335 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:57,337 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,337 [api2.py:131] throughput: 0.40766528668379676
(INFO) 2023-04-10 17:07:57,338 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,338 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 149, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:07:57,340 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,341 [api2.py:131] throughput: 1.018902848673038
(INFO) 2023-04-10 17:07:57,342 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,342 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 498, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:07:57,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,344 [api2.py:131] throughput: 0.3382955436101479
(INFO) 2023-04-10 17:07:57,345 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,345 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 186, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 17:07:57,347 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,348 [api2.py:131] throughput: 0.6673177471662665
(INFO) 2023-04-10 17:07:57,348 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,348 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:07:57,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,350 [api2.py:131] throughput: 1.7836356367432253
(INFO) 2023-04-10 17:07:57,350 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,350 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 53}
(INFO) 2023-04-10 17:07:57,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,352 [api2.py:131] throughput: 41.6323710191441
(INFO) 2023-04-10 17:07:57,352 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,352 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 162, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:07:57,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,354 [api2.py:131] throughput: 0.6169122521723178
(INFO) 2023-04-10 17:07:57,354 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,354 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 207, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:07:57,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,357 [api2.py:131] throughput: 0.801511838560691
(INFO) 2023-04-10 17:07:57,357 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,357 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:57,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,359 [api2.py:131] throughput: 1.8146251001963396
(INFO) 2023-04-10 17:07:57,359 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,360 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 17:07:57,361 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,361 [api2.py:131] throughput: 260.4185815091778
(INFO) 2023-04-10 17:07:57,362 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,362 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:57,363 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,363 [api2.py:131] throughput: 1.894448828451627
(INFO) 2023-04-10 17:07:57,364 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:57,364 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:07:57,366 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:57,366 [api2.py:131] throughput: 1.2481769128011626
(INFO) 2023-04-10 17:07:59,049 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,050 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 72}
(INFO) 2023-04-10 17:07:59,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,054 [api2.py:131] throughput: 75.94207752806057
(INFO) 2023-04-10 17:07:59,055 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,055 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 207, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 69}
(INFO) 2023-04-10 17:07:59,060 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,060 [api2.py:131] throughput: 0.3457531860851835
(INFO) 2023-04-10 17:07:59,060 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,061 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 149, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:07:59,065 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,065 [api2.py:131] throughput: 1.0237130906715026
(INFO) 2023-04-10 17:07:59,066 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,066 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 498, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 17:07:59,070 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,071 [api2.py:131] throughput: 0.33848607500317884
(INFO) 2023-04-10 17:07:59,071 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,071 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 255}
(INFO) 2023-04-10 17:07:59,076 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,076 [api2.py:131] throughput: 0.7180965989853199
(INFO) 2023-04-10 17:07:59,077 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,077 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:07:59,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,082 [api2.py:131] throughput: 1.8569731339822855
(INFO) 2023-04-10 17:07:59,082 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,083 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 173}
(INFO) 2023-04-10 17:07:59,087 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,087 [api2.py:131] throughput: 45.747200543437586
(INFO) 2023-04-10 17:07:59,088 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,088 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 83}
(INFO) 2023-04-10 17:07:59,092 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,092 [api2.py:131] throughput: 0.6325755931177433
(INFO) 2023-04-10 17:07:59,093 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,093 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 207, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 166}
(INFO) 2023-04-10 17:07:59,211 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,212 [api2.py:131] throughput: 0.815819691259385
(INFO) 2023-04-10 17:07:59,213 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,213 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:07:59,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,218 [api2.py:131] throughput: 1.9130814182824247
(INFO) 2023-04-10 17:07:59,219 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,219 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 166}
(INFO) 2023-04-10 17:07:59,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,223 [api2.py:131] throughput: 195.837197063033
(INFO) 2023-04-10 17:07:59,224 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,224 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:07:59,229 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,229 [api2.py:131] throughput: 2.081575246774623
(INFO) 2023-04-10 17:07:59,230 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,230 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:07:59,234 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,235 [api2.py:131] throughput: 1.6406352047509738
(INFO) 2023-04-10 17:07:59,235 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,235 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:07:59,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,240 [api2.py:131] throughput: 3.063893772630515
(INFO) 2023-04-10 17:07:59,241 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,241 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 276, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:07:59,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,246 [api2.py:131] throughput: 0.7783437372652453
(INFO) 2023-04-10 17:07:59,246 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,247 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 115, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 253}
(INFO) 2023-04-10 17:07:59,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,252 [api2.py:131] throughput: 0.6518817754951374
(INFO) 2023-04-10 17:07:59,252 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,252 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:59,257 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,257 [api2.py:131] throughput: 1.1472246414895462
(INFO) 2023-04-10 17:07:59,258 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,258 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 57}
(INFO) 2023-04-10 17:07:59,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,263 [api2.py:131] throughput: 1.1330643285052318
(INFO) 2023-04-10 17:07:59,264 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,264 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 186, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:07:59,269 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,270 [api2.py:131] throughput: 0.8812050994681274
(INFO) 2023-04-10 17:07:59,270 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:07:59,270 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:07:59,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:07:59,275 [api2.py:131] throughput: 0.41383290916737114
(INFO) 2023-04-10 17:08:08,005 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,005 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 24, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:08:08,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,115 [api1.py:131] throughput: 4.086232182720089
(INFO) 2023-04-10 17:08:08,116 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,116 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 30, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:08:08,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,204 [api1.py:131] throughput: 1.9504975025461155
(INFO) 2023-04-10 17:08:08,205 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 16, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:08,208 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,304 [api1.py:131] throughput: 0.7963395267405827
(INFO) 2023-04-10 17:08:08,305 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 23, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:08,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,400 [api1.py:131] throughput: 0.8252987238055907
(INFO) 2023-04-10 17:08:08,401 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 31, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:08,403 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,504 [api1.py:131] throughput: 0.6009436026063222
(INFO) 2023-04-10 17:08:08,505 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,505 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 12, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:08,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,597 [api1.py:131] throughput: 2.903557042098104
(INFO) 2023-04-10 17:08:08,598 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,598 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 105}
(INFO) 2023-04-10 17:08:08,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,703 [api1.py:131] throughput: 3.54653906641538
(INFO) 2023-04-10 17:08:08,704 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 13, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:08,707 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,806 [api1.py:131] throughput: 0.7232019540878228
(INFO) 2023-04-10 17:08:08,807 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,807 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 15, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 197}
(INFO) 2023-04-10 17:08:08,809 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:08,904 [api1.py:131] throughput: 5.3089843219514785
(INFO) 2023-04-10 17:08:08,905 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:08,905 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 26, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:08,907 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,013 [api1.py:131] throughput: 2.511338552931527
(INFO) 2023-04-10 17:08:09,014 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,015 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 22, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:09,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,260 [api1.py:131] throughput: 2.4117050968551057
(INFO) 2023-04-10 17:08:09,261 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,261 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 25, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 17:08:09,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,351 [api1.py:131] throughput: 7.574103747927443
(INFO) 2023-04-10 17:08:09,352 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 26, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:09,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,458 [api1.py:131] throughput: 0.8972443086286133
(INFO) 2023-04-10 17:08:09,460 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,460 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 21, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:09,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,570 [api1.py:131] throughput: 2.7292296742694213
(INFO) 2023-04-10 17:08:09,571 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,571 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:09,573 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,664 [api1.py:131] throughput: 0.9702617587475161
(INFO) 2023-04-10 17:08:09,665 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,665 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 15, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:09,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,768 [api1.py:131] throughput: 0.08497567427884828
(INFO) 2023-04-10 17:08:09,769 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,769 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 24, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 17:08:09,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,864 [api1.py:131] throughput: 0.3451525565410128
(INFO) 2023-04-10 17:08:09,865 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,865 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 31, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:08:09,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:09,955 [api1.py:131] throughput: 2.965889142877969
(INFO) 2023-04-10 17:08:09,957 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:09,957 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 21, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 17:08:09,959 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,047 [api1.py:131] throughput: 2.9236534358515045
(INFO) 2023-04-10 17:08:10,048 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:08:10,048 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 27, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:08:10,050 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,132 [api1.py:131] throughput: 2.821393257258339
(INFO) 2023-04-10 17:08:10,755 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,755 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 115, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:08:10,757 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,757 [api2.py:131] throughput: 0.42768170789551024
(INFO) 2023-04-10 17:08:10,758 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,758 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:08:10,759 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,759 [api2.py:131] throughput: 0.5187392270537822
(INFO) 2023-04-10 17:08:10,760 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,760 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:08:10,762 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,762 [api2.py:131] throughput: 0.875640292062655
(INFO) 2023-04-10 17:08:10,762 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,763 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:10,767 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,768 [api2.py:131] throughput: 0.7471860673827938
(INFO) 2023-04-10 17:08:10,769 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,769 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 13, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:08:10,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,771 [api2.py:131] throughput: 2.3257194337082807
(INFO) 2023-04-10 17:08:10,772 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,772 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 12, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:08:10,773 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,773 [api2.py:131] throughput: 0.4292366996454744
(INFO) 2023-04-10 17:08:10,774 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,774 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 13, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:10,775 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,776 [api2.py:131] throughput: 2.444677342879023
(INFO) 2023-04-10 17:08:10,776 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,776 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 7, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:10,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,778 [api2.py:131] throughput: 3.4995760753525365
(INFO) 2023-04-10 17:08:10,779 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:10,779 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 473, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:08:10,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:10,781 [api2.py:131] throughput: 0.46116066959120805
(INFO) 2023-04-10 17:08:14,476 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,476 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:08:14,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,478 [api2.py:131] throughput: 0.4446712509725565
(INFO) 2023-04-10 17:08:14,479 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,479 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 12, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:08:14,480 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,480 [api2.py:131] throughput: 0.5212191037023922
(INFO) 2023-04-10 17:08:14,481 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,481 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:08:14,482 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,483 [api2.py:131] throughput: 0.914898179708042
(INFO) 2023-04-10 17:08:14,483 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,483 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:14,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,485 [api2.py:131] throughput: 0.7695708087100227
(INFO) 2023-04-10 17:08:14,485 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,485 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 21, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:14,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,487 [api2.py:131] throughput: 1.9502939532973935
(INFO) 2023-04-10 17:08:14,488 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,488 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 19, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:14,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,490 [api2.py:131] throughput: 0.42327519308097983
(INFO) 2023-04-10 17:08:14,491 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,491 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 21, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:14,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,492 [api2.py:131] throughput: 2.675556947358875
(INFO) 2023-04-10 17:08:14,493 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,493 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 12, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:08:14,494 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,494 [api2.py:131] throughput: 10.68047468604578
(INFO) 2023-04-10 17:08:14,495 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:14,495 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 473, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:14,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:14,496 [api2.py:131] throughput: 0.3881944717112241
(INFO) 2023-04-10 17:08:15,520 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,520 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 12, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:15,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,562 [api1.py:131] throughput: 0.5887642121793178
(INFO) 2023-04-10 17:08:15,563 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,563 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:15,565 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,609 [api1.py:131] throughput: 1.7690886934456316
(INFO) 2023-04-10 17:08:15,610 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,610 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 11, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 114}
(INFO) 2023-04-10 17:08:15,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,650 [api1.py:131] throughput: 16.365153696837922
(INFO) 2023-04-10 17:08:15,650 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,651 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 98, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:08:15,652 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,697 [api1.py:131] throughput: 0.9224221974511276
(INFO) 2023-04-10 17:08:15,698 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,698 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 10, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:15,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,746 [api1.py:131] throughput: 0.9236770801281736
(INFO) 2023-04-10 17:08:15,747 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,747 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 12, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:15,748 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,799 [api1.py:131] throughput: 1.451171728295608
(INFO) 2023-04-10 17:08:15,800 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,801 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 8, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:08:15,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,856 [api1.py:131] throughput: 0.9932900633932022
(INFO) 2023-04-10 17:08:15,856 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,856 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:15,858 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,898 [api1.py:131] throughput: 0.46952256886810545
(INFO) 2023-04-10 17:08:15,899 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,899 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 10, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:15,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:15,952 [api1.py:131] throughput: 0.5790591012270818
(INFO) 2023-04-10 17:08:15,953 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:15,953 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 11, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:15,954 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:16,009 [api1.py:131] throughput: 0.33275273571197567
(INFO) 2023-04-10 17:08:16,010 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:16,010 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 9, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:08:16,011 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:16,054 [api1.py:131] throughput: 0.40025497751600825
(INFO) 2023-04-10 17:08:16,055 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:16,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 10, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:16,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:16,105 [api1.py:131] throughput: 0.37334510451619984
(INFO) 2023-04-10 17:08:16,106 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:16,106 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 13, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:16,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:16,159 [api1.py:131] throughput: 1.3003257805002648
(INFO) 2023-04-10 17:08:18,863 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,863 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 14, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:08:18,872 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,872 [api2.py:131] throughput: 1.045082408207881
(INFO) 2023-04-10 17:08:18,873 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,873 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 166}
(INFO) 2023-04-10 17:08:18,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,882 [api2.py:131] throughput: 3.149098863062441
(INFO) 2023-04-10 17:08:18,883 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,883 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 13, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 73}
(INFO) 2023-04-10 17:08:18,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,892 [api2.py:131] throughput: 6.5217849868526
(INFO) 2023-04-10 17:08:18,893 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,893 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 189}
(INFO) 2023-04-10 17:08:18,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,902 [api2.py:131] throughput: 1.526566187598197
(INFO) 2023-04-10 17:08:18,903 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,903 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 11, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 107}
(INFO) 2023-04-10 17:08:18,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,912 [api2.py:131] throughput: 1.3599624670896813
(INFO) 2023-04-10 17:08:18,913 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,913 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 14, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 453}
(INFO) 2023-04-10 17:08:18,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,922 [api2.py:131] throughput: 3.0482856162553853
(INFO) 2023-04-10 17:08:18,923 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,923 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:08:18,932 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,932 [api2.py:131] throughput: 0.7488620141345708
(INFO) 2023-04-10 17:08:18,933 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,933 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 17:08:18,941 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,942 [api2.py:131] throughput: 0.8124199918265796
(INFO) 2023-04-10 17:08:18,942 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,942 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 12, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 102}
(INFO) 2023-04-10 17:08:18,951 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,951 [api2.py:131] throughput: 2.989322995043574
(INFO) 2023-04-10 17:08:18,952 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,952 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 13, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:08:18,960 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,960 [api2.py:131] throughput: 10.205405474216441
(INFO) 2023-04-10 17:08:18,961 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,961 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 11, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 17:08:18,970 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,971 [api2.py:131] throughput: 0.3832604798343902
(INFO) 2023-04-10 17:08:18,971 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,971 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 11, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 102}
(INFO) 2023-04-10 17:08:18,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,981 [api2.py:131] throughput: 0.4122383432905252
(INFO) 2023-04-10 17:08:18,982 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,982 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 15, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 17:08:18,991 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:18,991 [api2.py:131] throughput: 1.051065531354272
(INFO) 2023-04-10 17:08:18,992 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:18,992 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 13, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:19,002 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,002 [api2.py:131] throughput: 2.1127694058847144
(INFO) 2023-04-10 17:08:19,003 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:19,003 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 104, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 77}
(INFO) 2023-04-10 17:08:19,158 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,158 [api2.py:131] throughput: 0.9644516838715045
(INFO) 2023-04-10 17:08:19,159 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:19,159 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 47}
(INFO) 2023-04-10 17:08:19,168 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,168 [api2.py:131] throughput: 0.36599881845447424
(INFO) 2023-04-10 17:08:19,169 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:19,169 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 75}
(INFO) 2023-04-10 17:08:19,178 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,178 [api2.py:131] throughput: 4.2204161025372295
(INFO) 2023-04-10 17:08:19,179 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:19,179 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 12, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:08:19,188 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,188 [api2.py:131] throughput: 0.7036370354554762
(INFO) 2023-04-10 17:08:19,189 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:19,189 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 102, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:08:19,198 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,198 [api2.py:131] throughput: 0.6943138957641289
(INFO) 2023-04-10 17:08:19,199 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:08:19,199 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 13, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 94}
(INFO) 2023-04-10 17:08:19,208 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:19,208 [api2.py:131] throughput: 4.241365580769294
(INFO) 2023-04-10 17:08:23,867 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:23,867 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 9, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 207}
(INFO) 2023-04-10 17:08:23,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:23,876 [api2.py:131] throughput: 3.37062019003103
(INFO) 2023-04-10 17:08:23,876 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:23,877 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 14, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:23,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:23,885 [api2.py:131] throughput: 2.897719936453048
(INFO) 2023-04-10 17:08:23,886 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:23,886 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 10, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:23,895 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:23,895 [api2.py:131] throughput: 0.5841542373385676
(INFO) 2023-04-10 17:08:23,895 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:23,895 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 367}
(INFO) 2023-04-10 17:08:24,025 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,026 [api2.py:131] throughput: 0.6952610357352162
(INFO) 2023-04-10 17:08:24,027 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,027 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 12, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 174}
(INFO) 2023-04-10 17:08:24,035 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,035 [api2.py:131] throughput: 1.8742807241088708
(INFO) 2023-04-10 17:08:24,036 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,036 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 74, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:08:24,044 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,044 [api2.py:131] throughput: 1.069448007448567
(INFO) 2023-04-10 17:08:24,045 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,045 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 192}
(INFO) 2023-04-10 17:08:24,053 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,053 [api2.py:131] throughput: 2.6096366639437525
(INFO) 2023-04-10 17:08:24,054 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,054 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 16, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:08:24,063 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,063 [api2.py:131] throughput: 1.2825165283108357
(INFO) 2023-04-10 17:08:24,063 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,064 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 264}
(INFO) 2023-04-10 17:08:24,072 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,072 [api2.py:131] throughput: 0.5073192275853136
(INFO) 2023-04-10 17:08:24,073 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,073 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 71}
(INFO) 2023-04-10 17:08:24,081 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,082 [api2.py:131] throughput: 1.6210274658020516
(INFO) 2023-04-10 17:08:24,082 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,082 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 13, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:08:24,091 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,091 [api2.py:131] throughput: 0.33098413949786254
(INFO) 2023-04-10 17:08:24,092 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,092 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 15, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 534}
(INFO) 2023-04-10 17:08:24,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,102 [api2.py:131] throughput: 0.6869622566122945
(INFO) 2023-04-10 17:08:24,103 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,103 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:24,113 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,113 [api2.py:131] throughput: 0.8664278264788772
(INFO) 2023-04-10 17:08:24,114 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,114 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 14, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 132}
(INFO) 2023-04-10 17:08:24,124 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,124 [api2.py:131] throughput: 1.8265288632820786
(INFO) 2023-04-10 17:08:24,125 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,125 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 130}
(INFO) 2023-04-10 17:08:24,135 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,135 [api2.py:131] throughput: 1.612977380546088
(INFO) 2023-04-10 17:08:24,136 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,136 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 15, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 230}
(INFO) 2023-04-10 17:08:24,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,147 [api2.py:131] throughput: 3.200676597070959
(INFO) 2023-04-10 17:08:24,148 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,148 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:08:24,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,301 [api2.py:131] throughput: 0.9301775994150007
(INFO) 2023-04-10 17:08:24,302 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,302 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 8, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:08:24,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,312 [api2.py:131] throughput: 2.50245690360811
(INFO) 2023-04-10 17:08:24,313 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,313 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 13, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 303}
(INFO) 2023-04-10 17:08:24,322 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,322 [api2.py:131] throughput: 1.8395634408615047
(INFO) 2023-04-10 17:08:24,323 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:08:24,323 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 10, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 157}
(INFO) 2023-04-10 17:08:24,333 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:24,333 [api2.py:131] throughput: 1.2768178622344255
(INFO) 2023-04-10 17:08:25,789 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:08:25,790 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 10, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:25,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:25,791 [api2.py:131] throughput: 0.7716846128457915
(INFO) 2023-04-10 17:08:25,792 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:08:25,792 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:25,793 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:25,794 [api2.py:131] throughput: 1.5890394943137975
(INFO) 2023-04-10 17:08:25,794 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:08:25,794 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 84, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:25,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:25,796 [api2.py:131] throughput: 1.390027923261032
(INFO) 2023-04-10 17:08:25,796 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:08:25,797 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 7, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:25,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:25,798 [api2.py:131] throughput: 1.4233677204068043
(INFO) 2023-04-10 17:08:29,016 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:29,016 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:08:29,019 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:29,222 [api1.py:131] throughput: 0.285600067592401
(INFO) 2023-04-10 17:08:29,223 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:29,223 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:29,227 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:29,417 [api1.py:131] throughput: 0.31521816553452736
(INFO) 2023-04-10 17:08:29,418 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:29,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 84, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:29,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:29,753 [api1.py:131] throughput: 0.0848918419143894
(INFO) 2023-04-10 17:08:29,755 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:29,755 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:08:29,758 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:29,874 [api1.py:131] throughput: 3.409714987052595
(INFO) 2023-04-10 17:08:29,875 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:29,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:29,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,031 [api1.py:131] throughput: 0.3648799464856916
(INFO) 2023-04-10 17:08:30,033 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:30,033 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 7, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:30,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,166 [api1.py:131] throughput: 0.7590931010462361
(INFO) 2023-04-10 17:08:30,167 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:30,167 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 172, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:30,171 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,348 [api1.py:131] throughput: 0.11163625313888102
(INFO) 2023-04-10 17:08:30,349 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:30,349 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 376, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:30,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,558 [api1.py:131] throughput: 0.0771870609118978
(INFO) 2023-04-10 17:08:30,559 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:30,560 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:30,570 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,647 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:30,647 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 62}
(INFO) 2023-04-10 17:08:30,650 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,749 [api1.py:131] throughput: 0.6872749342196833
(INFO) 2023-04-10 17:08:30,750 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:30,751 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 509, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:30,753 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:30,777 [api1.py:131] throughput: 0.16782610995510297
(INFO) 2023-04-10 17:08:30,778 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:30,778 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:30,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,023 [api1.py:131] throughput: 0.1540200265897414
(INFO) 2023-04-10 17:08:31,024 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,024 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 467, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:31,028 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,133 [api1.py:131] throughput: 0.05379380121061155
(INFO) 2023-04-10 17:08:31,134 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:31,134 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 112, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:31,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,161 [api1.py:131] throughput: 0.12991625906413887
(INFO) 2023-04-10 17:08:31,162 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,162 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 257, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:08:31,165 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,276 [api1.py:131] throughput: 0.39530386397041734
(INFO) 2023-04-10 17:08:31,278 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 224, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:31,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,322 [api1.py:131] throughput: 0.4105635448494422
(INFO) 2023-04-10 17:08:31,323 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:31,323 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:31,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,409 [api1.py:131] throughput: 0.11999423229003402
(INFO) 2023-04-10 17:08:31,411 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,411 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 502, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:31,413 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,529 [api1.py:131] throughput: 0.09061439557630561
(INFO) 2023-04-10 17:08:31,530 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:31,530 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:31,533 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,550 [api1.py:131] throughput: 0.1532868487163087
(INFO) 2023-04-10 17:08:31,551 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 326, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:31,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,688 [api1.py:131] throughput: 0.07649023898587748
(INFO) 2023-04-10 17:08:31,690 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,690 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 483, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:08:31,692 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,748 [api1.py:131] throughput: 0.25212589344016595
(INFO) 2023-04-10 17:08:31,750 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:31,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:31,753 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,857 [api1.py:131] throughput: 0.23810043523641294
(INFO) 2023-04-10 17:08:31,858 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,858 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:31,860 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,954 [api1.py:131] throughput: 0.3483938000437391
(INFO) 2023-04-10 17:08:31,955 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:31,955 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:08:31,958 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:31,991 [api1.py:131] throughput: 0.219590031300089
(INFO) 2023-04-10 17:08:31,992 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:31,992 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 198, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:31,995 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,139 [api1.py:131] throughput: 0.18969374103941397
(INFO) 2023-04-10 17:08:32,141 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,141 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 143, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:32,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,162 [api1.py:131] throughput: 0.19335854501325028
(INFO) 2023-04-10 17:08:32,163 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:32,163 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 471, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 17:08:32,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,306 [api1.py:131] throughput: 0.30981598235345265
(INFO) 2023-04-10 17:08:32,307 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 214}
(INFO) 2023-04-10 17:08:32,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,339 [api1.py:131] throughput: 0.6750537302845856
(INFO) 2023-04-10 17:08:32,340 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:32,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 175}
(INFO) 2023-04-10 17:08:32,344 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,433 [api1.py:131] throughput: 4.253887446163641
(INFO) 2023-04-10 17:08:32,434 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,434 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 376, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:08:32,437 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,549 [api1.py:131] throughput: 0.20884842850477647
(INFO) 2023-04-10 17:08:32,550 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,550 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 452, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 79}
(INFO) 2023-04-10 17:08:32,553 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,649 [api1.py:131] throughput: 9.429240194180364
(INFO) 2023-04-10 17:08:32,651 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:32,651 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 60, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 198}
(INFO) 2023-04-10 17:08:32,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,655 [api1.py:131] throughput: 1.2019395496977114
(INFO) 2023-04-10 17:08:32,656 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,656 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 318, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:08:32,659 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,758 [api1.py:131] throughput: 8.811437195413108
(INFO) 2023-04-10 17:08:32,759 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:32,759 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:08:32,762 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,767 [api1.py:131] throughput: 0.31947634096232624
(INFO) 2023-04-10 17:08:32,768 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,768 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 425, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:32,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,885 [api1.py:131] throughput: 0.23715912288262514
(INFO) 2023-04-10 17:08:32,886 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:32,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 351, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:08:32,888 [api1.py:131] throughput: 3.2080196465252078
(INFO) 2023-04-10 17:08:32,888 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:32,889 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-10 17:08:32,889 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 156, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:32,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:33,053 [api1.py:131] throughput: 0.09170075009412769
(INFO) 2023-04-10 17:08:33,181 [api1.py:131] throughput: 0.3460151552733542
(INFO) 2023-04-10 17:08:33,182 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:33,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 98, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:33,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:33,334 [api1.py:131] throughput: 0.48317545045929283
(INFO) 2023-04-10 17:08:33,335 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:33,336 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 121, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:33,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:33,476 [api1.py:131] throughput: 0.09010256955897611
(INFO) 2023-04-10 17:08:33,478 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:08:33,478 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 486, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:33,480 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:33,612 [api1.py:131] throughput: 0.06542314243144255
(INFO) 2023-04-10 17:08:37,399 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,399 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 11, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 99}
(INFO) 2023-04-10 17:08:37,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,401 [api2.py:131] throughput: 10.236139295996034
(INFO) 2023-04-10 17:08:37,402 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,402 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:08:37,404 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,405 [api2.py:131] throughput: 0.9859653498785936
(INFO) 2023-04-10 17:08:37,405 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,405 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:08:37,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,408 [api2.py:131] throughput: 1.9920837271163283
(INFO) 2023-04-10 17:08:37,408 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,409 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 13, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:37,411 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,411 [api2.py:131] throughput: 0.3383852543162716
(INFO) 2023-04-10 17:08:37,412 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,412 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 9, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:37,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,414 [api2.py:131] throughput: 1.0442356679179148
(INFO) 2023-04-10 17:08:37,415 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,415 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 12, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:37,417 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,417 [api2.py:131] throughput: 1.4349409129072288
(INFO) 2023-04-10 17:08:37,418 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,418 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 14, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 172}
(INFO) 2023-04-10 17:08:37,420 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,421 [api2.py:131] throughput: 87.66473210965286
(INFO) 2023-04-10 17:08:37,421 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,421 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 89}
(INFO) 2023-04-10 17:08:37,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,424 [api2.py:131] throughput: 2.935999390368605
(INFO) 2023-04-10 17:08:37,424 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,425 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:08:37,427 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,427 [api2.py:131] throughput: 1.0853322996506884
(INFO) 2023-04-10 17:08:37,428 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,428 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:37,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,431 [api2.py:131] throughput: 2.227939000038645
(INFO) 2023-04-10 17:08:37,431 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,431 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 13, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:37,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,434 [api2.py:131] throughput: 0.9539887319739109
(INFO) 2023-04-10 17:08:37,435 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,435 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 12, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 232}
(INFO) 2023-04-10 17:08:37,437 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,437 [api2.py:131] throughput: 44.30490304298838
(INFO) 2023-04-10 17:08:37,438 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,438 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 11, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:08:37,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,440 [api2.py:131] throughput: 0.7282324773865991
(INFO) 2023-04-10 17:08:37,441 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,441 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 11, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 95}
(INFO) 2023-04-10 17:08:37,444 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,444 [api2.py:131] throughput: 1.371797575149487
(INFO) 2023-04-10 17:08:37,444 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,444 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 12, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:37,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,447 [api2.py:131] throughput: 0.3818898698406928
(INFO) 2023-04-10 17:08:37,448 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,448 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 97, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:08:37,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,450 [api2.py:131] throughput: 0.8174839870824624
(INFO) 2023-04-10 17:08:37,451 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,451 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 13, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 83}
(INFO) 2023-04-10 17:08:37,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,454 [api2.py:131] throughput: 55.70176149347759
(INFO) 2023-04-10 17:08:37,454 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,454 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 10, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:08:37,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,457 [api2.py:131] throughput: 1.9942249723002148
(INFO) 2023-04-10 17:08:37,458 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,458 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 13, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 79}
(INFO) 2023-04-10 17:08:37,460 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,460 [api2.py:131] throughput: 6.074677430217006
(INFO) 2023-04-10 17:08:37,461 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:08:37,461 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:37,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:37,463 [api2.py:131] throughput: 1.6536214340040198
(INFO) 2023-04-10 17:08:43,509 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,509 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:43,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,511 [api2.py:131] throughput: 3.552631306975594
(INFO) 2023-04-10 17:08:43,511 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,511 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 7, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:08:43,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,513 [api2.py:131] throughput: 1.037973985513607
(INFO) 2023-04-10 17:08:43,513 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,513 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 24, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 17:08:43,514 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,515 [api2.py:131] throughput: 37.80270758662676
(INFO) 2023-04-10 17:08:43,515 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,515 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 21, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:43,516 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,517 [api2.py:131] throughput: 2.8525457171866155
(INFO) 2023-04-10 17:08:43,517 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,517 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 121, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:43,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,519 [api2.py:131] throughput: 1.1593754778594396
(INFO) 2023-04-10 17:08:43,520 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,520 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 19, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:08:43,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,521 [api2.py:131] throughput: 0.8808947297099768
(INFO) 2023-04-10 17:08:43,522 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,522 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 14, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:43,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,523 [api2.py:131] throughput: 0.9103341460643579
(INFO) 2023-04-10 17:08:43,524 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,524 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 22, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:08:43,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,525 [api2.py:131] throughput: 10.655207267209887
(INFO) 2023-04-10 17:08:43,526 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,526 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 7, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:08:43,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,527 [api2.py:131] throughput: 5.695616680423071
(INFO) 2023-04-10 17:08:43,528 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,528 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 350, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:08:43,529 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,529 [api2.py:131] throughput: 0.946238758708956
(INFO) 2023-04-10 17:08:43,530 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,530 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 19, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:43,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,532 [api2.py:131] throughput: 2.0668448716849794
(INFO) 2023-04-10 17:08:43,532 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,532 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 346, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:43,533 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,534 [api2.py:131] throughput: 0.5268228034260163
(INFO) 2023-04-10 17:08:43,534 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,534 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 24, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:43,535 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,536 [api2.py:131] throughput: 4.565607574985141
(INFO) 2023-04-10 17:08:43,536 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,536 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:08:43,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,538 [api2.py:131] throughput: 1.5171145408154678
(INFO) 2023-04-10 17:08:43,538 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,538 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 25, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:43,539 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,540 [api2.py:131] throughput: 1.938042507194841
(INFO) 2023-04-10 17:08:43,540 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,540 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:43,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,542 [api2.py:131] throughput: 0.8363520675547661
(INFO) 2023-04-10 17:08:43,542 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,542 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 16, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:43,544 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,544 [api2.py:131] throughput: 0.990327923086181
(INFO) 2023-04-10 17:08:43,545 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,545 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 22, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:08:43,546 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,546 [api2.py:131] throughput: 2.854199510867126
(INFO) 2023-04-10 17:08:43,547 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,547 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 23, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 17:08:43,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,548 [api2.py:131] throughput: 24.254742731560825
(INFO) 2023-04-10 17:08:43,549 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:08:43,549 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 10, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:08:43,550 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:43,550 [api2.py:131] throughput: 1.225845152093733
(INFO) 2023-04-10 17:08:46,617 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,617 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 373, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:46,620 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,620 [api2.py:131] throughput: 0.5135928758994093
(INFO) 2023-04-10 17:08:46,621 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,621 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 229, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:46,624 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,624 [api2.py:131] throughput: 0.619529499830211
(INFO) 2023-04-10 17:08:46,625 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,625 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 438, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 67}
(INFO) 2023-04-10 17:08:46,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,628 [api2.py:131] throughput: 70.18830977665262
(INFO) 2023-04-10 17:08:46,629 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,629 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 206, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:46,632 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,632 [api2.py:131] throughput: 0.8304589433438656
(INFO) 2023-04-10 17:08:46,633 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,633 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 121, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:46,636 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,636 [api2.py:131] throughput: 1.2012272460666558
(INFO) 2023-04-10 17:08:46,637 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,637 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 162, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:46,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,641 [api2.py:131] throughput: 0.6037715825486417
(INFO) 2023-04-10 17:08:46,641 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,641 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 355, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:46,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,645 [api2.py:131] throughput: 0.4299148666511939
(INFO) 2023-04-10 17:08:46,646 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,646 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:46,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,650 [api2.py:131] throughput: 4.995274293242692
(INFO) 2023-04-10 17:08:46,660 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,661 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:46,671 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,672 [api2.py:131] throughput: 0.79229441556705
(INFO) 2023-04-10 17:08:46,673 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,673 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 350, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:46,676 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,677 [api2.py:131] throughput: 0.9699009993387577
(INFO) 2023-04-10 17:08:46,677 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:08:46,677 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 271, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:46,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:46,681 [api2.py:131] throughput: 0.8091511113583194
(INFO) 2023-04-10 17:08:48,059 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,059 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:48,060 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,060 [api2.py:131] throughput: 2.4828526614896336
(INFO) 2023-04-10 17:08:48,061 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,061 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:48,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,062 [api2.py:131] throughput: 0.45397249200579126
(INFO) 2023-04-10 17:08:48,063 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,063 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 9, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:48,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,064 [api2.py:131] throughput: 0.412734197229822
(INFO) 2023-04-10 17:08:48,065 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,065 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 98, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:08:48,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,067 [api2.py:131] throughput: 0.738178370713198
(INFO) 2023-04-10 17:08:48,067 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,067 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 16, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:48,069 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,069 [api2.py:131] throughput: 0.4092496054398161
(INFO) 2023-04-10 17:08:48,070 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,070 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:48,071 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,072 [api2.py:131] throughput: 1.1627166087319816
(INFO) 2023-04-10 17:08:48,072 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,072 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 70}
(INFO) 2023-04-10 17:08:48,073 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,074 [api2.py:131] throughput: 26.951714974278147
(INFO) 2023-04-10 17:08:48,074 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:08:48,074 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 17, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:08:48,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:48,076 [api2.py:131] throughput: 1.818912375953259
(INFO) 2023-04-10 17:08:51,225 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:51,225 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 231, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:51,234 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:52,539 [api1.py:131] throughput: 0.01966357490503033
(INFO) 2023-04-10 17:08:52,541 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:52,541 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 301, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 73}
(INFO) 2023-04-10 17:08:52,550 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:52,876 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:52,876 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:52,878 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:52,923 [api1.py:131] throughput: 1.5966806536913807
(INFO) 2023-04-10 17:08:52,924 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:52,924 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:08:52,925 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:52,983 [api1.py:131] throughput: 0.5486783653251621
(INFO) 2023-04-10 17:08:52,984 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:52,984 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:08:52,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,055 [api1.py:131] throughput: 0.43881454233134504
(INFO) 2023-04-10 17:08:53,056 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,056 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:53,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,130 [api1.py:131] throughput: 0.210561443998984
(INFO) 2023-04-10 17:08:53,131 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,131 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:53,133 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,206 [api1.py:131] throughput: 0.29527286009096937
(INFO) 2023-04-10 17:08:53,207 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,207 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:08:53,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,253 [api1.py:131] throughput: 7.937917362416946
(INFO) 2023-04-10 17:08:53,254 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,254 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 109}
(INFO) 2023-04-10 17:08:53,256 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,304 [api1.py:131] throughput: 1.8523909961993401
(INFO) 2023-04-10 17:08:53,304 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,304 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:08:53,306 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,376 [api1.py:131] throughput: 0.24357044789389456
(INFO) 2023-04-10 17:08:53,377 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:08:53,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,449 [api1.py:131] throughput: 0.6970367520066272
(INFO) 2023-04-10 17:08:53,450 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,450 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:53,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,523 [api1.py:131] throughput: 0.409227641480485
(INFO) 2023-04-10 17:08:53,524 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,524 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 79}
(INFO) 2023-04-10 17:08:53,526 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,598 [api1.py:131] throughput: 1.142917505065116
(INFO) 2023-04-10 17:08:53,599 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:08:53,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,646 [api1.py:131] throughput: 0.4109412865002516
(INFO) 2023-04-10 17:08:53,648 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:53,648 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 230, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:53,669 [api1.py:131] throughput: 0.5180297150729811
(INFO) 2023-04-10 17:08:53,670 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,670 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 143, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:08:53,672 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,725 [api1.py:131] throughput: 0.6680266400338936
(INFO) 2023-04-10 17:08:53,726 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,726 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 78}
(INFO) 2023-04-10 17:08:53,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,801 [api1.py:131] throughput: 1.0580830365960618
(INFO) 2023-04-10 17:08:53,802 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,802 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:08:53,804 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,874 [api1.py:131] throughput: 0.22412654478772742
(INFO) 2023-04-10 17:08:53,875 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:08:53,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,919 [api1.py:131] throughput: 1.68140607860419
(INFO) 2023-04-10 17:08:53,920 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:08:53,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:53,981 [api1.py:131] throughput: 0.2757009949677484
(INFO) 2023-04-10 17:08:53,982 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:08:53,982 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:08:53,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:54,035 [api1.py:131] throughput: 0.62642799523509
(INFO) 2023-04-10 17:08:54,927 [api1.py:131] throughput: 0.0064804417048014455
(INFO) 2023-04-10 17:08:54,928 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:54,928 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 139, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:08:54,937 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:55,530 [api1.py:131] throughput: 0.8350838382090411
(INFO) 2023-04-10 17:08:55,531 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:55,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 115, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 88}
(INFO) 2023-04-10 17:08:55,671 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:56,378 [api1.py:131] throughput: 0.7995921798507672
(INFO) 2023-04-10 17:08:56,379 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:56,379 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:08:56,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:57,069 [api1.py:131] throughput: 4.319341271403549
(INFO) 2023-04-10 17:08:57,070 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:57,070 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 164, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 695}
(INFO) 2023-04-10 17:08:57,079 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:57,758 [api1.py:131] throughput: 7.355742813879562
(INFO) 2023-04-10 17:08:57,760 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:57,760 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:08:57,769 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:58,828 [api1.py:131] throughput: 0.6304630734837612
(INFO) 2023-04-10 17:08:58,829 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:58,830 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:58,839 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:08:59,838 [api1.py:131] throughput: 0.24103225912725923
(INFO) 2023-04-10 17:08:59,840 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:08:59,840 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 195, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:08:59,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:00,984 [api1.py:131] throughput: 0.12850798529907598
(INFO) 2023-04-10 17:09:00,985 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:00,986 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:09:00,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:02,257 [api1.py:131] throughput: 0.31449615221757266
(INFO) 2023-04-10 17:09:02,258 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:02,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 250}
(INFO) 2023-04-10 17:09:02,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:02,929 [api1.py:131] throughput: 1.6726842384567064
(INFO) 2023-04-10 17:09:02,930 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:02,931 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 477, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:09:02,940 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:04,195 [api1.py:131] throughput: 0.048718424795765686
(INFO) 2023-04-10 17:09:04,196 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:04,196 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 137, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:09:04,205 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:05,209 [api1.py:131] throughput: 0.7306227768133445
(INFO) 2023-04-10 17:09:05,210 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:05,210 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 196, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:09:05,357 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:06,503 [api1.py:131] throughput: 0.03206690835384127
(INFO) 2023-04-10 17:09:06,504 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:06,504 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 170, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:09:06,514 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:07,650 [api1.py:131] throughput: 0.05292456139136109
(INFO) 2023-04-10 17:09:07,651 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:07,651 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:09:07,803 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:09,083 [api1.py:131] throughput: 0.11053504711226489
(INFO) 2023-04-10 17:09:09,084 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:09,084 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 369, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 101}
(INFO) 2023-04-10 17:09:09,094 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:10,050 [api1.py:131] throughput: 1.6542611827505052
(INFO) 2023-04-10 17:09:10,052 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:10,052 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 171, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 203}
(INFO) 2023-04-10 17:09:10,061 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:10,901 [api1.py:131] throughput: 6.303684974180295
(INFO) 2023-04-10 17:09:10,902 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:10,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 380, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:09:10,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:11,975 [api1.py:131] throughput: 0.10180948915512497
(INFO) 2023-04-10 17:09:16,670 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,670 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 145, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 17:09:16,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,675 [api2.py:131] throughput: 0.8047276576203853
(INFO) 2023-04-10 17:09:16,675 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,675 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:09:16,680 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,680 [api2.py:131] throughput: 0.7948334759398494
(INFO) 2023-04-10 17:09:16,681 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,681 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 17:09:16,686 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,686 [api2.py:131] throughput: 0.7859087064181424
(INFO) 2023-04-10 17:09:16,687 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,687 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:09:16,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,692 [api2.py:131] throughput: 1.4051506811911292
(INFO) 2023-04-10 17:09:16,692 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,692 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:09:16,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,697 [api2.py:131] throughput: 1.5077472204344387
(INFO) 2023-04-10 17:09:16,698 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,698 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:09:16,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,703 [api2.py:131] throughput: 2.096934335603923
(INFO) 2023-04-10 17:09:16,704 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,704 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 621}
(INFO) 2023-04-10 17:09:16,708 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,709 [api2.py:131] throughput: 153.8530156603841
(INFO) 2023-04-10 17:09:16,709 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,709 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 169, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:09:16,714 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,714 [api2.py:131] throughput: 0.9697089338019298
(INFO) 2023-04-10 17:09:16,715 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,715 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 164, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 17:09:16,720 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,720 [api2.py:131] throughput: 0.32510442076164453
(INFO) 2023-04-10 17:09:16,721 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,721 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:09:16,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,726 [api2.py:131] throughput: 16.72282041898423
(INFO) 2023-04-10 17:09:16,726 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,726 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 280, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 245}
(INFO) 2023-04-10 17:09:16,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,732 [api2.py:131] throughput: 0.665744889835354
(INFO) 2023-04-10 17:09:16,732 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,732 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 153, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:09:16,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,737 [api2.py:131] throughput: 0.7290060608851705
(INFO) 2023-04-10 17:09:16,738 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,738 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 143, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 65}
(INFO) 2023-04-10 17:09:16,743 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,743 [api2.py:131] throughput: 0.6273196053126899
(INFO) 2023-04-10 17:09:16,743 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,744 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 90, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 67}
(INFO) 2023-04-10 17:09:16,748 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,749 [api2.py:131] throughput: 0.671748486567848
(INFO) 2023-04-10 17:09:16,749 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,749 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 90, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:09:16,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,754 [api2.py:131] throughput: 1.408670789814382
(INFO) 2023-04-10 17:09:16,755 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,755 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 108}
(INFO) 2023-04-10 17:09:16,760 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,760 [api2.py:131] throughput: 1.1100500725433398
(INFO) 2023-04-10 17:09:16,760 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,760 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:09:16,765 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,765 [api2.py:131] throughput: 0.3727751839557715
(INFO) 2023-04-10 17:09:16,766 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,766 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:09:16,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,770 [api2.py:131] throughput: 0.6908622861031584
(INFO) 2023-04-10 17:09:16,771 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,771 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:09:16,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,906 [api2.py:131] throughput: 4.053653626801954
(INFO) 2023-04-10 17:09:16,907 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:09:16,907 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 77}
(INFO) 2023-04-10 17:09:16,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:09:16,912 [api2.py:131] throughput: 1.0688257523691411
