(DEBUG) 2023-04-09 20:33:20,888 [logger.py:40] logger init.
(INFO) 2023-04-09 20:33:20,888 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-09-20-33-20-888152.log
(INFO) 2023-04-09 20:33:24,156 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:24,156 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,193 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:24,194 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,273 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:24,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,291 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,548 [api1.py:131] throughput: 0.004961253977952574
(INFO) 2023-04-09 20:33:24,585 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:24,585 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,654 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:24,655 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,663 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,728 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 20:33:24,771 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:24,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,777 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,887 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:24,924 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:24,925 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:24,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:24,965 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:33:24,966 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:33:24,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,012 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:25,013 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,023 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,113 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:25,160 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:25,160 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,168 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,200 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:33:25,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,238 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:25,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,259 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:33:25,272 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,305 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:25,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,307 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:25,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,313 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,320 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,336 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:33:25,342 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:25,342 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,364 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:25,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,372 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,377 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:25,378 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,431 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:25,436 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:33:25,437 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,471 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,476 [api1.py:131] throughput: 0.004961254469161529
(INFO) 2023-04-09 20:33:25,480 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:25,480 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,501 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:25,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,517 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:25,517 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,531 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:25,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,535 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:25,536 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,552 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:25,553 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,557 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:25,557 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,566 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,571 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:33:25,571 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,572 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 20:33:25,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,584 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,601 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 20:33:25,617 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:25,618 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,649 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:25,650 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,660 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,665 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:25,665 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,675 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,689 [api1.py:131] throughput: 0.005788073306210985
(INFO) 2023-04-09 20:33:25,690 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:33:25,723 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:25,730 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:25,732 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 20:33:25,736 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:25,737 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,737 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:25,737 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,738 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:25,739 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,758 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:25,764 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:25,764 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,772 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:25,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,773 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,776 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:25,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,777 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:25,777 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,779 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,787 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:25,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,797 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 61}
(INFO) 2023-04-09 20:33:25,797 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,800 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,806 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,822 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:25,822 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,823 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:25,823 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,831 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,833 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,843 [api1.py:131] throughput: 0.00496125378551189
(INFO) 2023-04-09 20:33:25,876 [api1.py:131] throughput: 0.00868180734098237
(INFO) 2023-04-09 20:33:25,896 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:33:25,909 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:25,910 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,911 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:25,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,954 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:25,955 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:33:25,955 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,963 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,963 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:25,964 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,964 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:25,965 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,969 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,974 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:25,974 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:25,975 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:25,980 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:26,008 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:26,008 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,043 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,044 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,050 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:26,051 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:26,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,077 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:33:26,086 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:26,086 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:26,091 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:33:26,094 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,095 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,102 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:26,103 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,109 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:33:26,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,131 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 20:33:26,131 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,132 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,137 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:26,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,137 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:26,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,138 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,138 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,144 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,150 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 20:33:26,150 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:26,151 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,164 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,167 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:26,178 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:33:26,178 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,188 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,188 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,203 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:33:26,204 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,225 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:26,225 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,233 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,254 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 20:33:26,266 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 46}
(INFO) 2023-04-09 20:33:26,266 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,274 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:26,276 [api1.py:131] throughput: 0.00868180734098237
(INFO) 2023-04-09 20:33:26,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,280 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:26,304 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:26,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,324 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:33:26,325 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,325 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,325 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,326 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,326 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:33:26,326 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,327 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:26,332 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,335 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:26,341 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,349 [api1.py:131] throughput: 0.0069455907138505305
(INFO) 2023-04-09 20:33:26,354 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:26,359 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:26,361 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,368 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:26,369 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:33:26,370 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,370 [api1.py:131] throughput: 0.005788072145144621
(INFO) 2023-04-09 20:33:26,371 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,374 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,389 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:26,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,393 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 20:33:26,395 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,411 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:26,412 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,414 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:33:26,414 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,427 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,430 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:26,431 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:26,431 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,431 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,432 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,440 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:26,440 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,441 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,442 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 20:33:26,443 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:33:26,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,488 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:26,489 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,495 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:26,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,499 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:26,501 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:26,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,505 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,505 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,508 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,516 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,517 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,522 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:33:26,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,537 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:26,538 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,540 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,541 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,544 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,556 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,578 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:26,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,589 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,598 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:26,605 [api1.py:131] throughput: 0.008681805821076185
(INFO) 2023-04-09 20:33:26,610 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 20:33:26,613 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:33:26,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,623 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,626 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:26,632 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,632 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,637 [api1.py:131] throughput: 0.017361806706678782
(INFO) 2023-04-09 20:33:26,642 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,643 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:33:26,652 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:26,652 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,660 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:26,666 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:26,672 [api1.py:131] throughput: 0.011575340936814248
(INFO) 2023-04-09 20:33:26,672 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,672 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,672 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,673 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,673 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:26,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,674 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,675 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,675 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:26,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,684 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,685 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,688 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:26,689 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:26,706 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:26,706 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,718 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,721 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,722 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,722 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:26,723 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,725 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:33:26,726 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,729 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,740 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,742 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:26,743 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,750 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,752 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:26,769 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:33:26,773 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,774 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,777 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:26,777 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,781 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:26,781 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,790 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:26,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,791 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:26,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,797 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,800 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,808 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,816 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:33:26,843 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:26,865 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 20:33:26,866 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:26,867 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,876 [api1.py:131] throughput: 0.0038588046427999855
(INFO) 2023-04-09 20:33:26,880 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:26,882 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:26,882 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,899 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:26,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,900 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,907 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:26,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,922 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:26,922 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,933 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,934 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,939 [api1.py:131] throughput: 0.00385880439348466
(INFO) 2023-04-09 20:33:26,939 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,940 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,940 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,940 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:26,941 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:26,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,942 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:26,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,942 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:26,943 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,944 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:33:26,944 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,947 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,948 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,952 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,958 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,958 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:26,959 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,961 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,966 [api1.py:131] throughput: 0.00694558872227711
(INFO) 2023-04-09 20:33:26,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,969 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,972 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:26,973 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:26,973 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:33:26,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,974 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,977 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,979 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:26,979 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,980 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:26,980 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:26,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,983 [api1.py:131] throughput: 0.011575340434531399
(INFO) 2023-04-09 20:33:26,987 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:26,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,987 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:33:26,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,990 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,992 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:26,992 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:26,993 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:26,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,003 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,014 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:27,014 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,015 [api1.py:131] throughput: 0.011575336245798548
(INFO) 2023-04-09 20:33:27,025 [api1.py:131] throughput: 0.005788071437064676
(INFO) 2023-04-09 20:33:27,027 [api1.py:131] throughput: 0.00868180376558486
(INFO) 2023-04-09 20:33:27,030 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,070 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:27,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,078 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,104 [api1.py:131] throughput: 0.0069455906312028875
(INFO) 2023-04-09 20:33:27,117 [api1.py:131] throughput: 0.005788071248872792
(INFO) 2023-04-09 20:33:27,117 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:27,128 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:27,136 [api1.py:131] throughput: 0.008681806211909154
(INFO) 2023-04-09 20:33:27,139 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:27,139 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,143 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:27,143 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,144 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,166 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:27,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,171 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:27,174 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,174 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:27,175 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:27,175 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,188 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:27,189 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,201 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,218 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:27,222 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:27,223 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,231 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,234 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,234 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:33:27,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,238 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:27,239 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,239 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:33:27,239 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:27,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,240 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:27,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,241 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:27,241 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:27,242 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:27,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,258 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,289 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:27,289 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,292 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:27,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,302 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:27,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,324 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:33:27,355 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:27,370 [api1.py:131] throughput: 0.008681805821076185
(INFO) 2023-04-09 20:33:27,387 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:27,387 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,406 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 67}
(INFO) 2023-04-09 20:33:27,406 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,415 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:27,416 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,420 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,438 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:27,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,440 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:27,440 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,442 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:27,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,443 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 20:33:27,444 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:27,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,453 [api1.py:131] throughput: 0.011575342540798778
(INFO) 2023-04-09 20:33:27,481 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:27,482 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,486 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:27,486 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:27,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,494 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,502 [api1.py:131] throughput: 0.006945591228188721
(INFO) 2023-04-09 20:33:27,508 [api1.py:131] throughput: 0.008681805213113862
(INFO) 2023-04-09 20:33:27,508 [api1.py:131] throughput: 0.011575337662575496
(INFO) 2023-04-09 20:33:27,509 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:33:27,510 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:27,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,511 [api1.py:131] throughput: 0.0038588045860345724
(INFO) 2023-04-09 20:33:27,518 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,524 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:27,564 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:33:27,574 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:27,574 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,582 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:27,618 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:33:27,634 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:27,636 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,641 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,642 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:27,650 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:27,650 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,653 [api1.py:131] throughput: 0.006945585553867201
(INFO) 2023-04-09 20:33:27,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,672 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:27,673 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,677 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,683 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:27,688 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:27,689 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,705 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 20:33:27,739 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 20:33:27,741 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:27,741 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,748 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,771 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:27,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,772 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:27,772 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:27,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,775 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:27,775 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:27,775 [api1.py:131] throughput: 0.004961254430754241
(INFO) 2023-04-09 20:33:27,775 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,775 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,777 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:27,777 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,780 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,783 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,787 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:27,796 [api1.py:131] throughput: 0.005788072739931908
(INFO) 2023-04-09 20:33:27,823 [api1.py:131] throughput: 0.00578807258268928
(INFO) 2023-04-09 20:33:27,832 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:33:27,840 [api1.py:131] throughput: 0.008681807437366683
(INFO) 2023-04-09 20:33:27,841 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 20:33:27,841 [api1.py:131] throughput: 0.0069455903689555724
(INFO) 2023-04-09 20:33:27,842 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:27,842 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 61}
(INFO) 2023-04-09 20:33:27,842 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:27,843 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,844 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:27,844 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,845 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,859 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:27,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,869 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:27,870 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:27,875 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:27,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,875 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:27,876 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,884 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:27,893 [api1.py:131] throughput: 0.003472940161883284
(INFO) 2023-04-09 20:33:27,912 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 20:33:27,922 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 20:33:27,923 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:33:27,949 [api1.py:131] throughput: 0.004961254802257489
(INFO) 2023-04-09 20:33:27,954 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:27,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:27,962 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,001 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:28,011 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:28,011 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,015 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,020 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:28,037 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:28,037 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,037 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:33:28,038 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,039 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:28,041 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,048 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 20:33:28,050 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,061 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:28,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,071 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,083 [api1.py:131] throughput: 0.0057880730392393265
(INFO) 2023-04-09 20:33:28,094 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:33:28,094 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,094 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:28,094 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:28,095 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,095 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,097 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:33:28,097 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,098 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:28,099 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,100 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 20:33:28,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,105 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:28,106 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,107 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:28,109 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,109 [api1.py:131] throughput: 0.008681807132995177
(INFO) 2023-04-09 20:33:28,113 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,121 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,122 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,126 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:28,127 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,130 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:28,135 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,135 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:28,136 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,138 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:28,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,139 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:28,139 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,144 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,144 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:28,145 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,145 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:28,146 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,149 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,153 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,155 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,178 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:28,186 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:28,187 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,204 [api1.py:131] throughput: 0.004961254669087043
(INFO) 2023-04-09 20:33:28,242 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 20:33:28,260 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:28,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,268 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,285 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 20:33:28,288 [api1.py:131] throughput: 0.0069455879054211534
(INFO) 2023-04-09 20:33:28,290 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:28,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,299 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:28,299 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,308 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:28,312 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:28,312 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,315 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:28,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,316 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:28,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,320 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,322 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:28,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,325 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:28,325 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,326 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:28,326 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,331 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,333 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,342 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:28,400 [api1.py:131] throughput: 0.0049612539575765
(INFO) 2023-04-09 20:33:28,428 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:28,429 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,433 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,433 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 20:33:28,467 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:28,471 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 20:33:28,476 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:28,477 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,487 [api1.py:131] throughput: 0.0038588046610442826
(INFO) 2023-04-09 20:33:28,488 [api1.py:131] throughput: 0.006945591272251963
(INFO) 2023-04-09 20:33:28,494 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:28,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,499 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,502 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:28,503 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,510 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:28,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,513 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,525 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:28,526 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,529 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:28,530 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,532 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:28,533 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:28,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,547 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:28,553 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:28,554 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,561 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,561 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 20:33:28,579 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:28,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,583 [api1.py:131] throughput: 0.008681806387110151
(INFO) 2023-04-09 20:33:28,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,587 [api1.py:131] throughput: 0.006945587560526473
(INFO) 2023-04-09 20:33:28,600 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:28,600 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,606 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:28,607 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,612 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,613 [api1.py:131] throughput: 0.003472940176097175
(INFO) 2023-04-09 20:33:28,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,618 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:28,619 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:28,643 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:28,656 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:28,662 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 64}
(INFO) 2023-04-09 20:33:28,662 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,673 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 20:33:28,682 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:28,683 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,689 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:28,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,691 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:28,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,694 [api1.py:131] throughput: 0.011575333837278531
(INFO) 2023-04-09 20:33:28,712 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 20:33:28,717 [api1.py:131] throughput: 0.005788072145144621
(INFO) 2023-04-09 20:33:28,717 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:28,718 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,723 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:33:28,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,725 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:28,726 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,728 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:33:28,728 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 67}
(INFO) 2023-04-09 20:33:28,729 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,729 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:28,730 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 63}
(INFO) 2023-04-09 20:33:28,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,730 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 53}
(INFO) 2023-04-09 20:33:28,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,731 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:28,732 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,740 [api1.py:131] throughput: 0.006945589767853012
(INFO) 2023-04-09 20:33:28,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,741 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:28,741 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,742 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,743 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:33:28,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,746 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:28,747 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,764 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,767 [api1.py:131] throughput: 0.003858804694605771
(INFO) 2023-04-09 20:33:28,776 [api1.py:131] throughput: 0.00434112994820707
(INFO) 2023-04-09 20:33:28,778 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 61}
(INFO) 2023-04-09 20:33:28,779 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,781 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:28,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,790 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:28,798 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:28,798 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,803 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:33:28,804 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,812 [api1.py:131] throughput: 0.008681806211909154
(INFO) 2023-04-09 20:33:28,815 [api1.py:131] throughput: 0.004961254721404003
(INFO) 2023-04-09 20:33:28,817 [api1.py:131] throughput: 0.011575340260000804
(INFO) 2023-04-09 20:33:28,826 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,842 [api1.py:131] throughput: 0.004961254742055436
(INFO) 2023-04-09 20:33:28,884 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:28,886 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:28,887 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,946 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:28,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,950 [api1.py:131] throughput: 0.0026715181520459543
(INFO) 2023-04-09 20:33:28,954 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:33:28,955 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,966 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:28,966 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,979 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:28,979 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:28,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:28,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,987 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:28,998 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:28,999 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,002 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:29,002 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,003 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:29,003 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,003 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:29,003 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:29,004 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,004 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,008 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,014 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,019 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:29,019 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,035 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:29,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,038 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:29,039 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,043 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:33:29,045 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,045 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,047 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:29,048 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,053 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:33:29,054 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:29,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,069 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:29,069 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:29,070 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,073 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,074 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:29,078 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,078 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:29,079 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,084 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,088 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:29,089 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,101 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 20:33:29,110 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 20:33:29,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,115 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 20:33:29,122 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:29,130 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:29,157 [api1.py:131] throughput: 0.006945591181922313
(INFO) 2023-04-09 20:33:29,176 [api1.py:131] throughput: 0.006945589917307465
(INFO) 2023-04-09 20:33:29,186 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:29,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,193 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:29,193 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,202 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,231 [api1.py:131] throughput: 0.011575338524129215
(INFO) 2023-04-09 20:33:29,231 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:33:29,235 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:29,238 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:29,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,245 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:29,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,271 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:33:29,282 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:29,287 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:29,288 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:29,288 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,288 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,289 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:29,289 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:29,289 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,289 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,294 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,302 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,333 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 20:33:29,335 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 20:33:29,363 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:29,441 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:29,441 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,446 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,457 [api1.py:131] throughput: 0.002315325668197821
(INFO) 2023-04-09 20:33:29,490 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:29,490 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,495 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:29,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,500 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:29,500 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,502 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:29,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,513 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,522 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:29,523 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,525 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:29,526 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,530 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,542 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:29,542 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,546 [api1.py:131] throughput: 0.00868180451829988
(INFO) 2023-04-09 20:33:29,550 [api1.py:131] throughput: 0.008681806211909154
(INFO) 2023-04-09 20:33:29,559 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:29,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,604 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:29,605 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,640 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:33:29,641 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,655 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:29,656 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,657 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:29,660 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:29,661 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,662 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,665 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:33:29,665 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,673 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,675 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:29,675 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,676 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:29,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,676 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:29,677 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,680 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,683 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,683 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,683 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 20:33:29,685 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:29,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,689 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:29,689 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:29,689 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,689 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,691 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:33:29,693 [api1.py:131] throughput: 0.00868180376558486
(INFO) 2023-04-09 20:33:29,694 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,694 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,699 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:29,699 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,706 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,750 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:29,751 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,757 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,773 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:29,773 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,784 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,801 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:29,801 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,824 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:29,824 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,833 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,834 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:29,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,853 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 20:33:29,862 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:29,863 [api1.py:131] throughput: 0.0028941369886029524
(INFO) 2023-04-09 20:33:29,909 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:29,909 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:29,911 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:29,914 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:29,915 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:29,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:29,918 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:29,983 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:29,989 [api1.py:131] throughput: 0.005788070518376598
(INFO) 2023-04-09 20:33:30,003 [api1.py:131] throughput: 0.011575335385612713
(INFO) 2023-04-09 20:33:30,004 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:30,015 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:30,020 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:30,021 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,021 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:30,021 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,030 [api1.py:131] throughput: 0.011575338781083858
(INFO) 2023-04-09 20:33:30,034 [api1.py:131] throughput: 0.011575342323907762
(INFO) 2023-04-09 20:33:30,047 [api1.py:131] throughput: 0.00496125450707324
(INFO) 2023-04-09 20:33:30,050 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:30,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,069 [api1.py:131] throughput: 0.002671518242496278
(INFO) 2023-04-09 20:33:30,078 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:30,078 [api1.py:131] throughput: 0.0069455907138505305
(INFO) 2023-04-09 20:33:30,080 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:30,100 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:30,100 [api1.py:131] throughput: 0.0028941369955520625
(INFO) 2023-04-09 20:33:30,130 [api1.py:131] throughput: 0.0057880728029738844
(INFO) 2023-04-09 20:33:30,136 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:30,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,143 [api1.py:131] throughput: 0.011575342643434709
(INFO) 2023-04-09 20:33:30,146 [api1.py:131] throughput: 0.004961253441382704
(INFO) 2023-04-09 20:33:30,160 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,161 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,166 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,174 [api1.py:131] throughput: 0.004961254621254395
(INFO) 2023-04-09 20:33:30,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,199 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:30,210 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:33:30,211 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,211 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 20:33:30,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,223 [api1.py:131] throughput: 0.002042941060815811
(INFO) 2023-04-09 20:33:30,247 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:33:30,251 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 53}
(INFO) 2023-04-09 20:33:30,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,296 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:33:30,312 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:30,319 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:33:30,332 [api1.py:131] throughput: 0.005788072969484766
(INFO) 2023-04-09 20:33:30,340 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:33:30,341 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,342 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:30,346 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:30,346 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,358 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,376 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:30,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,383 [api1.py:131] throughput: 0.011575332580659792
(INFO) 2023-04-09 20:33:30,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,392 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:30,392 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,398 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,442 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:33:30,464 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 20:33:30,473 [api1.py:131] throughput: 0.005788071943735196
(INFO) 2023-04-09 20:33:30,481 [api1.py:131] throughput: 0.006945588020386053
(INFO) 2023-04-09 20:33:30,487 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,488 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,491 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:30,492 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,493 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,495 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:30,496 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,499 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 20:33:30,500 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,505 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,508 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:30,508 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,508 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:30,509 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,509 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,510 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:30,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,511 [api1.py:131] throughput: 0.0034729403578626916
(INFO) 2023-04-09 20:33:30,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,519 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:30,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,519 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,520 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,522 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,530 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:30,530 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,532 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,538 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:30,538 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,572 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:30,572 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,584 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:30,586 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,590 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,607 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:30,607 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,614 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:33:30,635 [api1.py:131] throughput: 0.0031572304449604444
(INFO) 2023-04-09 20:33:30,638 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:30,681 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:30,684 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:30,700 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:30,704 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:30,712 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:30,758 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:30,759 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,776 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:30,777 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,778 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:30,779 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,780 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:30,781 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,783 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:30,783 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:30,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,784 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:30,785 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,786 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,786 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,786 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:30,786 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:30,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,788 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,788 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:30,790 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 62}
(INFO) 2023-04-09 20:33:30,790 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,793 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,798 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,799 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,798 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:30,799 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,801 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:30,803 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,805 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,809 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,811 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:30,811 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,811 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,812 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,813 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:30,813 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,814 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:30,814 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,814 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 20:33:30,816 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,817 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,820 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,825 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,841 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:33:30,851 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:33:30,874 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:30,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:30,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:30,890 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:30,908 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 20:33:30,961 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:30,995 [api1.py:131] throughput: 0.004961254669087043
(INFO) 2023-04-09 20:33:31,005 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:31,009 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:31,010 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,010 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:31,022 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 20:33:31,022 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:31,022 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,023 [api1.py:131] throughput: 0.0038588045044240116
(INFO) 2023-04-09 20:33:31,023 [api1.py:131] throughput: 0.0038588046610442826
(INFO) 2023-04-09 20:33:31,026 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:31,029 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:31,030 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,030 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,031 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:31,031 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,034 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:31,034 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:31,034 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:31,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,035 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:31,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,039 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,039 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:31,040 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,043 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,044 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,048 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:31,049 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,052 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:31,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,055 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:31,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,057 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:33:31,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,066 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:31,161 [api1.py:131] throughput: 0.0023153256743794217
(INFO) 2023-04-09 20:33:31,164 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:31,189 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:31,198 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:31,198 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,198 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 20:33:31,204 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,215 [api1.py:131] throughput: 0.004961254998426613
(INFO) 2023-04-09 20:33:31,298 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:31,299 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,305 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,309 [api1.py:131] throughput: 0.0026715182259590088
(INFO) 2023-04-09 20:33:31,309 [api1.py:131] throughput: 0.0031572303788588506
(INFO) 2023-04-09 20:33:31,331 [api1.py:131] throughput: 0.008681807780066478
(INFO) 2023-04-09 20:33:31,338 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:31,349 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:33:31,350 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:33:31,368 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:33:31,371 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:31,376 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,383 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:33:31,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,398 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:31,399 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,407 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:31,407 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,424 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 20:33:31,427 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:33:31,431 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:31,430 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:31,431 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,466 [api1.py:131] throughput: 0.011575341471835993
(INFO) 2023-04-09 20:33:31,468 [api1.py:131] throughput: 0.004961253977952574
(INFO) 2023-04-09 20:33:31,494 [api1.py:131] throughput: 0.006945591673717102
(INFO) 2023-04-09 20:33:31,496 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:31,497 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,503 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:31,504 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,511 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,524 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:31,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,525 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:31,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,525 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:31,526 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,531 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:31,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,535 [api1.py:131] throughput: 0.011575338524129215
(INFO) 2023-04-09 20:33:31,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,537 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:31,537 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,544 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:31,545 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,577 [api1.py:131] throughput: 0.011575339024861351
(INFO) 2023-04-09 20:33:31,584 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:31,585 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,585 [api1.py:131] throughput: 0.008681806981731769
(INFO) 2023-04-09 20:33:31,586 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:31,591 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,599 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:31,600 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,605 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,613 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:33:31,619 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:31,647 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:31,647 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,656 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,659 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:31,673 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:31,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,680 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,681 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:31,682 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:31,704 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:31,714 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:31,726 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:33:31,733 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:31,745 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,750 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,767 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:31,775 [api1.py:131] throughput: 0.006945588615498541
(INFO) 2023-04-09 20:33:31,779 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:33:31,779 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:31,780 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,795 [api1.py:131] throughput: 0.003858804317578792
(INFO) 2023-04-09 20:33:31,804 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:31,805 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,838 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:31,838 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,842 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,847 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:33:31,858 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:31,859 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,863 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,866 [api1.py:131] throughput: 0.004341130174229805
(INFO) 2023-04-09 20:33:31,869 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:31,869 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:31,870 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:31,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,871 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:31,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,874 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:31,874 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,883 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:31,883 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,883 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:31,885 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:31,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,889 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,892 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,901 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,907 [api1.py:131] throughput: 0.017361806957399074
(INFO) 2023-04-09 20:33:31,914 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:31,914 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,917 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:31,918 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:31,919 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,920 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 20:33:31,921 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:31,921 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,928 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,935 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,937 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:33:31,948 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 64}
(INFO) 2023-04-09 20:33:31,948 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:31,949 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:33:31,968 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:31,970 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:31,981 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:32,035 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:32,047 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:32,048 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,052 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,062 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:32,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,067 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,112 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 20:33:32,113 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:32,114 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,122 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:32,122 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,137 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:32,165 [api1.py:131] throughput: 0.004341130048198692
(INFO) 2023-04-09 20:33:32,168 [api1.py:131] throughput: 0.005788072979933829
(INFO) 2023-04-09 20:33:32,172 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:32,172 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,177 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,178 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:32,179 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,181 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:32,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,182 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:33:32,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,184 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,184 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:32,185 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,189 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,208 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:32,208 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,211 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:32,214 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,217 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:32,232 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:33:32,232 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,239 [api1.py:131] throughput: 0.003472940143137429
(INFO) 2023-04-09 20:33:32,241 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,249 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:33:32,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,254 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:32,254 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,262 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:32,262 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,269 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,281 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:33:32,282 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,290 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:32,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,293 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,298 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,306 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:32,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,312 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,328 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:32,329 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,329 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:32,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,374 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:32,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,421 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 20:33:32,486 [api1.py:131] throughput: 0.004341130085695552
(INFO) 2023-04-09 20:33:32,490 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:32,490 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:32,491 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,491 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,493 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:33:32,494 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,501 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:32,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,501 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,502 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:32,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,505 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:32,505 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:33:32,505 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:32,506 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,506 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,506 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:33:32,507 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,508 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:32,508 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,508 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,509 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:33:32,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,509 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,511 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,511 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,515 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:32,517 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:32,518 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,518 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:32,518 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,522 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,524 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:32,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,525 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 20:33:32,526 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:32,529 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:32,530 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,541 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:32,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,571 [api1.py:131] throughput: 0.005788072453301067
(INFO) 2023-04-09 20:33:32,613 [api1.py:131] throughput: 0.006945589779849356
(INFO) 2023-04-09 20:33:32,636 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 20:33:32,668 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:33:32,680 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:32,699 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:32,740 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 20:33:32,747 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:33:32,750 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:32,754 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,758 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,791 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:32,803 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:33:32,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,813 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,849 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:32,849 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,859 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 20:33:32,869 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,886 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:32,906 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:32,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,914 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,914 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:32,915 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,938 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:32,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:32,984 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:32,985 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:32,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,004 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:33,005 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,022 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,023 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,026 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:33,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,033 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:33,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,034 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,042 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,054 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:33,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,056 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:33,057 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,067 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,099 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,104 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,113 [api1.py:131] throughput: 0.005788071882782347
(INFO) 2023-04-09 20:33:33,114 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 52}
(INFO) 2023-04-09 20:33:33,115 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,125 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,130 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:33,131 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,162 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:33,162 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,168 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:33,171 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:33,171 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,178 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,183 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,190 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,191 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:33:33,191 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,211 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:33,215 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:33,216 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,218 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:33:33,220 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,222 [api1.py:131] throughput: 0.011575340385444667
(INFO) 2023-04-09 20:33:33,238 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:33:33,240 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 20:33:33,241 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,248 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:33,248 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:33,249 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,252 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,254 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:33,254 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,258 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:33,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,260 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,263 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:33,264 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,265 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:33:33,265 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:33,266 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,266 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,283 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:33,288 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:33,313 [api1.py:131] throughput: 0.017361803286854736
(INFO) 2023-04-09 20:33:33,321 [api1.py:131] throughput: 0.0069455901226020515
(INFO) 2023-04-09 20:33:33,323 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:33,333 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:33,344 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,364 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,368 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:33,378 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:33,378 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,430 [api1.py:131] throughput: 0.005788072314328548
(INFO) 2023-04-09 20:33:33,487 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:33,515 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:33,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,531 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:33,532 [api1.py:131] throughput: 0.004961254932609519
(INFO) 2023-04-09 20:33:33,560 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:33,562 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:33,563 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,573 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:33,579 [api1.py:131] throughput: 0.006945591272251963
(INFO) 2023-04-09 20:33:33,581 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:33,581 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,584 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:33,590 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:33,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,593 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,600 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,602 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,602 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,607 [api1.py:131] throughput: 0.017361798545737262
(INFO) 2023-04-09 20:33:33,608 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,612 [api1.py:131] throughput: 0.005788071398745061
(INFO) 2023-04-09 20:33:33,615 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:33,630 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:33,630 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,637 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:33:33,638 [api1.py:131] throughput: 0.0026715181953301218
(INFO) 2023-04-09 20:33:33,643 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:33:33,644 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,714 [api1.py:131] throughput: 0.004961254645835062
(INFO) 2023-04-09 20:33:33,729 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:33,730 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,754 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:33,758 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:33,758 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,771 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 61}
(INFO) 2023-04-09 20:33:33,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,783 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,790 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:33,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,798 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:33,821 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:33,840 [api1.py:131] throughput: 0.0031572305046491988
(INFO) 2023-04-09 20:33:33,856 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:33,861 [api1.py:131] throughput: 0.006945589779849356
(INFO) 2023-04-09 20:33:33,864 [api1.py:131] throughput: 0.0020429410642917836
(INFO) 2023-04-09 20:33:33,923 [api1.py:131] throughput: 0.0034729403959386936
(INFO) 2023-04-09 20:33:33,940 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:33,940 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:33,942 [api1.py:131] throughput: 0.011575339886586523
(INFO) 2023-04-09 20:33:33,946 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:33,966 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:33,969 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 40, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:33:33,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,006 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:34,007 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,011 [api1.py:131] throughput: 0.005788072001640405
(INFO) 2023-04-09 20:33:34,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,023 [api1.py:131] throughput: 0.006945589895198816
(INFO) 2023-04-09 20:33:34,026 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:34,027 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,083 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:34,083 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,088 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,089 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:34,090 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,096 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,103 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:34,103 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,108 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,116 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:34,116 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,120 [api1.py:131] throughput: 0.00496125378551189
(INFO) 2023-04-09 20:33:34,125 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:33:34,125 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,125 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:34,125 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,125 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,135 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,144 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,158 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:34,164 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:34,164 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,225 [api1.py:131] throughput: 0.0038588046734744637
(INFO) 2023-04-09 20:33:34,239 [api1.py:131] throughput: 0.005788071398745061
(INFO) 2023-04-09 20:33:34,251 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:34,280 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:34,281 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,285 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,291 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:34,297 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:33:34,314 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:33:34,314 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,316 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:34,323 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,349 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 52}
(INFO) 2023-04-09 20:33:34,350 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,370 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:34,371 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,376 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:34,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,368 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:34,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,395 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:34,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,397 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:34,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,397 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:34,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,398 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:34,399 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,403 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,406 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,408 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:34,409 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,413 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:34,414 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,421 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:34,422 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,427 [api1.py:131] throughput: 0.011575342209205787
(INFO) 2023-04-09 20:33:34,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,451 [api1.py:131] throughput: 0.011575331866671992
(INFO) 2023-04-09 20:33:34,469 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:33:34,481 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:34,483 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,500 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:33:34,503 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,517 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:34,518 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,551 [api1.py:131] throughput: 0.008681808265861712
(INFO) 2023-04-09 20:33:34,570 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:34,571 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,616 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:34,624 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:34,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,629 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:33:34,632 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:34,635 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:34,640 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,642 [api1.py:131] throughput: 0.005788072389159903
(INFO) 2023-04-09 20:33:34,644 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,653 [api1.py:131] throughput: 0.00578807160311635
(INFO) 2023-04-09 20:33:34,660 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:34,662 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:33:34,680 [api1.py:131] throughput: 0.004961254123495958
(INFO) 2023-04-09 20:33:34,687 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 20:33:34,761 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:34,761 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,764 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:34,764 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,772 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:33:34,773 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,792 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:34,792 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,822 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:34,823 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,829 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,839 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:34,843 [api1.py:131] throughput: 0.005788073306210985
(INFO) 2023-04-09 20:33:34,846 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:34,846 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,852 [api1.py:131] throughput: 0.0026715182344679966
(INFO) 2023-04-09 20:33:34,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,869 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:33:34,870 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:34,871 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:34,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:34,914 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:34,926 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 20:33:34,943 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:33:34,983 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:34,997 [api1.py:131] throughput: 0.008681805842040405
(INFO) 2023-04-09 20:33:35,015 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:35,016 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,023 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,035 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:35,036 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,041 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,047 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:35,047 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,053 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,058 [api1.py:131] throughput: 0.008681806901898306
(INFO) 2023-04-09 20:33:35,058 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:35,059 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,062 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:35,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,067 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,106 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:35,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,107 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:35,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,112 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:35,113 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,113 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,150 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:35,175 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:35,193 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:35,199 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:33:35,203 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:35,206 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:33:35,208 [api1.py:131] throughput: 0.008681807780066478
(INFO) 2023-04-09 20:33:35,216 [api1.py:131] throughput: 0.006945588020386053
(INFO) 2023-04-09 20:33:35,226 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:35,226 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,232 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:33:35,239 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,242 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 20:33:35,266 [api1.py:131] throughput: 0.0038588044797708216
(INFO) 2023-04-09 20:33:35,271 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:35,272 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,275 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:35,275 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,288 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:35,288 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,301 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:33:35,301 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:35,302 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,308 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:33:35,309 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,312 [api1.py:131] throughput: 0.017361806706678782
(INFO) 2023-04-09 20:33:35,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,323 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:35,324 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,327 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:35,328 [api1.py:131] throughput: 0.006945590319684866
(INFO) 2023-04-09 20:33:35,340 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,349 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:33:35,401 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:35,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,411 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,426 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:35,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,456 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:35,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,467 [api1.py:131] throughput: 0.00868180764496367
(INFO) 2023-04-09 20:33:35,506 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:33:35,511 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:35,512 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,524 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:35,524 [api1.py:131] throughput: 0.017361804016257653
(INFO) 2023-04-09 20:33:35,550 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:35,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,555 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,616 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:35,616 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,620 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,679 [api1.py:131] throughput: 0.004341130263587637
(INFO) 2023-04-09 20:33:35,700 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:35,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,710 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,726 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:35,726 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,735 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:33:35,745 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:35,746 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,751 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,752 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:35,882 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:35,884 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:35,929 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 20:33:35,943 [api1.py:131] throughput: 0.0038588048730479334
(INFO) 2023-04-09 20:33:35,944 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:35,944 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,949 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,953 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 65}
(INFO) 2023-04-09 20:33:35,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,959 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:35,961 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,976 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:33:35,978 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:35,978 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,988 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:35,993 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:35,994 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:35,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:35,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,001 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:36,001 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,020 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:36,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,027 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:36,028 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,039 [api1.py:131] throughput: 0.0115753389908042
(INFO) 2023-04-09 20:33:36,055 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,070 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 62}
(INFO) 2023-04-09 20:33:36,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,074 [api1.py:131] throughput: 0.006945588615498541
(INFO) 2023-04-09 20:33:36,075 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:36,081 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:36,082 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,086 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:36,087 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,088 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,092 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,112 [api1.py:131] throughput: 0.003472940274200892
(INFO) 2023-04-09 20:33:36,121 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,121 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:36,122 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,127 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,148 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:36,150 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,164 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,167 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:36,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,190 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:33:36,191 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,202 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,213 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:36,222 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:36,223 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,233 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,263 [api1.py:131] throughput: 0.0049612545676238524
(INFO) 2023-04-09 20:33:36,266 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:36,267 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,330 [api1.py:131] throughput: 0.0057880734999380635
(INFO) 2023-04-09 20:33:36,361 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,361 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,364 [api1.py:131] throughput: 0.005788073375573406
(INFO) 2023-04-09 20:33:36,365 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,369 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:36,369 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,379 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:36,380 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,380 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,381 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:36,393 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:33:36,393 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:36,393 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,395 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:36,398 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:36,400 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,405 [api1.py:131] throughput: 0.006945589629895063
(INFO) 2023-04-09 20:33:36,409 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,411 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:36,411 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,413 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:36,414 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,417 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,434 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:33:36,420 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,467 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:36,468 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,473 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,475 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:36,510 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,521 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,521 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,526 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,548 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:36,553 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:36,553 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,553 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:36,555 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,555 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:36,556 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:36,557 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,557 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:33:36,557 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:36,557 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,559 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,559 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,565 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,585 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 62}
(INFO) 2023-04-09 20:33:36,586 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,586 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:36,586 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:36,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,602 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:36,607 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,608 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,616 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,621 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:36,622 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,622 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,687 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:36,687 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,694 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:33:36,694 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:36,694 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,696 [api1.py:131] throughput: 0.008681806981731769
(INFO) 2023-04-09 20:33:36,699 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:36,710 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:36,725 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:36,731 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:36,731 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,739 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:33:36,746 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:36,748 [api1.py:131] throughput: 0.008681808503263928
(INFO) 2023-04-09 20:33:36,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,762 [api1.py:131] throughput: 0.00694559097115314
(INFO) 2023-04-09 20:33:36,765 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:36,771 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:36,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,773 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:36,774 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,783 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:36,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,833 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:36,833 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,855 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:36,856 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,863 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,892 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 20:33:36,906 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:36,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,981 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:36,982 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:36,991 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:36,996 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:36,998 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,002 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:37,002 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,003 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,009 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:37,010 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,011 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,019 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:33:37,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,045 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:37,046 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,085 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:37,085 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,091 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,163 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:37,166 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:37,184 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:37,212 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:33:37,213 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,237 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:37,237 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,243 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,291 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:33:37,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,297 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:37,298 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,288 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:37,303 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,313 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,357 [api1.py:131] throughput: 0.005788070518376598
(INFO) 2023-04-09 20:33:37,377 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:37,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,382 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,384 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:37,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,395 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:37,395 [api1.py:131] throughput: 0.011575337387769596
(INFO) 2023-04-09 20:33:37,396 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,397 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:37,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,401 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:37,402 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,417 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:37,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,429 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:33:37,455 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:37,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,462 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:37,465 [api1.py:131] throughput: 0.011575341966037603
(INFO) 2023-04-09 20:33:37,465 [api1.py:131] throughput: 0.0057880730392393265
(INFO) 2023-04-09 20:33:37,468 [api1.py:131] throughput: 0.01157534156248192
(INFO) 2023-04-09 20:33:37,473 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,530 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:37,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,581 [api1.py:131] throughput: 0.0031572304449604444
(INFO) 2023-04-09 20:33:37,626 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:37,627 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:37,638 [api1.py:131] throughput: 0.004961254159565406
(INFO) 2023-04-09 20:33:37,645 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:37,664 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:37,665 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,672 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,724 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:37,731 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:37,732 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,738 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,765 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:37,766 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,775 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:37,777 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,792 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:37,800 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:37,801 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,822 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,832 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:37,833 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,856 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:37,856 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,860 [api1.py:131] throughput: 0.004961254243727455
(INFO) 2023-04-09 20:33:37,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,871 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 20:33:37,906 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:37,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,911 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:37,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:37,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:37,984 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:38,001 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:33:38,026 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:38,026 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,043 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:38,043 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,048 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,057 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:38,057 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,059 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,059 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,063 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,065 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:33:38,071 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:38,073 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:38,079 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:38,080 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,115 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:38,115 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,139 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:38,140 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,154 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,155 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:38,199 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:38,200 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,216 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:38,230 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:38,231 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,234 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:38,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,238 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,239 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,243 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:38,243 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,245 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:38,249 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:38,252 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,252 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,257 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,266 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 20:33:38,276 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:38,277 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,289 [api1.py:131] throughput: 0.017361806957399074
(INFO) 2023-04-09 20:33:38,314 [api1.py:131] throughput: 0.011575339969499231
(INFO) 2023-04-09 20:33:38,314 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:38,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,316 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:33:38,322 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,324 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 20:33:38,328 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,329 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,338 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:38,339 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,344 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,384 [api1.py:131] throughput: 0.004961254215673439
(INFO) 2023-04-09 20:33:38,404 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:33:38,422 [api1.py:131] throughput: 0.006945590462400015
(INFO) 2023-04-09 20:33:38,441 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:38,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,441 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:38,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,462 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:33:38,466 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:38,473 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:38,474 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,477 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:38,483 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:38,488 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:33:38,493 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,502 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:38,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,545 [api1.py:131] throughput: 0.003858804696319121
(INFO) 2023-04-09 20:33:38,562 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:38,579 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:33:38,589 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 63}
(INFO) 2023-04-09 20:33:38,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,614 [api1.py:131] throughput: 0.01157534342504685
(INFO) 2023-04-09 20:33:38,623 [api1.py:131] throughput: 0.00868180764496367
(INFO) 2023-04-09 20:33:38,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,654 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:38,654 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,657 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:38,657 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,659 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,660 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 20:33:38,661 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,665 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:38,693 [api1.py:131] throughput: 0.01736180289982464
(INFO) 2023-04-09 20:33:38,695 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:38,721 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,722 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,724 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:33:38,730 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:38,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,732 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,737 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:38,739 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,747 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:38,752 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,737 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,755 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,759 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,764 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:38,772 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,784 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:38,814 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:38,814 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,818 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,830 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,868 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:38,869 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,876 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:38,899 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:38,899 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,909 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,909 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:38,910 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,912 [api1.py:131] throughput: 0.0034729401950490285
(INFO) 2023-04-09 20:33:38,915 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,926 [api1.py:131] throughput: 0.0038588046610442826
(INFO) 2023-04-09 20:33:38,947 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 20:33:38,957 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:38,958 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:38,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:38,995 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 20:33:39,008 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:39,042 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:33:39,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,061 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:33:39,091 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:39,101 [api1.py:131] throughput: 0.00496125469960527
(INFO) 2023-04-09 20:33:39,104 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:39,117 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:39,118 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,126 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,140 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:39,142 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,148 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,183 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:39,183 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,190 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:39,190 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,198 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,209 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:39,209 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,268 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:39,269 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,276 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,288 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:39,288 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,297 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:39,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,298 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,303 [api1.py:131] throughput: 0.00434112989977363
(INFO) 2023-04-09 20:33:39,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,314 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:39,375 [api1.py:131] throughput: 0.011575337966170602
(INFO) 2023-04-09 20:33:39,402 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:39,411 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:39,418 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:39,419 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,449 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:39,450 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,468 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:39,469 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,471 [api1.py:131] throughput: 0.0069455915303366905
(INFO) 2023-04-09 20:33:39,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,517 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:39,585 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:39,586 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,590 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:39,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,595 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,604 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:39,605 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,618 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:39,618 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,638 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:39,639 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,639 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:39,647 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,651 [api1.py:131] throughput: 0.00694558872227711
(INFO) 2023-04-09 20:33:39,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,681 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:39,681 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,687 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,695 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:39,745 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:39,746 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,749 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:39,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,751 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,783 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:39,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,795 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:39,800 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:33:39,809 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:33:39,816 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 66}
(INFO) 2023-04-09 20:33:39,817 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,827 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,827 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:39,828 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,832 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,841 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:39,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,851 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,856 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:39,862 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:39,862 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,865 [api1.py:131] throughput: 0.004961254344721918
(INFO) 2023-04-09 20:33:39,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,884 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:39,887 [api1.py:131] throughput: 0.0049612539575765
(INFO) 2023-04-09 20:33:39,895 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:39,896 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,901 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:39,906 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:39,909 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,915 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:39,935 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:39,945 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:39,946 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,949 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 20:33:39,952 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,964 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:39,970 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:39,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:39,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:39,985 [api1.py:131] throughput: 0.005788072522787328
(INFO) 2023-04-09 20:33:39,980 [api1.py:131] throughput: 0.0026715182650234
(INFO) 2023-04-09 20:33:40,003 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 67}
(INFO) 2023-04-09 20:33:40,004 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,026 [api1.py:131] throughput: 0.008681806023730315
(INFO) 2023-04-09 20:33:40,036 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:40,037 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,037 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,047 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,093 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 20:33:40,098 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:40,098 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:40,099 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,113 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:40,113 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,115 [api1.py:131] throughput: 0.011575339886586523
(INFO) 2023-04-09 20:33:40,119 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:40,120 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,122 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,124 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,132 [api1.py:131] throughput: 0.004961254389392547
(INFO) 2023-04-09 20:33:40,135 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:33:40,143 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:33:40,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,145 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:40,146 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,151 [api1.py:131] throughput: 0.0026715181864864672
(INFO) 2023-04-09 20:33:40,152 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:40,158 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 20:33:40,206 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:40,207 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,212 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,217 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:40,217 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,226 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,254 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:40,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,315 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:40,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,321 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,332 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:33:40,333 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,341 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:33:40,342 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,350 [api1.py:131] throughput: 0.011575333837278531
(INFO) 2023-04-09 20:33:40,354 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:40,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,356 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:40,363 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:40,366 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,402 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:40,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,450 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:40,451 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,452 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:40,452 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,455 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,458 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,459 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:40,463 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:40,513 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:40,514 [api1.py:131] throughput: 0.011575342434361521
(INFO) 2023-04-09 20:33:40,524 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:40,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,529 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,539 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:40,540 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,546 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,550 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:40,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,555 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:40,557 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,576 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:40,577 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,577 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:40,577 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:40,578 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,578 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,581 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,584 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,592 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,599 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:40,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,600 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 61}
(INFO) 2023-04-09 20:33:40,600 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,608 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,609 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:40,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,614 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,612 [api1.py:131] throughput: 0.0038588046702727505
(INFO) 2023-04-09 20:33:40,631 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 20:33:40,641 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:40,660 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,666 [api1.py:131] throughput: 0.0057880720490173935
(INFO) 2023-04-09 20:33:40,670 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:40,672 [api1.py:131] throughput: 0.00496125462628226
(INFO) 2023-04-09 20:33:40,674 [api1.py:131] throughput: 0.004341130186648638
(INFO) 2023-04-09 20:33:40,710 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:40,710 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,714 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,720 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:33:40,724 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:40,762 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:40,762 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,772 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:40,817 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:40,818 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,823 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,828 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:40,834 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:40,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,903 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:40,905 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:40,911 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,940 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:33:40,946 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:40,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:40,954 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:40,998 [api1.py:131] throughput: 0.011575331866671992
(INFO) 2023-04-09 20:33:41,010 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:33:41,021 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:33:41,049 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:41,067 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:41,068 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,101 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:41,102 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,104 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,170 [api1.py:131] throughput: 0.0049612539575765
(INFO) 2023-04-09 20:33:41,197 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:41,198 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,202 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,218 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:41,252 [api1.py:131] throughput: 0.0031572303252307564
(INFO) 2023-04-09 20:33:41,278 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:41,279 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,287 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,294 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,295 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,300 [api1.py:131] throughput: 0.005788071943735196
(INFO) 2023-04-09 20:33:41,302 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,305 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,375 [api1.py:131] throughput: 0.017361807884304456
(INFO) 2023-04-09 20:33:41,382 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:41,384 [api1.py:131] throughput: 0.01736179912392218
(INFO) 2023-04-09 20:33:41,407 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:33:41,424 [api1.py:131] throughput: 0.011575338409704105
(INFO) 2023-04-09 20:33:41,438 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:41,438 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:41,439 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,439 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,444 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,457 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:41,458 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,467 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:41,469 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,472 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 20:33:41,479 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:41,480 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,481 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,481 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,481 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:41,482 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,482 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:41,483 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,509 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:41,514 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:41,521 [api1.py:131] throughput: 0.01157533490773175
(INFO) 2023-04-09 20:33:41,536 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,537 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,544 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:33:41,565 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:41,565 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,571 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,608 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:33:41,611 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:41,611 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,617 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,678 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:41,679 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:41,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,684 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,729 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:41,733 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:41,733 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,733 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,772 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:41,773 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,776 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:41,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,830 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,830 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,835 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:41,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,835 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,839 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:41,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,842 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:41,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,867 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:41,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,878 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,884 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:33:41,887 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:41,887 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:41,914 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 52}
(INFO) 2023-04-09 20:33:41,915 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,920 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:41,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,963 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:33:41,964 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,963 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:41,973 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,974 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 46}
(INFO) 2023-04-09 20:33:41,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,989 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:41,989 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:41,990 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,990 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:41,995 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:41,995 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,072 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:42,075 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:42,075 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,094 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:42,094 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,100 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,115 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:42,116 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,123 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,165 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:42,165 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,172 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:42,173 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,177 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,178 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,199 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:42,199 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,201 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:42,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,204 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,211 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:33:42,233 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:42,234 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:42,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,240 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:42,244 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:42,245 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,268 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:42,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,274 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,290 [api1.py:131] throughput: 0.005788072389159903
(INFO) 2023-04-09 20:33:42,299 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:42,299 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,306 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,335 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:42,335 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,340 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,355 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:42,358 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:42,376 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:42,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,383 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,389 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:42,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,396 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:33:42,399 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:42,416 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:42,417 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,420 [api1.py:131] throughput: 0.006945591392691501
(INFO) 2023-04-09 20:33:42,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,442 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 20:33:42,447 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:42,453 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:42,478 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:42,479 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,481 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:42,482 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,482 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:42,482 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,498 [api1.py:131] throughput: 0.002315325646562217
(INFO) 2023-04-09 20:33:42,503 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:33:42,509 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:42,518 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:42,524 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:42,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,529 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,569 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:42,572 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:42,572 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,578 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,584 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:42,585 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,591 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,681 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:42,682 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,686 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,725 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:42,726 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,727 [api1.py:131] throughput: 0.008681807447786608
(INFO) 2023-04-09 20:33:42,730 [api1.py:131] throughput: 0.005788073101839575
(INFO) 2023-04-09 20:33:42,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,741 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:42,742 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:42,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,793 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,824 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:42,842 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:42,851 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:42,891 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:42,909 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:42,910 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,938 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:42,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,944 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:33:42,945 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,947 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:33:42,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,956 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,956 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:42,961 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 20:33:42,964 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:42,964 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:42,967 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:42,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,012 [api1.py:131] throughput: 0.004341129863975001
(INFO) 2023-04-09 20:33:43,026 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:43,029 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 20:33:43,029 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:43,030 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,047 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:43,081 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:43,081 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,090 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:43,090 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,143 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:43,143 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,155 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,159 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:43,160 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,174 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:43,174 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,189 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:43,202 [api1.py:131] throughput: 0.0069455903689555724
(INFO) 2023-04-09 20:33:43,212 [api1.py:131] throughput: 0.002671518244437273
(INFO) 2023-04-09 20:33:43,219 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:43,219 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,219 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:43,220 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,224 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,230 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,244 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:43,244 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,250 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,260 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:43,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,266 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,285 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:43,293 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:43,294 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,334 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:43,335 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,340 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,380 [api1.py:131] throughput: 0.008681809169407528
(INFO) 2023-04-09 20:33:43,390 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:43,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,395 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,421 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:43,421 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,432 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,452 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:43,453 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,456 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:33:43,458 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,468 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:43,469 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,471 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:43,472 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,493 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:43,495 [api1.py:131] throughput: 0.011575342209205787
(INFO) 2023-04-09 20:33:43,502 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:43,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,526 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:43,526 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,536 [api1.py:131] throughput: 0.01736179662366335
(INFO) 2023-04-09 20:33:43,537 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:43,539 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,562 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:43,562 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,591 [api1.py:131] throughput: 0.001929446980369796
(INFO) 2023-04-09 20:33:43,595 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:43,595 [api1.py:131] throughput: 0.004961254296328738
(INFO) 2023-04-09 20:33:43,622 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:43,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,630 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,650 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:43,650 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,656 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:43,658 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,666 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:43,667 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,675 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:43,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,678 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:43,679 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:43,700 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:33:43,714 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:33:43,715 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,744 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,755 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:43,755 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,756 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:43,762 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,794 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:43,795 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:43,795 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,796 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,798 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:33:43,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,803 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:43,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,804 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,808 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:43,809 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,812 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 20:33:43,821 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:43,822 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,825 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:43,828 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 20:33:43,832 [api1.py:131] throughput: 0.004341130203119931
(INFO) 2023-04-09 20:33:43,836 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:43,836 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,843 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,847 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:43,847 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:43,852 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,865 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:43,891 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:43,985 [api1.py:131] throughput: 0.017361805010898096
(INFO) 2023-04-09 20:33:43,998 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:44,000 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,031 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:44,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,037 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:44,037 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,037 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,043 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:44,049 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,063 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:33:44,064 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,091 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:44,091 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,097 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,125 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:44,126 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,131 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,162 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:33:44,171 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:44,182 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:44,188 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,229 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:44,294 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:44,294 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,295 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:44,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,302 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:44,302 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,306 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 20:33:44,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,309 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:44,312 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,320 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,339 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:44,339 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:44,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,346 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:44,346 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,352 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,366 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:44,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,374 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,397 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:44,398 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,400 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:44,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,402 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,435 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:44,446 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:44,447 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,448 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 20:33:44,471 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:33:44,479 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:44,486 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:44,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,508 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:44,536 [api1.py:131] throughput: 0.004341129975883322
(INFO) 2023-04-09 20:33:44,536 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:44,553 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:33:44,601 [api1.py:131] throughput: 0.004961255045270922
(INFO) 2023-04-09 20:33:44,610 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:33:44,610 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,616 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:33:44,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,673 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:44,704 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:44,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,710 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,719 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:44,730 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:44,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,734 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,740 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:44,754 [api1.py:131] throughput: 0.005788072522787328
(INFO) 2023-04-09 20:33:44,816 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:44,817 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,822 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,833 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:44,834 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,839 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,840 [api1.py:131] throughput: 0.006945590910933379
(INFO) 2023-04-09 20:33:44,864 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:44,865 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,871 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,910 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:44,922 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:44,922 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,942 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:33:44,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,950 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:44,951 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,959 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,962 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:44,968 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:33:44,968 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:44,980 [api1.py:131] throughput: 0.008681807548969573
(INFO) 2023-04-09 20:33:44,986 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:44,986 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:44,990 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,016 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:45,033 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:45,043 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:45,044 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,050 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,061 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 20:33:45,062 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:45,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,069 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:45,072 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,111 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:45,111 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,120 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,146 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:45,165 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:45,166 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,178 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,239 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:45,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,253 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:45,254 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,333 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:45,333 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,339 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,346 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:33:45,366 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:45,389 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:45,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,390 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:45,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,391 [api1.py:131] throughput: 0.0038588045271808025
(INFO) 2023-04-09 20:33:45,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,396 [api1.py:131] throughput: 0.004341129560739578
(INFO) 2023-04-09 20:33:45,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,446 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:45,446 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,480 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:33:45,481 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,495 [api1.py:131] throughput: 0.00868180764496367
(INFO) 2023-04-09 20:33:45,500 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:45,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,505 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,507 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:45,508 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,529 [api1.py:131] throughput: 0.005788072969484766
(INFO) 2023-04-09 20:33:45,531 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 46}
(INFO) 2023-04-09 20:33:45,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,558 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,595 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:45,596 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,600 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,610 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:45,612 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,623 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:45,638 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,643 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:45,644 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,686 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:45,687 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,701 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,704 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:45,704 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,705 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:45,706 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:45,706 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:45,706 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,707 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,708 [api1.py:131] throughput: 0.006945590013111603
(INFO) 2023-04-09 20:33:45,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,714 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:45,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,726 [api1.py:131] throughput: 0.0018278993500276794
(INFO) 2023-04-09 20:33:45,746 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:33:45,747 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:45,753 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:45,754 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,760 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 59}
(INFO) 2023-04-09 20:33:45,761 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,790 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:45,790 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,813 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:45,814 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:45,833 [api1.py:131] throughput: 0.008681804107728034
(INFO) 2023-04-09 20:33:45,848 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 20:33:45,867 [api1.py:131] throughput: 0.004341129886212267
(INFO) 2023-04-09 20:33:45,877 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:45,885 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 20:33:45,899 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:45,904 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:45,909 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,914 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:45,914 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,915 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,920 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:45,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,923 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:45,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,924 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:45,925 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,925 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,925 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:45,925 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,925 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:45,927 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:45,927 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,931 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,939 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:45,963 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:33:45,971 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:45,972 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:45,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,028 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:46,028 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,050 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:46,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,091 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:46,105 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:46,112 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,163 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:46,193 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:33:46,193 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,201 [api1.py:131] throughput: 0.017361798545737262
(INFO) 2023-04-09 20:33:46,216 [api1.py:131] throughput: 0.00868180560220974
(INFO) 2023-04-09 20:33:46,222 [api1.py:131] throughput: 0.00868180560220974
(INFO) 2023-04-09 20:33:46,231 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:46,240 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:46,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,243 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,245 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,258 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:46,262 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:46,263 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,271 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,272 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:46,282 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:46,283 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,283 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:46,284 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,290 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,290 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,295 [api1.py:131] throughput: 0.00578807160311635
(INFO) 2023-04-09 20:33:46,318 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:46,325 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:46,326 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,331 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,353 [api1.py:131] throughput: 0.011575338409704105
(INFO) 2023-04-09 20:33:46,370 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:46,371 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,377 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:46,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,383 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,390 [api1.py:131] throughput: 0.004341129621918651
(INFO) 2023-04-09 20:33:46,394 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:46,398 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:46,403 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:33:46,417 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:33:46,422 [api1.py:131] throughput: 0.003472940259240793
(INFO) 2023-04-09 20:33:46,434 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:46,434 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,449 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:46,449 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,462 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:46,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,463 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,493 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:46,494 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,499 [api1.py:131] throughput: 0.005788072458448197
(INFO) 2023-04-09 20:33:46,626 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:46,654 [api1.py:131] throughput: 0.006945591028117781
(INFO) 2023-04-09 20:33:46,659 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:46,668 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:46,690 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:46,690 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,700 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,745 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 20:33:46,748 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:46,770 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:46,770 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,776 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,781 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:33:46,790 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:46,790 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,833 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:46,904 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:46,904 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,913 [api1.py:131] throughput: 0.008681808658234827
(INFO) 2023-04-09 20:33:46,915 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 20:33:46,917 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:46,917 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,926 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:46,927 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,928 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:46,928 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,936 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,944 [api1.py:131] throughput: 0.004341129560739578
(INFO) 2023-04-09 20:33:46,952 [api1.py:131] throughput: 0.004961253570431144
(INFO) 2023-04-09 20:33:46,954 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:46,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,961 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 20:33:46,964 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,970 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:46,970 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,971 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:46,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,975 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:46,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,976 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:46,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:46,985 [api1.py:131] throughput: 0.011575340434531399
(INFO) 2023-04-09 20:33:46,996 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 20:33:47,015 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:47,041 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 20:33:47,093 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:47,093 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,112 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:47,114 [api1.py:131] throughput: 0.017361806179741247
(INFO) 2023-04-09 20:33:47,115 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:47,124 [api1.py:131] throughput: 0.006945589629895063
(INFO) 2023-04-09 20:33:47,138 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:47,139 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,155 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:47,161 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,162 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,166 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,183 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:47,225 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:47,226 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,231 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,294 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,295 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,302 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:47,313 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:47,313 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,318 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:47,321 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,336 [api1.py:131] throughput: 0.0019294469769735815
(INFO) 2023-04-09 20:33:47,340 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:47,366 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,366 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:47,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,368 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:47,369 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,370 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,376 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 20:33:47,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,398 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,400 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,431 [api1.py:131] throughput: 0.00868180482622879
(INFO) 2023-04-09 20:33:47,451 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:47,451 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,493 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:47,509 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:47,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,539 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:47,562 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:47,562 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:33:47,563 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,593 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:47,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,598 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,613 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:47,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,629 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,630 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,650 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:47,651 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,658 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,672 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:47,678 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:47,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,696 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:47,697 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,707 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,729 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:47,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,785 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:47,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,826 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 20:33:47,829 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:47,830 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,835 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,838 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,838 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,849 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:47,854 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,854 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,857 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:47,863 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,864 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 20:33:47,867 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:47,870 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:47,896 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:33:47,908 [api1.py:131] throughput: 0.004341130089104357
(INFO) 2023-04-09 20:33:47,911 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:47,916 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,921 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:33:47,925 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,930 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:47,930 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,937 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,941 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:47,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,949 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,950 [api1.py:131] throughput: 0.00496125387607221
(INFO) 2023-04-09 20:33:47,953 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 20:33:47,958 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:47,961 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:47,962 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:47,970 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:47,970 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:47,972 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:47,974 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,055 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:48,085 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:33:48,085 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,090 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:48,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,098 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,099 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:48,102 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,103 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:48,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,183 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:48,183 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,191 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,213 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,214 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,234 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:48,236 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,237 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:33:48,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,261 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 67}
(INFO) 2023-04-09 20:33:48,261 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,276 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:48,353 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:48,353 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,355 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:48,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,364 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:48,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,370 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,383 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:48,383 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:48,386 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,389 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,389 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,396 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:48,402 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,478 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 20:33:48,533 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:48,533 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,539 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,566 [api1.py:131] throughput: 0.008681806502731015
(INFO) 2023-04-09 20:33:48,622 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:48,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,637 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,637 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,640 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 20:33:48,650 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,655 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:48,656 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:48,656 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:48,657 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:48,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,705 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 20:33:48,724 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:48,724 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,727 [api1.py:131] throughput: 0.0038588047813790153
(INFO) 2023-04-09 20:33:48,729 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,749 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:48,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,755 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:33:48,759 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,780 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 20:33:48,784 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:48,785 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,813 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:48,814 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,816 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:48,828 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,830 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:48,830 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,833 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:48,835 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,841 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,859 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,864 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,877 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:48,878 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:33:48,894 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:48,895 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,899 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,916 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:48,917 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,924 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:48,925 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,925 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:48,926 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,929 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,932 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,933 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:48,945 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:48,976 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:48,977 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:48,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,011 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:33:49,012 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,016 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:49,016 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,025 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:49,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,029 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:49,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,046 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:33:49,096 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:49,096 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,113 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:49,167 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:49,191 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:49,192 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,196 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,219 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:49,219 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,225 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,235 [api1.py:131] throughput: 0.011575332580659792
(INFO) 2023-04-09 20:33:49,235 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:49,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,275 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:49,275 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,308 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:49,309 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,361 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:49,361 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,369 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,377 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:49,393 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:49,402 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:49,409 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:49,409 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,424 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:49,484 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:49,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,488 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:49,489 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,509 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:49,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,515 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 65}
(INFO) 2023-04-09 20:33:49,515 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,533 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,537 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:49,554 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:49,554 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,557 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:49,559 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:49,560 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,560 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,565 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,571 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:49,572 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 20:33:49,586 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 40, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:49,586 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,597 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,598 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:49,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,607 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,605 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:49,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,627 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:49,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,635 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:49,636 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,669 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:49,670 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,677 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,740 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:49,741 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,748 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:49,765 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:49,766 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,781 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:33:49,786 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:49,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,822 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 20:33:49,873 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:49,874 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,895 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:49,895 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,898 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:49,903 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,913 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:49,913 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,940 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:49,940 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,959 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:49,960 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:49,962 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:49,965 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:49,987 [api1.py:131] throughput: 0.008681806901898306
(INFO) 2023-04-09 20:33:50,008 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:50,012 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:50,012 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,022 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:33:50,065 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:50,066 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,086 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,105 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:50,139 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:50,149 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:50,150 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,154 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,159 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:50,186 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:50,187 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:50,221 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:33:50,233 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:50,233 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,239 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,250 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:50,267 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:50,269 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,274 [api1.py:131] throughput: 0.011575342896194843
(INFO) 2023-04-09 20:33:50,277 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:50,317 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:50,322 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:50,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,323 [api1.py:131] throughput: 0.008681808310281307
(INFO) 2023-04-09 20:33:50,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,340 [api1.py:131] throughput: 0.008681805107381292
(INFO) 2023-04-09 20:33:50,398 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:50,400 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,402 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:50,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,423 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:33:50,440 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:50,457 [api1.py:131] throughput: 0.01736180469163078
(INFO) 2023-04-09 20:33:50,458 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:50,490 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:50,490 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:50,504 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:50,505 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,532 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:50,533 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,534 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:50,535 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,540 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,562 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:50,562 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,564 [api1.py:131] throughput: 0.0069455907138505305
(INFO) 2023-04-09 20:33:50,566 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,722 [api1.py:131] throughput: 0.008681808658234827
(INFO) 2023-04-09 20:33:50,735 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:50,747 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:33:50,764 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:50,765 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,768 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:50,768 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,774 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,798 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:50,799 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,809 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,812 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:50,813 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,819 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,821 [api1.py:131] throughput: 0.011575337340580706
(INFO) 2023-04-09 20:33:50,894 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:50,895 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,909 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:33:50,913 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:33:50,990 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:50,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,992 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:50,992 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:50,996 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:50,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,004 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:51,005 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,033 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:51,034 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,035 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:51,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,039 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,043 [api1.py:131] throughput: 0.008681806193698297
(INFO) 2023-04-09 20:33:51,045 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,055 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:51,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,059 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,063 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:33:51,063 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,071 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:51,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,072 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,105 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:51,105 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,111 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:51,111 [api1.py:131] throughput: 0.0023153256541639158
(INFO) 2023-04-09 20:33:51,113 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,114 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,115 [api1.py:131] throughput: 0.008681807529161267
(INFO) 2023-04-09 20:33:51,116 [api1.py:131] throughput: 0.011575340385444667
(INFO) 2023-04-09 20:33:51,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,137 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:51,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,143 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:51,144 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:33:51,152 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,159 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:51,184 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:51,226 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:51,257 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:33:51,283 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:33:51,284 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,285 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:51,293 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,314 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:51,314 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:51,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,319 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:51,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,365 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:51,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,366 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 20:33:51,371 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,388 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:51,434 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:51,434 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,435 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:51,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,435 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:51,443 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,446 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,464 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:51,465 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,477 [api1.py:131] throughput: 0.003472940161883284
(INFO) 2023-04-09 20:33:51,495 [api1.py:131] throughput: 0.003472940283310523
(INFO) 2023-04-09 20:33:51,510 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:51,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,514 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:33:51,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,515 [api1.py:131] throughput: 0.011575333837278531
(INFO) 2023-04-09 20:33:51,523 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:33:51,524 [api1.py:131] throughput: 0.00192944696733781
(INFO) 2023-04-09 20:33:51,526 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:51,527 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,535 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:51,535 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,535 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,567 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:51,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,597 [api1.py:131] throughput: 0.003858804803343962
(INFO) 2023-04-09 20:33:51,771 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:51,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,788 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:51,789 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,807 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:51,832 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:51,833 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,858 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:51,858 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,862 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,872 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:51,873 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,876 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:51,878 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,898 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:51,918 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 20:33:51,922 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 20:33:51,969 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:51,982 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:51,983 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:51,983 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:51,990 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:51,998 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:52,002 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:52,007 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:52,008 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:52,009 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,013 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:52,013 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,014 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,024 [api1.py:131] throughput: 0.0038588046427999855
(INFO) 2023-04-09 20:33:52,058 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 57}
(INFO) 2023-04-09 20:33:52,059 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,060 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:52,066 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 41, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:52,066 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,075 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:52,076 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,084 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,089 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,091 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:52,095 [api1.py:131] throughput: 0.0069455906312028875
(INFO) 2023-04-09 20:33:52,099 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:52,099 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,101 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:52,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,111 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,116 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:33:52,116 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,131 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,134 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:52,135 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,136 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:52,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,143 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:52,144 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,148 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,159 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:52,159 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,165 [api1.py:131] throughput: 0.003157230466346254
(INFO) 2023-04-09 20:33:52,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,190 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:52,190 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,196 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,198 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:52,199 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,200 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:52,204 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:33:52,205 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,208 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:52,208 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,212 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,222 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:33:52,225 [api1.py:131] throughput: 0.008681804107728034
(INFO) 2023-04-09 20:33:52,267 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 58}
(INFO) 2023-04-09 20:33:52,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,275 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:52,275 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,304 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:52,320 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:52,326 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:52,326 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,333 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,369 [api1.py:131] throughput: 0.011575339969499231
(INFO) 2023-04-09 20:33:52,434 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:52,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,453 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:52,481 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:52,482 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,486 [api1.py:131] throughput: 0.0028941369955520625
(INFO) 2023-04-09 20:33:52,488 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,517 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:52,541 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:52,542 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,547 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,562 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:52,568 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:52,568 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,579 [api1.py:131] throughput: 0.0069455901226020515
(INFO) 2023-04-09 20:33:52,586 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:33:52,596 [api1.py:131] throughput: 0.0020429410761932097
(INFO) 2023-04-09 20:33:52,598 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 34, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:52,606 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,628 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:52,629 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,644 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:52,665 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 20:33:52,676 [api1.py:131] throughput: 0.006945586755772356
(INFO) 2023-04-09 20:33:52,702 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:52,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,707 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,742 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:33:52,743 [api1.py:131] throughput: 0.011575342434361521
(INFO) 2023-04-09 20:33:52,785 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:52,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,814 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:52,815 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,829 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:52,830 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,853 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:52,854 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,854 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:52,855 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,858 [api1.py:131] throughput: 0.006945589137188145
(INFO) 2023-04-09 20:33:52,860 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,861 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,879 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:52,879 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:33:52,880 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,893 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,924 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:33:52,925 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:52,938 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:52,982 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:53,009 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:53,010 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,010 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:33:53,015 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,021 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:53,030 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:53,031 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,030 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:53,031 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:33:53,031 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,035 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,074 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:53,093 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:53,094 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,099 [api1.py:131] throughput: 0.0019294469808719247
(INFO) 2023-04-09 20:33:53,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,111 [api1.py:131] throughput: 0.00868180482622879
(INFO) 2023-04-09 20:33:53,114 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:53,114 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,127 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:53,131 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:53,134 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:53,134 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,156 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,168 [api1.py:131] throughput: 0.005788072785992881
(INFO) 2023-04-09 20:33:53,179 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:33:53,180 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,195 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:33:53,207 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,272 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:53,277 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:53,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,285 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:33:53,288 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,294 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:53,294 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,298 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:53,299 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,301 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:53,302 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,318 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:53,318 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,324 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,336 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:53,338 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:53,339 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,351 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:33:53,351 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:53,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,362 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:53,362 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,368 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,368 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,394 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 20:33:53,458 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:53,459 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,480 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:53,481 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,495 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:53,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,500 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,522 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:53,523 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,532 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:53,565 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:53,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,577 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,589 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:53,604 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:53,605 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,608 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:53,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,617 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:53,617 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,640 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:53,640 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,640 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:53,641 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,644 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,647 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,693 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:53,694 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,711 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:53,712 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,715 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:33:53,717 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,728 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:53,729 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,771 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:33:53,772 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,822 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:53,835 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:53,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,845 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:53,848 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,848 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:53,853 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,863 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,920 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:53,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,932 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:53,947 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:53,954 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:53,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,961 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:53,961 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,962 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,963 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:53,966 [api1.py:131] throughput: 0.017361805902644797
(INFO) 2023-04-09 20:33:53,966 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,972 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:53,972 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:53,980 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:53,983 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:53,998 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:53,999 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,000 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:54,003 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:54,003 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,003 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:54,004 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,005 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:54,006 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,006 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:54,006 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,012 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,017 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:54,018 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,022 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:33:54,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,024 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,028 [api1.py:131] throughput: 0.011575337966170602
(INFO) 2023-04-09 20:33:54,110 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:54,104 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:54,139 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:33:54,140 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,148 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,158 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:54,159 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,185 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:33:54,205 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:54,247 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:54,256 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:54,257 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,268 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,269 [api1.py:131] throughput: 0.0173618036587072
(INFO) 2023-04-09 20:33:54,273 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:54,273 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,282 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:33:54,309 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:54,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,365 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,367 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:54,368 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,372 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,384 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:54,394 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:54,395 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,395 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:54,396 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,402 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,403 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,426 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:33:54,442 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:33:54,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,449 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:33:54,452 [api1.py:131] throughput: 0.005788072829344386
(INFO) 2023-04-09 20:33:54,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,528 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:54,550 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:54,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,558 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,580 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:33:54,626 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:33:54,644 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 20:33:54,647 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:54,647 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,715 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:54,718 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:33:54,721 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 20:33:54,751 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:54,752 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,757 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,887 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:54,888 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:54,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:54,927 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 20:33:54,927 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:55,003 [api1.py:131] throughput: 0.002042941088224706
(INFO) 2023-04-09 20:33:55,031 [api1.py:131] throughput: 0.003858804391723718
(INFO) 2023-04-09 20:33:55,032 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:55,033 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,034 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:55,034 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,040 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,047 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,048 [api1.py:131] throughput: 0.005788072145144621
(INFO) 2023-04-09 20:33:55,052 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:55,054 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:55,059 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:55,060 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,063 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:55,064 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,068 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,110 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:55,111 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,164 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:55,164 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,167 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:55,169 [api1.py:131] throughput: 0.0069455915303366905
(INFO) 2023-04-09 20:33:55,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,172 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:33:55,175 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:33:55,190 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:33:55,191 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,201 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,204 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:55,204 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,216 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:55,217 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,221 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,237 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:55,237 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,238 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:33:55,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,245 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 20:33:55,250 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:33:55,251 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:55,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,283 [api1.py:131] throughput: 0.008681808796893004
(INFO) 2023-04-09 20:33:55,303 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 20:33:55,351 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:55,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,358 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,360 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:33:55,376 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:33:55,404 [api1.py:131] throughput: 0.005788073511630467
(INFO) 2023-04-09 20:33:55,410 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:55,412 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,418 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,450 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:55,450 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,451 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:55,451 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,498 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:55,498 [api1.py:131] throughput: 0.004961254750752543
(INFO) 2023-04-09 20:33:55,542 [api1.py:131] throughput: 0.008681808447474406
(INFO) 2023-04-09 20:33:55,551 [api1.py:131] throughput: 0.005788072785992881
(INFO) 2023-04-09 20:33:55,606 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:55,607 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,622 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:55,623 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:55,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,627 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:55,628 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,631 [api1.py:131] throughput: 0.011575340601635167
(INFO) 2023-04-09 20:33:55,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,637 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,641 [api1.py:131] throughput: 0.008681806643613585
(INFO) 2023-04-09 20:33:55,663 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:33:55,667 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:33:55,680 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:55,713 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:55,714 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,722 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:55,723 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,700 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:55,743 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:55,746 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:55,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,752 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,757 [api1.py:131] throughput: 0.005788072233261248
(INFO) 2023-04-09 20:33:55,764 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:33:55,786 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:55,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,793 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,815 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:33:55,852 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:55,856 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:55,862 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:33:55,886 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:55,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,890 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,896 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:55,897 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:55,911 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,911 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,903 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:55,911 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,916 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:33:55,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,920 [api1.py:131] throughput: 0.004961254159565406
(INFO) 2023-04-09 20:33:55,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,922 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:55,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,941 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 64}
(INFO) 2023-04-09 20:33:55,941 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,944 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:33:55,944 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,945 [api1.py:131] throughput: 0.006945589316354288
(INFO) 2023-04-09 20:33:55,949 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:55,950 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:55,957 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:55,968 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 20:33:55,972 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,058 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:56,060 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,072 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,073 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,077 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,120 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:56,137 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:33:56,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,138 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:56,139 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,156 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,159 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:56,162 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,174 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:33:56,203 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:56,203 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,223 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,224 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,230 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,231 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,232 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,237 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,275 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 20:33:56,293 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:56,308 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,314 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,342 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:56,343 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:56,401 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:56,402 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,425 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:33:56,426 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:56,427 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,432 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,438 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,438 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,443 [api1.py:131] throughput: 0.0038588046734744637
(INFO) 2023-04-09 20:33:56,444 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,445 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:56,455 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:56,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,458 [api1.py:131] throughput: 0.006945589917307465
(INFO) 2023-04-09 20:33:56,461 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:56,462 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:56,462 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,466 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:56,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,473 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:33:56,506 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,507 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,513 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,523 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,524 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,530 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,534 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:56,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,555 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:56,556 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,569 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:56,590 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:56,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,591 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:56,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,595 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,598 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,607 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:56,618 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 20:33:56,636 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:56,637 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,651 [api1.py:131] throughput: 0.004961254669087043
(INFO) 2023-04-09 20:33:56,666 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:56,667 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,671 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,677 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,680 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:56,684 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:56,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,685 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:56,686 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,696 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,717 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:33:56,723 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:56,725 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,728 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:56,729 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,731 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:56,732 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,748 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:33:56,767 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:56,774 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:33:56,786 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:56,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,805 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:33:56,815 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:56,823 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:56,824 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,832 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,861 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:56,867 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:56,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:56,944 [api1.py:131] throughput: 0.008681805213113862
(INFO) 2023-04-09 20:33:56,949 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:56,949 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:56,953 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:56,961 [api1.py:131] throughput: 0.006945591647205251
(INFO) 2023-04-09 20:33:56,961 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,028 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:33:57,047 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:33:57,057 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:57,057 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:57,059 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:57,060 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,068 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:33:57,071 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,102 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:57,118 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:57,119 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,119 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:57,120 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,123 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,139 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,140 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:57,141 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,152 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,155 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:57,156 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,163 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,215 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:57,215 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,218 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:57,219 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,222 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,250 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:57,269 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:33:57,300 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:57,310 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:57,311 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,328 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:57,333 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:57,334 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,339 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 20:33:57,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,389 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:57,400 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:57,427 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:57,427 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,430 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:33:57,430 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,436 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,460 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,471 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:33:57,562 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:57,562 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,566 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:57,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,567 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 57}
(INFO) 2023-04-09 20:33:57,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,568 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,573 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,575 [api1.py:131] throughput: 0.008681805821076185
(INFO) 2023-04-09 20:33:57,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,634 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:57,635 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,664 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,695 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:57,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,701 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,702 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:57,703 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,711 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:57,711 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,713 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:33:57,713 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,721 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,729 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:57,737 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:57,738 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,749 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 20:33:57,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,758 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:57,759 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,767 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:33:57,767 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,782 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:57,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,793 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,795 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:33:57,796 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,819 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:57,819 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,824 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,834 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:57,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,852 [api1.py:131] throughput: 0.003858804575644339
(INFO) 2023-04-09 20:33:57,874 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:57,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,911 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:33:57,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,926 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:57,927 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,936 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,941 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:57,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,944 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:33:57,947 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,952 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:33:57,952 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,961 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:57,966 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:57,967 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:57,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,000 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 20:33:58,035 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:58,036 [api1.py:131] throughput: 0.008681805842040405
(INFO) 2023-04-09 20:33:58,040 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:58,040 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,065 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:58,066 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,076 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:33:58,076 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,078 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,090 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:58,091 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,101 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:58,104 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:33:58,108 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,113 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,135 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:58,135 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,137 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:33:58,164 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 20:33:58,166 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,168 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:58,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,185 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:58,219 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:33:58,222 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:33:58,239 [api1.py:131] throughput: 0.00578807160311635
(INFO) 2023-04-09 20:33:58,242 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:58,245 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,245 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:33:58,250 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,269 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:33:58,279 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:58,279 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,288 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,289 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:58,304 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:58,327 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:33:58,357 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:33:58,363 [api1.py:131] throughput: 0.011575337751124067
(INFO) 2023-04-09 20:33:58,375 [api1.py:131] throughput: 0.003858804827156706
(INFO) 2023-04-09 20:33:58,396 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:33:58,396 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,406 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,406 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:58,408 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:58,409 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,413 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,416 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:33:58,417 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:33:58,422 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 62}
(INFO) 2023-04-09 20:33:58,423 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,434 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:58,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,435 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:33:58,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,435 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:33:58,436 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:33:58,436 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,436 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:58,437 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,438 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:33:58,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,441 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:58,441 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,441 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,441 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:33:58,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,446 [api1.py:131] throughput: 0.00204294107022637
(INFO) 2023-04-09 20:33:58,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,455 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,525 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:58,547 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:33:58,550 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:58,550 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,557 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,616 [api1.py:131] throughput: 0.005788072378024285
(INFO) 2023-04-09 20:33:58,624 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:33:58,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,627 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:58,627 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,635 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,656 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:58,775 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:33:58,776 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:58,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,822 [api1.py:131] throughput: 0.017361807435334647
(INFO) 2023-04-09 20:33:58,837 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:58,838 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,843 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,870 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:58,870 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,872 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:58,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,877 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,947 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:58,948 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,949 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:58,950 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,952 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,955 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,960 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:33:58,961 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,989 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:58,990 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:58,996 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:58,998 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:58,999 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,004 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,004 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:59,005 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,015 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:59,031 [api1.py:131] throughput: 0.008681806703602293
(INFO) 2023-04-09 20:33:59,042 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:33:59,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,042 [api1.py:131] throughput: 0.008681806703602293
(INFO) 2023-04-09 20:33:59,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,051 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:33:59,052 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,100 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:59,127 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:59,128 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,135 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:33:59,135 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,159 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:59,159 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,164 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,174 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:33:59,175 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,179 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,186 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:59,192 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:33:59,197 [api1.py:131] throughput: 0.0038588045044240116
(INFO) 2023-04-09 20:33:59,207 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:33:59,216 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:59,238 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:33:59,245 [api1.py:131] throughput: 0.004961254054362849
(INFO) 2023-04-09 20:33:59,266 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:33:59,367 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:59,369 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,371 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 20:33:59,373 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:33:59,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,374 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:59,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,378 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,393 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:33:59,412 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:33:59,446 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:33:59,483 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:33:59,483 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,522 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:33:59,523 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,526 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:59,527 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 20:33:59,536 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,538 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:59,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,544 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:33:59,546 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,558 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:33:59,565 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:33:59,565 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,571 [api1.py:131] throughput: 0.0038588044529738767
(INFO) 2023-04-09 20:33:59,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,621 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:33:59,663 [api1.py:131] throughput: 0.00868180376558486
(INFO) 2023-04-09 20:33:59,723 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:33:59,776 [api1.py:131] throughput: 0.0069455905496148326
(INFO) 2023-04-09 20:33:59,804 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:33:59,805 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,806 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,809 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:33:59,809 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,812 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,815 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:33:59,817 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,824 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:33:59,826 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,827 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,838 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:33:59,839 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,844 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,859 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:33:59,949 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:33:59,950 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:33:59,956 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:33:59,986 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:00,046 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:00,087 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 20:34:00,101 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:00,102 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,096 [api1.py:131] throughput: 0.011575341950192058
(INFO) 2023-04-09 20:34:00,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,135 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:00,136 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,142 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:00,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,142 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,143 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:00,143 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,145 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:34:00,145 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,153 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,156 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:00,156 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,162 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,184 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:00,184 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,206 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:34:00,206 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,223 [api1.py:131] throughput: 0.006945588480245695
(INFO) 2023-04-09 20:34:00,245 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:00,260 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:34:00,271 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:00,271 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,272 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:00,276 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:00,278 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,290 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:00,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,295 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 20:34:00,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,301 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:00,302 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,312 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:34:00,318 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:00,319 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,361 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:00,362 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,370 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,380 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:00,380 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,405 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:34:00,414 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:34:00,414 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,425 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:00,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,431 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:00,432 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,446 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:00,446 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,452 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,457 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:00,459 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,466 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:00,466 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:00,466 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,502 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:00,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,528 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:00,529 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,533 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,541 [api1.py:131] throughput: 0.004961254430754241
(INFO) 2023-04-09 20:34:00,546 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,554 [api1.py:131] throughput: 0.003858804317578792
(INFO) 2023-04-09 20:34:00,570 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:34:00,570 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:34:00,579 [api1.py:131] throughput: 0.0034729402316892805
(INFO) 2023-04-09 20:34:00,583 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 54}
(INFO) 2023-04-09 20:34:00,584 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:00,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,624 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:00,629 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 20:34:00,642 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:00,662 [api1.py:131] throughput: 0.011575341702605418
(INFO) 2023-04-09 20:34:00,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:00,667 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:00,680 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:34:00,681 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:00,691 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:34:00,799 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 20:34:00,818 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:34:00,835 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:34:00,925 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:34:00,931 [api1.py:131] throughput: 0.004341130287241182
(INFO) 2023-04-09 20:34:00,935 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:34:00,939 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:34:01,028 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:34:01,220 [api1.py:131] throughput: 0.004341130189065275
(INFO) 2023-04-09 20:34:01,292 [api1.py:131] throughput: 0.003472940300052548
(INFO) 2023-04-09 20:34:01,310 [api1.py:131] throughput: 0.00434112962531749
(INFO) 2023-04-09 20:34:01,379 [api1.py:131] throughput: 0.003858804575644339
(INFO) 2023-04-09 20:34:01,390 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:01,391 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,394 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:01,395 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,414 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:01,414 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,423 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,494 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:01,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,523 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:01,524 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:01,525 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:01,529 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,534 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:01,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,541 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:01,541 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,569 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:01,570 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,570 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,571 [api1.py:131] throughput: 0.0026715181764636587
(INFO) 2023-04-09 20:34:01,575 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,582 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:01,583 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,631 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:01,632 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,637 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,668 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,668 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,695 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:01,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,717 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,717 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,728 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:01,729 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,731 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:34:01,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,782 [api1.py:131] throughput: 0.01157534156248192
(INFO) 2023-04-09 20:34:01,832 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:01,838 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:01,846 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:01,846 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:01,847 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,847 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:01,847 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,851 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,852 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,862 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,862 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,867 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,867 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,867 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:01,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,869 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:01,870 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,871 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,873 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,874 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,874 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:01,878 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,878 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,879 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,881 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,882 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,886 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:01,886 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:01,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,886 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:01,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,887 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,888 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:01,889 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,890 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,890 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:34:01,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,891 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:01,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,891 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:01,892 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,894 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:01,894 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,895 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,900 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:01,900 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:01,901 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,901 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,902 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:01,903 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,903 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:01,904 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:01,907 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,907 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,909 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,909 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:01,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:02,001 [api1.py:131] throughput: 0.003157230416994385
(INFO) 2023-04-09 20:34:02,042 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:02,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:02,047 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:02,048 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:02,068 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:02,068 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 20:34:02,071 [api1.py:131] throughput: 0.003157230393023477
(INFO) 2023-04-09 20:34:02,074 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:02,077 [api1.py:131] throughput: 0.01736180469163078
(INFO) 2023-04-09 20:34:02,107 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:34:02,108 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:02,136 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:34:02,142 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:34:02,203 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:02,204 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:02,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:02,207 [api1.py:131] throughput: 0.006945585553867201
(INFO) 2023-04-09 20:34:02,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:02,252 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:02,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:02,258 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:02,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:02,270 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:02,270 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:02,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:02,288 [api1.py:131] throughput: 0.0038588046734744637
(INFO) 2023-04-09 20:34:02,339 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:02,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:02,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:02,366 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:02,366 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:34:02,376 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:34:02,477 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 20:34:02,484 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 20:34:02,501 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:34:02,521 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 20:34:02,567 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 20:34:02,603 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:34:02,614 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 20:34:02,634 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:02,670 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:34:02,671 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 20:34:02,711 [api1.py:131] throughput: 0.008681804420119654
(INFO) 2023-04-09 20:34:02,821 [api1.py:131] throughput: 0.011575335830536406
(INFO) 2023-04-09 20:34:02,856 [api1.py:131] throughput: 0.004341129782149566
(INFO) 2023-04-09 20:34:02,924 [api1.py:131] throughput: 0.008681806023730315
(INFO) 2023-04-09 20:34:03,069 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 20:34:03,086 [api1.py:131] throughput: 0.005788072378024285
(INFO) 2023-04-09 20:34:03,205 [api1.py:131] throughput: 0.005788073475421732
(INFO) 2023-04-09 20:34:03,220 [api1.py:131] throughput: 0.00267151823864598
(INFO) 2023-04-09 20:34:03,456 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:03,456 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:03,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,459 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,460 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,462 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,463 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,467 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,471 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:34:03,472 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,475 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:03,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,476 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:03,477 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,477 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:03,477 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,479 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,479 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,482 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:03,482 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,482 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,483 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,483 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,484 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,485 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:03,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,486 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,485 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:03,485 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:03,486 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,486 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,486 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:03,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,488 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,494 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,502 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,506 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:34:03,506 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,508 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,512 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:03,512 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,513 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,514 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:03,515 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,518 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:03,519 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,520 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,526 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,526 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,530 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:34:03,531 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,534 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:03,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,550 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,552 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:03,552 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,561 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,563 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,564 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,569 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,573 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,574 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,574 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,575 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,578 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,580 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:03,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,582 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:34:03,582 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,586 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:03,587 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,590 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,591 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,602 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:03,611 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:03,618 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:03,619 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:03,620 [api1.py:131] throughput: 0.00496125450707324
(INFO) 2023-04-09 20:34:03,627 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:03,633 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:03,634 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,636 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,636 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,637 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,637 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,638 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:03,638 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,638 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,638 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,638 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:03,638 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,638 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,639 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,639 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:34:03,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,641 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:03,641 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:03,642 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,643 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:03,642 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 34, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:03,643 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,643 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:03,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,644 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,644 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:03,645 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:03,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,646 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,648 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,648 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,653 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:34:03,654 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,655 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:03,656 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,657 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:03,658 [api1.py:131] throughput: 0.004341129886212267
(INFO) 2023-04-09 20:34:03,657 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:03,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,658 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:03,658 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,658 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:03,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,660 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,660 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:03,661 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,661 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,662 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,664 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,664 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:03,664 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,667 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:03,668 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,668 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,669 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:34:03,671 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:34:03,671 [api1.py:131] throughput: 0.0034729403056015145
(INFO) 2023-04-09 20:34:03,671 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,684 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:03,684 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:03,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:03,686 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:03,688 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:03,692 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,693 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:03,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,708 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 20:34:03,717 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:34:03,719 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:34:03,732 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:03,805 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:34:03,875 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:03,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,881 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:03,898 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:03,903 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:03,903 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:34:03,907 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:34:03,919 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:34:03,922 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:34:03,995 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:34:04,011 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:04,070 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:04,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:04,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:04,130 [api1.py:131] throughput: 0.017361797937901876
(INFO) 2023-04-09 20:34:04,137 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 20:34:04,169 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:34:04,185 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:04,189 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:34:04,242 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:04,340 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:34:04,358 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 20:34:04,396 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:34:04,415 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 20:34:04,431 [api1.py:131] throughput: 0.017361801179691095
(INFO) 2023-04-09 20:34:04,467 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:34:04,495 [api1.py:131] throughput: 0.004341129621918651
(INFO) 2023-04-09 20:34:04,523 [api1.py:131] throughput: 0.008681807108526096
(INFO) 2023-04-09 20:34:04,561 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:34:04,766 [api1.py:131] throughput: 0.011575340578199882
(INFO) 2023-04-09 20:34:04,925 [api1.py:131] throughput: 0.008681807132995177
(INFO) 2023-04-09 20:34:05,028 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 20:34:05,130 [api1.py:131] throughput: 0.011575341966037603
(INFO) 2023-04-09 20:34:05,172 [api1.py:131] throughput: 0.006945591028117781
(INFO) 2023-04-09 20:34:05,173 [api1.py:131] throughput: 0.011575341204519385
(INFO) 2023-04-09 20:34:05,232 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,233 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,238 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,240 [api1.py:131] throughput: 0.002315325646562217
(INFO) 2023-04-09 20:34:05,290 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,305 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:05,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,309 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:05,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,354 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:34:05,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,355 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:05,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,456 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,479 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:34:05,480 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,495 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:05,506 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,506 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,509 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:05,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,511 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:34:05,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,515 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:05,515 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,516 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:05,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,520 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,520 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,522 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,535 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:05,536 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,538 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:05,538 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:05,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,539 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:05,540 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,539 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:05,540 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,540 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,540 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,540 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,541 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:34:05,541 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,541 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:05,542 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,542 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,542 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,544 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,544 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:34:05,544 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,544 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,544 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,544 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,546 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,546 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:05,546 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,546 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,547 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,548 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:34:05,549 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,550 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,551 [api1.py:131] throughput: 0.003157230346767118
(INFO) 2023-04-09 20:34:05,552 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,553 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,553 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,555 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,558 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:05,558 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,561 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,564 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:05,564 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:05,566 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:34:05,566 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,570 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:05,571 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,571 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,572 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,572 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:05,572 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,580 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,581 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:05,582 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,584 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,588 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:34:05,589 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,613 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:34:05,614 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,627 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:05,628 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,635 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,639 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:34:05,646 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:05,660 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:05,679 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,684 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,685 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:34:05,690 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:05,690 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:34:05,691 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,691 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:05,695 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,700 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,700 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,704 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,705 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:05,712 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:34:05,713 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:05,719 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:05,719 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:05,720 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,720 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,723 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:34:05,723 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,727 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:05,727 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,731 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:05,734 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,751 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:05,760 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:34:05,783 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:34:05,794 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:05,801 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:05,801 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,818 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:34:05,820 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,828 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:05,834 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:05,841 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:34:05,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:05,842 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,848 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,855 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 20:34:05,865 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:34:05,888 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:05,896 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:05,899 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:34:05,916 [api1.py:131] throughput: 0.005788070518376598
(INFO) 2023-04-09 20:34:05,919 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:05,989 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:34:05,994 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 20:34:06,055 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:06,056 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,070 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:06,074 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:34:06,166 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 20:34:06,205 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 20:34:06,316 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:34:06,317 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,429 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:34:06,430 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,458 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,495 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:34:06,511 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:06,512 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,516 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:06,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,530 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:06,531 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,543 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:06,543 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,567 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:34:06,567 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:06,568 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,568 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,576 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,588 [api1.py:131] throughput: 0.01736179912392218
(INFO) 2023-04-09 20:34:06,591 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 20:34:06,598 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:06,598 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,602 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,607 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,610 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:06,611 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,617 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:06,618 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,626 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:34:06,626 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,635 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,646 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:06,646 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,730 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:34:06,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,773 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:06,775 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:06,775 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,784 [api1.py:131] throughput: 0.005788072314328548
(INFO) 2023-04-09 20:34:06,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,786 [api1.py:131] throughput: 0.011575337662575496
(INFO) 2023-04-09 20:34:06,793 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:06,795 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:06,795 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,800 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,809 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:06,814 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 20:34:06,843 [api1.py:131] throughput: 0.008681806502731015
(INFO) 2023-04-09 20:34:06,867 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 20:34:06,890 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:06,890 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,899 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,904 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:34:06,912 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:06,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:06,944 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:06,951 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:06,974 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:06,975 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:06,976 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:06,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,009 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:07,148 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:07,149 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,166 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:34:07,181 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,202 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:07,203 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,203 [api1.py:131] throughput: 0.003858804726302733
(INFO) 2023-04-09 20:34:07,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,222 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 20:34:07,227 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:34:07,228 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,256 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:07,256 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,268 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:07,269 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,300 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:07,301 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,358 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:07,359 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,363 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,378 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:07,379 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,420 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:07,421 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,424 [api1.py:131] throughput: 0.0049612545382946485
(INFO) 2023-04-09 20:34:07,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,435 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 20:34:07,447 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:34:07,458 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:07,459 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,479 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:07,480 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,491 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:07,491 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,563 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:07,563 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,566 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:34:07,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,575 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 60}
(INFO) 2023-04-09 20:34:07,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,585 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:07,602 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,616 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:07,617 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,625 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:07,625 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,630 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,636 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,657 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:07,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,662 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,684 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:34:07,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,685 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 20:34:07,688 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:07,693 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,700 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:07,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,720 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:07,723 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:07,724 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,747 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:34:07,747 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:34:07,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,758 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,768 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:07,775 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:07,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,776 [api1.py:131] throughput: 0.005788072848504203
(INFO) 2023-04-09 20:34:07,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,783 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:07,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,788 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:07,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,794 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:07,808 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:07,809 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,817 [api1.py:131] throughput: 0.00496125387607221
(INFO) 2023-04-09 20:34:07,828 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,836 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 20:34:07,837 [api1.py:131] throughput: 0.017361800700790336
(INFO) 2023-04-09 20:34:07,838 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:07,839 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,839 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:07,840 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,850 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,850 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,858 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:07,859 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,901 [api1.py:131] throughput: 0.00496125462628226
(INFO) 2023-04-09 20:34:07,964 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:34:07,973 [api1.py:131] throughput: 0.008681808365512023
(INFO) 2023-04-09 20:34:07,972 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:07,973 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,974 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:07,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:07,977 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:07,995 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:34:08,006 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:08,007 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:08,012 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:08,014 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:08,014 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:08,025 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:08,028 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:34:08,035 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 20:34:08,051 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:08,052 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:08,059 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:08,066 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:34:08,067 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:08,074 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:34:08,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:08,086 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:08,142 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:34:08,150 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:08,175 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:34:08,178 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:08,250 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 20:34:08,275 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:08,296 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:08,310 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:08,324 [api1.py:131] throughput: 0.003858804356504878
(INFO) 2023-04-09 20:34:08,358 [api1.py:131] throughput: 0.011575339024861351
(INFO) 2023-04-09 20:34:08,372 [api1.py:131] throughput: 0.011575342540798778
(INFO) 2023-04-09 20:34:08,399 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:34:08,404 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:34:08,412 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:08,427 [api1.py:131] throughput: 0.006945591028117781
(INFO) 2023-04-09 20:34:08,448 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 20:34:08,460 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:08,486 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 20:34:08,488 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:34:08,523 [api1.py:131] throughput: 0.0173618036587072
(INFO) 2023-04-09 20:34:08,535 [api1.py:131] throughput: 0.011575338091036336
(INFO) 2023-04-09 20:34:08,578 [api1.py:131] throughput: 0.003858804696319121
(INFO) 2023-04-09 20:34:08,589 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:34:08,792 [api1.py:131] throughput: 0.0069455907138505305
(INFO) 2023-04-09 20:34:08,829 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:34:09,070 [api1.py:131] throughput: 0.003858804809318588
(INFO) 2023-04-09 20:34:09,118 [api1.py:131] throughput: 0.00385880473578473
(INFO) 2023-04-09 20:34:09,225 [api1.py:131] throughput: 0.0028941370049427513
(INFO) 2023-04-09 20:34:09,410 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:09,412 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,417 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:09,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,418 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,430 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:09,430 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,438 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,467 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:09,468 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,473 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,486 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:09,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,533 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:09,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,636 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:09,636 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,662 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:09,669 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:09,670 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,680 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:09,686 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:09,686 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,696 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:09,696 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,699 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:09,700 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,701 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,706 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,750 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:09,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,804 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:09,809 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:09,819 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:09,859 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:09,859 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,865 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,866 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:34:09,885 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:09,892 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:09,892 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,898 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:09,898 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,901 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:09,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,913 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:09,913 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,919 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:09,920 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:09,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,931 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:34:09,931 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,933 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:09,934 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,940 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:09,940 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,941 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,951 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,966 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:09,966 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,974 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:09,974 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:34:09,975 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,990 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:09,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:09,996 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:10,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,032 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:10,053 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:34:10,096 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:34:10,403 [api1.py:131] throughput: 0.006945588020386053
(INFO) 2023-04-09 20:34:10,412 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:10,412 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,412 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:10,415 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,418 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:10,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,422 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,427 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,433 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,527 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:34:10,573 [api1.py:131] throughput: 0.005788071943735196
(INFO) 2023-04-09 20:34:10,586 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:34:10,601 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:34:10,601 [api1.py:131] throughput: 0.004961253684297419
(INFO) 2023-04-09 20:34:10,607 [api1.py:131] throughput: 0.006945590319684866
(INFO) 2023-04-09 20:34:10,653 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:34:10,890 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:34:10,898 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:34:10,984 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:10,985 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,988 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:10,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,988 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:10,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,990 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:10,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,991 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:10,992 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:10,992 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,992 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,993 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:10,994 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,995 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,997 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:10,998 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:10,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:10,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,137 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:11,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,163 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:11,164 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,168 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,180 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:11,180 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,198 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:11,199 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,237 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:34:11,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,239 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:11,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,248 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:11,249 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,249 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:11,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,254 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:11,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,264 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:11,264 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,267 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:11,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,269 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 65}
(INFO) 2023-04-09 20:34:11,269 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,272 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:11,275 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:11,275 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:11,275 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:11,275 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:34:11,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,279 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:11,280 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,280 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:11,281 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,284 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,284 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,287 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,292 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:11,310 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:11,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,317 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,322 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:11,323 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,330 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,338 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:34:11,354 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:11,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,357 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:34:11,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,358 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:11,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,382 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:34:11,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,422 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:34:11,422 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,425 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:34:11,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,429 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 61}
(INFO) 2023-04-09 20:34:11,429 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,430 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:34:11,430 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,431 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:11,431 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,432 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,435 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,437 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,438 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:34:11,438 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,449 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:11,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,457 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:11,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,475 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:11,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,477 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:34:11,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,487 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:34:11,488 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:11,490 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 20:34:11,502 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:11,506 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 20:34:11,537 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:11,538 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:11,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,539 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:11,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,545 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:34:11,546 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,548 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 20:34:11,549 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,551 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:34:11,561 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:11,561 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,569 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:34:11,571 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:34:11,577 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:34:11,659 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:11,660 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,664 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,695 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:11,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,820 [api1.py:131] throughput: 0.0034729404205761072
(INFO) 2023-04-09 20:34:11,933 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:11,934 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:11,935 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:34:11,947 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:11,949 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:12,008 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:34:12,010 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:34:12,011 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,053 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:12,054 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,066 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:12,066 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,070 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,098 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:12,099 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,108 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,119 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:12,124 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:12,125 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,155 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:34:12,155 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,168 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,200 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:12,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,207 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,212 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:12,213 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,220 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,229 [api1.py:131] throughput: 0.01736179967457452
(INFO) 2023-04-09 20:34:12,230 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:34:12,240 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:12,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,242 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:12,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,251 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:12,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,256 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,258 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:12,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,264 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,264 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,270 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:12,271 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,271 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:12,273 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,274 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:12,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,276 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:12,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,299 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:12,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,306 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:12,306 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,309 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,312 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,325 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:12,339 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:12,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,344 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,353 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:34:12,366 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:12,374 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:12,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,423 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 20:34:12,426 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:12,427 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,433 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,439 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:34:12,443 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:12,450 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:12,451 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,460 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:12,461 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,466 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,476 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:34:12,515 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 34, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:12,515 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,536 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:12,539 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:12,577 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:12,577 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:34:12,585 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 20:34:12,587 [api1.py:131] throughput: 0.01736179662366335
(INFO) 2023-04-09 20:34:12,604 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:12,605 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,610 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,633 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:12,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,704 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:34:12,725 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:34:12,728 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 46}
(INFO) 2023-04-09 20:34:12,728 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,743 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:12,743 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,748 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,767 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:12,772 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:12,773 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,777 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,782 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:34:12,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,804 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:34:12,807 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:34:12,827 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:12,849 [api1.py:131] throughput: 0.008681807529161267
(INFO) 2023-04-09 20:34:12,858 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:12,858 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:12,864 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:34:12,869 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:12,870 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,870 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:12,871 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:12,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:12,891 [api1.py:131] throughput: 0.011575340936814248
(INFO) 2023-04-09 20:34:12,921 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 20:34:12,939 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:34:12,954 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:13,086 [api1.py:131] throughput: 0.011575337966170602
(INFO) 2023-04-09 20:34:13,098 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:34:13,104 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:13,104 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,105 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:13,110 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,119 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:34:13,172 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:34:13,173 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,183 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,185 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:13,188 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:13,189 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,219 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:13,228 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:34:13,242 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:13,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,267 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:13,267 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,310 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:34:13,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,312 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 46}
(INFO) 2023-04-09 20:34:13,312 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,312 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:13,313 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,316 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:34:13,317 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,320 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,321 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:13,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,323 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:34:13,323 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:13,324 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,324 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,324 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:13,325 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,329 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,331 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,334 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:13,334 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,339 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,341 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,349 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,371 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:13,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,382 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,402 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:34:13,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,409 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:34:13,409 [api1.py:131] throughput: 0.017361807663273162
(INFO) 2023-04-09 20:34:13,411 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,418 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:13,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,442 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:13,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,456 [api1.py:131] throughput: 0.004961254123495958
(INFO) 2023-04-09 20:34:13,546 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:13,547 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,556 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:34:13,573 [api1.py:131] throughput: 0.0057880728029738844
(INFO) 2023-04-09 20:34:13,581 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:13,596 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:13,602 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:13,671 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:13,675 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:34:13,720 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:34:13,731 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:13,740 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:13,740 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,751 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:13,759 [api1.py:131] throughput: 0.006945590043768928
(INFO) 2023-04-09 20:34:13,762 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:34:13,780 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:13,780 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:13,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,798 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:13,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,810 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,815 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 20:34:13,840 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:13,841 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,985 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:13,986 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:13,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:13,999 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:14,000 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,005 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,018 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:14,018 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,025 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,050 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:14,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,060 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:14,060 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,100 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:14,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,116 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:14,117 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,123 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:34:14,126 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,142 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:14,142 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,146 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:14,146 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,150 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,153 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,164 [api1.py:131] throughput: 0.01736179912392218
(INFO) 2023-04-09 20:34:14,219 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 20:34:14,234 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:34:14,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,237 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:14,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,243 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,255 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:14,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,272 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,289 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:14,337 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:34:14,337 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:14,345 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,345 [api1.py:131] throughput: 0.004341130158546594
(INFO) 2023-04-09 20:34:14,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,366 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:34:14,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,372 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,372 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:14,393 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:14,402 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:14,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:14,407 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:14,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:14,412 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:14,467 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:34:14,500 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:14,517 [api1.py:131] throughput: 0.00868180805948366
(INFO) 2023-04-09 20:34:14,519 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:14,555 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:34:14,619 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:14,639 [api1.py:131] throughput: 0.017361800199615158
(INFO) 2023-04-09 20:34:14,643 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:14,679 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:34:14,701 [api1.py:131] throughput: 0.00578807293153008
(INFO) 2023-04-09 20:34:14,798 [api1.py:131] throughput: 0.00868180764496367
(INFO) 2023-04-09 20:34:14,813 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 20:34:14,815 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:34:14,928 [api1.py:131] throughput: 0.005788073184561335
(INFO) 2023-04-09 20:34:15,024 [api1.py:131] throughput: 0.01736179591178423
(INFO) 2023-04-09 20:34:15,088 [api1.py:131] throughput: 0.008681808200060013
(INFO) 2023-04-09 20:34:15,090 [api1.py:131] throughput: 0.011575341204519385
(INFO) 2023-04-09 20:34:15,231 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:34:15,234 [api1.py:131] throughput: 0.0038588048406039024
(INFO) 2023-04-09 20:34:15,248 [api1.py:131] throughput: 0.0038588045105873085
(INFO) 2023-04-09 20:34:15,362 [api1.py:131] throughput: 0.01157534156248192
(INFO) 2023-04-09 20:34:15,366 [api1.py:131] throughput: 0.008681808503263928
(INFO) 2023-04-09 20:34:15,367 [api1.py:131] throughput: 0.004961254669087043
(INFO) 2023-04-09 20:34:15,416 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,417 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,441 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,442 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,442 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:34:15,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,444 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,445 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,447 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:15,448 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,454 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,462 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:15,462 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,466 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,466 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,467 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,480 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,508 [api1.py:131] throughput: 0.0038588048002623125
(INFO) 2023-04-09 20:34:15,510 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:15,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,522 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,531 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,536 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,545 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 58}
(INFO) 2023-04-09 20:34:15,546 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,581 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:34:15,581 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,588 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,592 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:15,593 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,597 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,641 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:15,642 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,645 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:15,646 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,646 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,663 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:15,663 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,668 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,671 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:15,684 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:15,702 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:15,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,710 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,713 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:15,729 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:15,729 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:15,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,732 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,733 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,748 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:15,758 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:15,784 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:15,806 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:15,823 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:15,835 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:15,836 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:34:15,864 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:15,876 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,877 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,878 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:15,879 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,881 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:34:15,884 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,884 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,887 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:34:15,906 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:15,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,906 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:15,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,908 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:15,909 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,914 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,914 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:34:15,915 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,917 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,919 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:15,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,920 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:15,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,920 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:15,921 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,921 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:15,921 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,922 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:34:15,922 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 88, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,926 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,928 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,938 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:15,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,950 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:34:15,972 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:15,972 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,977 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:15,989 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:15,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,990 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:15,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:15,991 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:15,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,001 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:34:16,023 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:16,035 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:16,203 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:16,207 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:16,244 [api1.py:131] throughput: 0.0043411302935585875
(INFO) 2023-04-09 20:34:16,257 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:16,263 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:34:16,264 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,266 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:16,266 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,268 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:16,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,316 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:34:16,381 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:16,382 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,390 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,448 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:16,448 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,452 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,505 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:34:16,521 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:34:16,590 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:16,633 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:16,634 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,654 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:16,655 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,668 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 20:34:16,681 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:16,682 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,686 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,698 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:16,699 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,704 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,705 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:16,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,705 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:34:16,706 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,765 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:34:16,785 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:16,785 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,807 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:16,807 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,810 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:16,811 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,821 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:16,821 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,829 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:16,829 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,830 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,838 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,842 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 20:34:16,854 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:34:16,854 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,859 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:16,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,862 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,869 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,870 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:16,870 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,874 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:16,877 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,880 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:34:16,897 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:16,897 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,903 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:16,910 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:16,916 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:34:16,916 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:34:16,917 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:16,929 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 20:34:16,950 [api1.py:131] throughput: 0.011575338781083858
(INFO) 2023-04-09 20:34:16,955 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:34:16,982 [api1.py:131] throughput: 0.0020429410841996238
(INFO) 2023-04-09 20:34:16,991 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:17,095 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 20:34:17,169 [api1.py:131] throughput: 0.005788071750717845
(INFO) 2023-04-09 20:34:17,186 [api1.py:131] throughput: 0.017361805902644797
(INFO) 2023-04-09 20:34:17,196 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:34:17,207 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:34:17,215 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:34:17,246 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:34:17,355 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:34:17,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:17,398 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:17,398 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:17,399 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:17,399 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:17,413 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:17,420 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:17,420 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:17,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:17,433 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:17,452 [api1.py:131] throughput: 0.017361803286854736
(INFO) 2023-04-09 20:34:17,475 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:17,549 [api1.py:131] throughput: 0.006945589479940776
(INFO) 2023-04-09 20:34:17,582 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:17,599 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 44}
(INFO) 2023-04-09 20:34:17,600 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:17,609 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:17,650 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:17,699 [api1.py:131] throughput: 0.006945589629895063
(INFO) 2023-04-09 20:34:17,788 [api1.py:131] throughput: 0.00868180805948366
(INFO) 2023-04-09 20:34:17,787 [api1.py:131] throughput: 0.0020429410643387196
(INFO) 2023-04-09 20:34:17,795 [api1.py:131] throughput: 0.005788072587126461
(INFO) 2023-04-09 20:34:17,824 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:17,931 [api1.py:131] throughput: 0.01736179967457452
(INFO) 2023-04-09 20:34:18,030 [api1.py:131] throughput: 0.011575340077537005
(INFO) 2023-04-09 20:34:18,108 [api1.py:131] throughput: 0.011575341204519385
(INFO) 2023-04-09 20:34:18,159 [api1.py:131] throughput: 0.0031572304926402836
(INFO) 2023-04-09 20:34:18,246 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:18,246 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,248 [api1.py:131] throughput: 0.004961254846251298
(INFO) 2023-04-09 20:34:18,252 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:18,253 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,255 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:18,256 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,257 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:18,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,257 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:18,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,261 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,261 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,264 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,338 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:18,338 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,338 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:18,339 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,340 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:18,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,345 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,352 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:18,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,385 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:18,386 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,389 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:18,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,393 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:18,394 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,396 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:18,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,415 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:18,415 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:34:18,416 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,417 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,419 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:18,420 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,421 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:18,421 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,423 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,428 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,430 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:18,432 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:18,433 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,484 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:18,498 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:18,498 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,499 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:18,500 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,503 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:18,503 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,511 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:18,512 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,528 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:34:18,532 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,545 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:18,546 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,544 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:34:18,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,588 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 41, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:18,589 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,589 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:18,589 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,590 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:18,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,595 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:34:18,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,599 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:34:18,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,605 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:18,607 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:18,611 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:18,613 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:18,633 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:18,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,643 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:18,681 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:34:18,681 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,690 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,690 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:34:18,690 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:18,690 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:18,692 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,692 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:18,692 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,713 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:34:18,765 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 68}
(INFO) 2023-04-09 20:34:18,766 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,791 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:34:18,842 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 20:34:18,843 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:18,875 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:34:18,900 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:18,901 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,910 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:18,910 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:18,911 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:18,929 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:18,935 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:18,959 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:18,998 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:34:18,999 [api1.py:131] throughput: 0.011575342323907762
(INFO) 2023-04-09 20:34:19,095 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:34:19,128 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:19,234 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:19,406 [api1.py:131] throughput: 0.011575337966170602
(INFO) 2023-04-09 20:34:19,521 [api1.py:131] throughput: 0.0069455905705175566
(INFO) 2023-04-09 20:34:19,595 [api1.py:131] throughput: 0.008681807437366683
(INFO) 2023-04-09 20:34:20,003 [api1.py:131] throughput: 0.011575342975492143
(INFO) 2023-04-09 20:34:21,016 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:21,017 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,406 [api1.py:131] throughput: 0.005788072785992881
(INFO) 2023-04-09 20:34:21,468 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:21,468 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,474 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,478 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:21,479 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,493 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:21,497 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,499 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:21,500 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,511 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:21,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,514 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,527 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:34:21,528 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,528 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,536 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 57}
(INFO) 2023-04-09 20:34:21,536 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,551 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:34:21,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,561 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,579 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:21,580 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,580 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:21,581 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,589 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:21,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,594 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:21,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,595 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,610 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,623 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:21,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,623 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:21,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,624 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:21,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,628 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:21,628 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,629 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:21,629 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,632 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,640 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:21,641 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,648 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,655 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,672 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:21,704 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:21,704 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,709 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:21,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,753 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:21,759 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:21,766 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:21,769 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:21,770 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,775 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:21,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,783 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:34:21,784 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,784 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:21,784 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:34:21,784 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,785 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:21,785 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,785 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 52}
(INFO) 2023-04-09 20:34:21,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,785 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:34:21,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,786 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:21,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,788 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:21,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,789 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:21,790 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,790 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:21,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,793 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:21,794 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,797 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,801 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:21,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,803 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,806 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,825 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:34:21,870 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:34:21,871 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:34:21,909 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 20:34:21,926 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:21,927 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:21,931 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:21,965 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:34:21,978 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:34:21,984 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 20:34:21,999 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:34:22,138 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:22,184 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 20:34:22,201 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:34:22,291 [api1.py:131] throughput: 0.00868180376558486
(INFO) 2023-04-09 20:34:22,322 [api1.py:131] throughput: 0.00868180376558486
(INFO) 2023-04-09 20:34:22,467 [api1.py:131] throughput: 0.011575335385612713
(INFO) 2023-04-09 20:34:22,487 [api1.py:131] throughput: 0.004961253570431144
(INFO) 2023-04-09 20:34:22,498 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:34:22,659 [api1.py:131] throughput: 0.011575338524129215
(INFO) 2023-04-09 20:34:22,690 [api1.py:131] throughput: 0.005788072145144621
(INFO) 2023-04-09 20:34:22,901 [api1.py:131] throughput: 0.008681807437366683
(INFO) 2023-04-09 20:34:22,911 [api1.py:131] throughput: 0.011575341062841587
(INFO) 2023-04-09 20:34:22,929 [api1.py:131] throughput: 0.0069455907138505305
(INFO) 2023-04-09 20:34:22,946 [api1.py:131] throughput: 0.017361805615825673
(INFO) 2023-04-09 20:34:22,961 [api1.py:131] throughput: 0.006945590910933379
(INFO) 2023-04-09 20:34:23,025 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:34:23,149 [api1.py:131] throughput: 0.006945591487021769
(INFO) 2023-04-09 20:34:24,437 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:34:24,438 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,444 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:24,444 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,446 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:24,447 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,447 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,448 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,448 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,448 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,448 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:24,449 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,449 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:24,450 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,454 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:34:24,455 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,455 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:24,455 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,456 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,457 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,457 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:24,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,457 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:24,458 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,459 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,461 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,461 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,462 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,466 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,466 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,476 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:34:24,477 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,479 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,481 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,493 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,494 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:34:24,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,505 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,513 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:34:24,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,575 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,581 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,615 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:24,619 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:24,621 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:24,623 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:24,624 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,631 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:24,632 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,637 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,641 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:24,645 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:24,646 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,645 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:24,646 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,650 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:24,651 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:24,653 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,654 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,655 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:24,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,658 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:24,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,660 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 32, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:24,660 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,668 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:24,668 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:24,669 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,672 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,673 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:34:24,673 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,675 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,682 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:34:24,687 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,695 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:24,747 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:24,748 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:24,749 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:34:24,755 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:24,755 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:24,764 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:24,764 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,771 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:34:24,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,780 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:34:24,807 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:24,808 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,809 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:24,811 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,816 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,820 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:24,821 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,829 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,862 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,864 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:34:24,864 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,904 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 20:34:24,911 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:34:24,929 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 20:34:24,935 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:34:24,936 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:24,946 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:24,978 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:34:25,023 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:25,023 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,031 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:25,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,033 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,087 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:34:25,088 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:34:25,146 [api1.py:131] throughput: 0.005788071248872792
(INFO) 2023-04-09 20:34:25,159 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:34:25,167 [api1.py:131] throughput: 0.006945588615498541
(INFO) 2023-04-09 20:34:25,177 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:34:25,242 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:34:25,416 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:25,472 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:25,472 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,475 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:34:25,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,480 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:34:25,481 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,534 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:34:25,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,544 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:34:25,545 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,562 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:34:25,562 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,566 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:25,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,570 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,571 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,576 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:25,577 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,583 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,592 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:25,592 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:25,592 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,592 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:25,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,659 [api1.py:131] throughput: 0.004961253893052271
(INFO) 2023-04-09 20:34:25,677 [api1.py:131] throughput: 0.008681806847012802
(INFO) 2023-04-09 20:34:25,689 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:25,696 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:25,701 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 20:34:25,715 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:25,716 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 20:34:25,723 [api1.py:131] throughput: 0.011575339686543168
(INFO) 2023-04-09 20:34:25,812 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:25,904 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:34:25,934 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 20:34:25,985 [api1.py:131] throughput: 0.005788072908822148
(INFO) 2023-04-09 20:34:26,083 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:34:26,119 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:34:26,165 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:34:26,456 [api1.py:131] throughput: 0.01736180289982464
(INFO) 2023-04-09 20:34:26,472 [api1.py:131] throughput: 0.011575339686543168
(INFO) 2023-04-09 20:34:26,487 [api1.py:131] throughput: 0.004961254598786131
(INFO) 2023-04-09 20:34:26,858 [api1.py:131] throughput: 0.0038588047813790153
(INFO) 2023-04-09 20:34:27,080 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:27,081 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,097 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:27,098 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,101 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:27,102 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,104 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:27,104 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:27,105 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,105 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,108 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,109 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:27,110 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,111 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,111 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:27,111 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,111 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:27,112 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,111 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:34:27,114 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,112 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:27,117 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,117 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,121 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,141 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,160 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:27,161 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,165 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,167 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:34:27,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,172 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:27,172 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,177 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,186 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:27,187 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,296 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:27,325 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:27,349 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:27,349 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,356 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:27,356 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,363 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:27,369 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:27,373 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:34:27,373 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,379 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:34:27,381 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:27,382 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,382 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,385 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:27,386 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,391 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:27,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,396 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:34:27,430 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:27,437 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:34:27,438 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,439 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:27,460 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:27,461 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:34:27,476 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 40, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:27,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,479 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:27,480 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,494 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:34:27,502 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,545 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:27,545 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,579 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 20:34:27,596 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:34:27,637 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:27,646 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:27,647 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:27,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:27,682 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:27,786 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:34:27,880 [api1.py:131] throughput: 0.005788071437064676
(INFO) 2023-04-09 20:34:27,931 [api1.py:131] throughput: 0.011575337966170602
(INFO) 2023-04-09 20:34:28,008 [api1.py:131] throughput: 0.006945588615498541
(INFO) 2023-04-09 20:34:28,227 [api1.py:131] throughput: 0.008681806502731015
(INFO) 2023-04-09 20:34:28,311 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 20:34:28,443 [api1.py:131] throughput: 0.0069455910094748064
(INFO) 2023-04-09 20:34:30,263 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:34:30,264 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,266 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:30,266 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:30,267 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,267 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:30,267 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,267 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,267 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:34:30,267 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:30,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,268 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:30,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,272 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:30,273 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:34:30,273 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,273 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,273 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:30,273 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:30,275 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,278 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,285 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,359 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:34:30,360 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,376 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:34:30,383 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:30,384 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,384 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:30,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,433 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:30,477 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:30,564 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:34:30,569 [api1.py:131] throughput: 0.0021706215823013054
(INFO) 2023-04-09 20:34:30,577 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:34:30,595 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:34:30,603 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:30,603 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,612 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,617 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:34:30,645 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:34:30,681 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:30,681 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:30,688 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,705 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:34:30,772 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:34:30,917 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:30,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:31,052 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:31,188 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 20:34:31,433 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 20:34:31,466 [api1.py:131] throughput: 0.01157534156248192
(INFO) 2023-04-09 20:34:31,507 [api1.py:131] throughput: 0.0038588046189420577
(INFO) 2023-04-09 20:34:31,509 [api1.py:131] throughput: 0.011575342540798778
(INFO) 2023-04-09 20:34:32,700 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:32,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,703 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:32,703 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,705 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:32,706 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,706 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,706 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:32,706 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:32,707 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,707 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,708 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:32,709 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,710 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:32,711 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,711 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:34:32,712 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,713 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,713 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,712 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:32,713 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,714 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,717 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,721 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,721 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,748 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:32,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,792 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:32,793 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:32,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:32,813 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:32,866 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:32,876 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:32,893 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:32,961 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:34:33,009 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:34:33,021 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:34:33,024 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:34:33,055 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:33,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:33,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:33,081 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:33,149 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:33,149 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:33,158 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:34:33,177 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:33,298 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 20:34:33,594 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 20:34:33,860 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 20:34:35,097 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:34:35,097 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,097 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:35,098 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,097 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:35,098 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,099 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:35,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,099 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:35,099 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:34:35,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,101 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:34:35,102 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,103 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,103 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:35,104 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,103 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:35,104 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,108 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,110 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,110 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,116 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,140 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:34:35,141 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,155 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,173 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:35,212 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:35,241 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:35,301 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:35,501 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:34:35,504 [api1.py:131] throughput: 0.017361798545737262
(INFO) 2023-04-09 20:34:35,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,828 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:34:35,829 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,829 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:35,829 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,832 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,833 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,840 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:34:35,841 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,874 [api1.py:131] throughput: 0.011575342323907762
(INFO) 2023-04-09 20:34:35,958 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:34:35,975 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:35,985 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:35,985 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:35,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:36,010 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:34:36,082 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:36,098 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:34:36,099 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:36,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:36,104 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:34:36,121 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:34:36,122 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:36,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:36,180 [api1.py:131] throughput: 0.0049612545382946485
(INFO) 2023-04-09 20:34:36,323 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:34:36,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:36,371 [api1.py:131] throughput: 0.01736179662366335
(INFO) 2023-04-09 20:34:36,415 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 20:34:36,574 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:34:36,885 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:36,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:36,894 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:36,895 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:36,901 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:36,900 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:36,901 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:36,905 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:34:36,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:36,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:36,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:36,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,011 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:37,023 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:34:37,042 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:34:37,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,070 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:37,070 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,137 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 42}
(INFO) 2023-04-09 20:34:37,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,179 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:34:37,193 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:34:37,427 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:34:37,428 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,437 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,475 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 20:34:37,525 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:34:37,557 [api1.py:131] throughput: 0.011575339686543168
(INFO) 2023-04-09 20:34:37,630 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:34:37,818 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:37,818 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,822 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,834 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:37,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,880 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:34:37,890 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:34:37,896 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:34:37,896 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,907 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:34:37,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:37,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:37,988 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:34:37,989 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:34:38,161 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:38,162 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,165 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,173 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:38,173 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,184 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,283 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:38,371 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:34:38,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,371 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 60}
(INFO) 2023-04-09 20:34:38,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,511 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:34:38,512 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,516 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,603 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 20:34:38,604 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:34:38,610 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:38,611 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,618 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,643 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:38,644 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,714 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 20:34:38,715 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:38,792 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:34:38,871 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:38,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:38,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:38,897 [api1.py:131] throughput: 0.017361806447601158
(INFO) 2023-04-09 20:34:39,107 [api1.py:131] throughput: 0.0019294469615267638
(INFO) 2023-04-09 20:34:39,159 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 20:34:39,201 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 88, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:39,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,214 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:39,215 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,354 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:39,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,365 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:34:39,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,371 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,403 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 20:34:39,428 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:34:39,499 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 20:34:39,632 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 57}
(INFO) 2023-04-09 20:34:39,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,642 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,811 [api1.py:131] throughput: 0.01157534156248192
(INFO) 2023-04-09 20:34:39,959 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:34:39,960 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:39,960 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,960 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:39,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,968 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:39,973 [api1.py:131] throughput: 0.017361805615825673
(INFO) 2023-04-09 20:34:40,039 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:34:40,039 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,089 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:34:40,098 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:40,456 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:40,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,466 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,505 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:34:40,505 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,571 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:34:40,613 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:34:40,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,630 [api1.py:131] throughput: 0.006945590841688053
(INFO) 2023-04-09 20:34:40,630 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:34:40,631 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,636 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,765 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:34:40,801 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:40,801 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,806 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,930 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:34:40,975 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:34:40,975 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:40,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:40,990 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 56}
(INFO) 2023-04-09 20:34:40,990 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:41,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:41,044 [api1.py:131] throughput: 0.011575338091036336
(INFO) 2023-04-09 20:34:41,343 [api1.py:131] throughput: 0.004961254296328738
(INFO) 2023-04-09 20:34:41,450 [api1.py:131] throughput: 0.011575341837009593
(INFO) 2023-04-09 20:34:41,509 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:34:41,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:41,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:41,519 [api1.py:131] throughput: 0.004961254874084118
(INFO) 2023-04-09 20:34:41,665 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:34:43,167 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:34:43,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:43,168 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:34:43,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:34:43,175 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:43,180 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:34:43,281 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:34:43,611 [api1.py:131] throughput: 0.011575341837009593
