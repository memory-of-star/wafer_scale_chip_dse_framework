(DEBUG) 2023-04-10 17:13:43,753 [logger.py:40] logger init.
(INFO) 2023-04-10 17:13:43,753 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-17-13-43-752910.log
(INFO) 2023-04-10 17:13:47,390 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:13:47,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,435 [api1.py:131] throughput: 8.084077312831123
(INFO) 2023-04-10 17:13:47,436 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,436 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:47,437 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,476 [api1.py:131] throughput: 0.17350003143384346
(INFO) 2023-04-10 17:13:47,477 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,477 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 143, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:47,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,516 [api1.py:131] throughput: 0.1488832759254849
(INFO) 2023-04-10 17:13:47,517 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,517 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 12, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:13:47,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,549 [api1.py:131] throughput: 4.006039294236782
(INFO) 2023-04-10 17:13:47,550 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,550 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:47,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,583 [api1.py:131] throughput: 2.576944317740592
(INFO) 2023-04-10 17:13:47,584 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,584 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:47,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,626 [api1.py:131] throughput: 0.251595248442642
(INFO) 2023-04-10 17:13:47,627 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,627 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:47,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,651 [api1.py:131] throughput: 0.8018169483613025
(INFO) 2023-04-10 17:13:47,652 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,652 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:47,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,679 [api1.py:131] throughput: 0.11155451826529378
(INFO) 2023-04-10 17:13:47,680 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,680 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:13:47,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,701 [api1.py:131] throughput: 1.639985471265625
(INFO) 2023-04-10 17:13:47,702 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 10, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:13:47,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,721 [api1.py:131] throughput: 6.22102636953016
(INFO) 2023-04-10 17:13:47,722 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,722 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 11, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:13:47,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,740 [api1.py:131] throughput: 3.9899593471022046
(INFO) 2023-04-10 17:13:47,741 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,741 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 10, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:13:47,742 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,765 [api1.py:131] throughput: 8.483240235249498
(INFO) 2023-04-10 17:13:47,765 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,766 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:13:47,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,787 [api1.py:131] throughput: 1.2628582349233375
(INFO) 2023-04-10 17:13:47,788 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 11, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:13:47,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,806 [api1.py:131] throughput: 3.5809747192188417
(INFO) 2023-04-10 17:13:47,819 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,820 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 9, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:13:47,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,845 [api1.py:131] throughput: 3.1309840082018234
(INFO) 2023-04-10 17:13:47,846 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,846 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:47,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,876 [api1.py:131] throughput: 0.08210630764534661
(INFO) 2023-04-10 17:13:47,877 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,877 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 11, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:13:47,878 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,899 [api1.py:131] throughput: 5.397183021007107
(INFO) 2023-04-10 17:13:47,900 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,900 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 12, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:13:47,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,919 [api1.py:131] throughput: 8.613224747245658
(INFO) 2023-04-10 17:13:47,920 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 10, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:13:47,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,941 [api1.py:131] throughput: 6.560249571986826
(INFO) 2023-04-10 17:13:47,942 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:13:47,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 74, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:13:47,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,961 [api1.py:131] throughput: 1.350561013475308
(INFO) 2023-04-10 17:13:47,966 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:47,966 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:13:47,969 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,969 [api2.py:131] throughput: 0.7475956676758319
(INFO) 2023-04-10 17:13:47,970 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:47,970 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 62}
(INFO) 2023-04-10 17:13:47,973 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,973 [api2.py:131] throughput: 0.8951116353039678
(INFO) 2023-04-10 17:13:47,977 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:47,977 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 119}
(INFO) 2023-04-10 17:13:47,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,982 [api2.py:131] throughput: 0.526213250685238
(INFO) 2023-04-10 17:13:47,982 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:47,982 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 503, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:13:47,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,986 [api2.py:131] throughput: 0.4368278388760611
(INFO) 2023-04-10 17:13:47,987 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:47,987 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:13:47,991 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,991 [api2.py:131] throughput: 1.378684865173633
(INFO) 2023-04-10 17:13:47,992 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:47,992 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 191, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:13:47,995 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:47,996 [api2.py:131] throughput: 0.8688003235019892
(INFO) 2023-04-10 17:13:47,996 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:47,996 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:13:48,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,000 [api2.py:131] throughput: 1.423321028448138
(INFO) 2023-04-10 17:13:48,001 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,001 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 490, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:13:48,005 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,005 [api2.py:131] throughput: 0.6653881810162428
(INFO) 2023-04-10 17:13:48,006 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,006 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 128, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:13:48,104 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,104 [api2.py:131] throughput: 1.3289224906174746
(INFO) 2023-04-10 17:13:48,105 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,105 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:13:48,109 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,109 [api2.py:131] throughput: 4.4732424655038034
(INFO) 2023-04-10 17:13:48,110 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,110 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 305, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 443}
(INFO) 2023-04-10 17:13:48,113 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,114 [api2.py:131] throughput: 409.6004856745848
(INFO) 2023-04-10 17:13:48,114 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,114 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 348, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:13:48,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,118 [api2.py:131] throughput: 0.5442555945454469
(INFO) 2023-04-10 17:13:48,119 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,119 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 348, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:13:48,122 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,123 [api2.py:131] throughput: 0.4110856250867444
(INFO) 2023-04-10 17:13:48,123 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,123 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 236, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 17:13:48,127 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,127 [api2.py:131] throughput: 45.239169501317626
(INFO) 2023-04-10 17:13:48,128 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,128 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 166, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:13:48,131 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,132 [api2.py:131] throughput: 0.7457465052483194
(INFO) 2023-04-10 17:13:48,132 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,132 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 434, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 17:13:48,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,136 [api2.py:131] throughput: 0.4319638423929024
(INFO) 2023-04-10 17:13:48,137 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,137 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 431, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:13:48,140 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,140 [api2.py:131] throughput: 0.5270063910748046
(INFO) 2023-04-10 17:13:48,141 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,141 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 434, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:13:48,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,145 [api2.py:131] throughput: 0.4791946845996259
(INFO) 2023-04-10 17:13:48,146 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,146 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:13:48,149 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,150 [api2.py:131] throughput: 1.203421054645157
(INFO) 2023-04-10 17:13:48,150 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,150 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 348, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:13:48,154 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,154 [api2.py:131] throughput: 0.4843749734285729
(INFO) 2023-04-10 17:13:48,155 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,155 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 448, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:13:48,159 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,159 [api2.py:131] throughput: 0.38051485995204315
(INFO) 2023-04-10 17:13:48,160 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:13:48,160 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 198, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:13:48,163 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,164 [api2.py:131] throughput: 1.009935940324912
(INFO) 2023-04-10 17:13:48,169 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,169 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 345, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:13:48,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,208 [api1.py:131] throughput: 0.3960427186473756
(INFO) 2023-04-10 17:13:48,208 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,208 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:13:48,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,249 [api1.py:131] throughput: 0.7611804766588478
(INFO) 2023-04-10 17:13:48,250 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 101, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:48,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,294 [api1.py:131] throughput: 0.19799489283318678
(INFO) 2023-04-10 17:13:48,295 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,295 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 108, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:13:48,296 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,336 [api1.py:131] throughput: 0.12123052441129806
(INFO) 2023-04-10 17:13:48,337 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,337 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:48,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,380 [api1.py:131] throughput: 0.06750079144710373
(INFO) 2023-04-10 17:13:48,381 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,381 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 452, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:13:48,382 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,423 [api1.py:131] throughput: 0.20758193058591606
(INFO) 2023-04-10 17:13:48,423 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,424 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:13:48,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,461 [api1.py:131] throughput: 0.5109513863669963
(INFO) 2023-04-10 17:13:48,462 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,462 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:48,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,503 [api1.py:131] throughput: 0.17027182187105055
(INFO) 2023-04-10 17:13:48,504 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,504 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 179, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:13:48,505 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,542 [api1.py:131] throughput: 0.42941473578352213
(INFO) 2023-04-10 17:13:48,543 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:13:48,543 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 108, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:13:48,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:48,588 [api1.py:131] throughput: 0.06899721180363882
(INFO) 2023-04-10 17:13:53,099 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:53,099 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 292, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:53,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:53,101 [api2.py:131] throughput: 0.5259967228883684
(INFO) 2023-04-10 17:13:53,102 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:53,102 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 396, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:13:53,103 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:53,104 [api2.py:131] throughput: 0.4734519306633419
(INFO) 2023-04-10 17:13:53,104 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:53,104 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 359, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:53,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:53,106 [api2.py:131] throughput: 0.4358713318986541
(INFO) 2023-04-10 17:13:53,107 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:53,107 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:53,109 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:53,109 [api2.py:131] throughput: 29.99086358331798
(INFO) 2023-04-10 17:13:53,110 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:53,110 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 188, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:13:53,111 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:53,111 [api2.py:131] throughput: 0.6535706072983382
(INFO) 2023-04-10 17:13:53,112 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:53,112 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 311, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:53,114 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:53,114 [api2.py:131] throughput: 0.43610827591887635
(INFO) 2023-04-10 17:13:57,303 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,303 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 140, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:57,306 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,306 [api2.py:131] throughput: 1.325209601912789
(INFO) 2023-04-10 17:13:57,307 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,307 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 416, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:57,309 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,310 [api2.py:131] throughput: 0.588210662361315
(INFO) 2023-04-10 17:13:57,310 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,310 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 230, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:13:57,313 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,313 [api2.py:131] throughput: 0.44599301774700506
(INFO) 2023-04-10 17:13:57,314 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,314 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 247, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:13:57,317 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,317 [api2.py:131] throughput: 0.5206279124544548
(INFO) 2023-04-10 17:13:57,318 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,318 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 252, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:13:57,321 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,321 [api2.py:131] throughput: 0.810031877530382
(INFO) 2023-04-10 17:13:57,322 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,322 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:13:57,324 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,324 [api2.py:131] throughput: 69.37808593726248
(INFO) 2023-04-10 17:13:57,325 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,325 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:13:57,328 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,328 [api2.py:131] throughput: 1.9295124036446236
(INFO) 2023-04-10 17:13:57,329 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,329 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 386, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:13:57,331 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,332 [api2.py:131] throughput: 68.25151546993088
(INFO) 2023-04-10 17:13:57,332 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,332 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 293, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:57,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,335 [api2.py:131] throughput: 0.7095873121446067
(INFO) 2023-04-10 17:13:57,336 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,336 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:13:57,339 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,339 [api2.py:131] throughput: 0.5150937247033852
(INFO) 2023-04-10 17:13:57,340 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,340 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 145, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:13:57,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,343 [api2.py:131] throughput: 0.7770241860650843
(INFO) 2023-04-10 17:13:57,344 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,344 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 493, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:13:57,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,347 [api2.py:131] throughput: 0.535367411618501
(INFO) 2023-04-10 17:13:57,347 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,347 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:13:57,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,351 [api2.py:131] throughput: 3.266643599995673
(INFO) 2023-04-10 17:13:57,351 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,351 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:13:57,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,354 [api2.py:131] throughput: 0.8141808974995619
(INFO) 2023-04-10 17:13:57,355 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,355 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 17:13:57,358 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,358 [api2.py:131] throughput: 206.14938852530722
(INFO) 2023-04-10 17:13:57,359 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,359 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 393, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:13:57,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,362 [api2.py:131] throughput: 62.81539439755678
(INFO) 2023-04-10 17:13:57,363 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,363 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 439, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:13:57,365 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,366 [api2.py:131] throughput: 0.6821131384940885
(INFO) 2023-04-10 17:13:57,366 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,366 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:13:57,369 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,370 [api2.py:131] throughput: 19.766857586445578
(INFO) 2023-04-10 17:13:57,370 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,370 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 17:13:57,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,373 [api2.py:131] throughput: 1.728984744823295
(INFO) 2023-04-10 17:13:57,374 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:13:57,374 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 208, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:13:57,377 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:13:57,377 [api2.py:131] throughput: 0.7488504858455165
(INFO) 2023-04-10 17:14:01,580 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,580 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 102}
(INFO) 2023-04-10 17:14:01,584 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,584 [api2.py:131] throughput: 85.96896197850637
(INFO) 2023-04-10 17:14:01,584 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,585 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 213, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:14:01,588 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,588 [api2.py:131] throughput: 0.5292380673624613
(INFO) 2023-04-10 17:14:01,589 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,589 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 483, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:01,592 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,592 [api2.py:131] throughput: 0.3686882613703455
(INFO) 2023-04-10 17:14:01,593 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,593 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 312, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:14:01,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,596 [api2.py:131] throughput: 0.5155090493170068
(INFO) 2023-04-10 17:14:01,597 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,597 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 303, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:14:01,600 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,600 [api2.py:131] throughput: 0.6303368542848664
(INFO) 2023-04-10 17:14:01,601 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,601 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 250, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:01,604 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,604 [api2.py:131] throughput: 0.639062912434692
(INFO) 2023-04-10 17:14:01,605 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,605 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 243, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:01,608 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,609 [api2.py:131] throughput: 0.3969819675988094
(INFO) 2023-04-10 17:14:01,609 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,609 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 305, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 17:14:01,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,613 [api2.py:131] throughput: 0.7357986884187763
(INFO) 2023-04-10 17:14:01,613 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,614 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 150, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:14:01,617 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,617 [api2.py:131] throughput: 0.3414683024107833
(INFO) 2023-04-10 17:14:01,617 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,618 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 413, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:14:01,621 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,621 [api2.py:131] throughput: 0.4540536776379772
(INFO) 2023-04-10 17:14:01,622 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,622 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 358, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:01,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,625 [api2.py:131] throughput: 0.5097943457645812
(INFO) 2023-04-10 17:14:01,626 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,626 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 117, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:14:01,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,629 [api2.py:131] throughput: 1.9631185508598716
(INFO) 2023-04-10 17:14:01,630 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,630 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 248, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:14:01,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,633 [api2.py:131] throughput: 0.7494068275572427
(INFO) 2023-04-10 17:14:01,634 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,634 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 482, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:01,637 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,637 [api2.py:131] throughput: 0.4158135452389522
(INFO) 2023-04-10 17:14:01,638 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,638 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 258, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:14:01,641 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,641 [api2.py:131] throughput: 0.765499135498177
(INFO) 2023-04-10 17:14:01,642 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,642 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 217, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:14:01,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,646 [api2.py:131] throughput: 0.7708251409791472
(INFO) 2023-04-10 17:14:01,646 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,646 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 381, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:14:01,650 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,650 [api2.py:131] throughput: 0.4800647618635398
(INFO) 2023-04-10 17:14:01,650 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,650 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 17:14:01,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,654 [api2.py:131] throughput: 0.48925519400620165
(INFO) 2023-04-10 17:14:01,655 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,655 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:01,658 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,658 [api2.py:131] throughput: 1.168334120278476
(INFO) 2023-04-10 17:14:01,659 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 38, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:14:01,659 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 410, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:01,662 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:01,663 [api2.py:131] throughput: 0.5140299904787188
(INFO) 2023-04-10 17:14:05,848 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:05,848 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 288, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:05,852 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:06,176 [api1.py:131] throughput: 0.05725282060931477
(INFO) 2023-04-10 17:14:06,177 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:06,177 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 174, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:14:06,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:06,515 [api1.py:131] throughput: 1.3594656761716402
(INFO) 2023-04-10 17:14:06,516 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:06,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 293, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:14:06,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:06,827 [api1.py:131] throughput: 0.3275237765359008
(INFO) 2023-04-10 17:14:06,829 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:06,829 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:06,833 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:07,176 [api1.py:131] throughput: 0.08371586255548384
(INFO) 2023-04-10 17:14:07,177 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:07,177 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 414, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 17:14:07,181 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:07,501 [api1.py:131] throughput: 0.9039124913991872
(INFO) 2023-04-10 17:14:07,502 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:07,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:14:07,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:07,976 [api1.py:131] throughput: 0.631377297733189
(INFO) 2023-04-10 17:14:07,977 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:07,977 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 272, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:14:07,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:08,295 [api1.py:131] throughput: 0.30909835588116874
(INFO) 2023-04-10 17:14:08,296 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:08,296 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 282, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 59}
(INFO) 2023-04-10 17:14:08,301 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:08,621 [api1.py:131] throughput: 1.211028509405089
(INFO) 2023-04-10 17:14:08,622 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:08,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:08,627 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:08,958 [api1.py:131] throughput: 0.26608169092726
(INFO) 2023-04-10 17:14:08,959 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:08,959 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 414, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:14:08,964 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:09,280 [api1.py:131] throughput: 0.30895787875381914
(INFO) 2023-04-10 17:14:09,281 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:09,282 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 310, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:14:09,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:09,719 [api1.py:131] throughput: 0.6285956142889699
(INFO) 2023-04-10 17:14:09,720 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:09,720 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 186, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:09,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:10,047 [api1.py:131] throughput: 0.15115324841951316
(INFO) 2023-04-10 17:14:10,048 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:10,048 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 431, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:10,052 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:10,386 [api1.py:131] throughput: 0.038538628263596705
(INFO) 2023-04-10 17:14:10,387 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:10,387 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:10,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:10,728 [api1.py:131] throughput: 0.23547597789921107
(INFO) 2023-04-10 17:14:10,729 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:10,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:10,734 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:11,177 [api1.py:131] throughput: 0.3682716700586709
(INFO) 2023-04-10 17:14:11,178 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:11,178 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 503, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:11,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:11,515 [api1.py:131] throughput: 0.027518496843037917
(INFO) 2023-04-10 17:14:11,516 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:11,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 455, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:14:11,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:11,922 [api1.py:131] throughput: 0.32412258103314173
(INFO) 2023-04-10 17:14:11,923 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:11,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:14:11,927 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:12,173 [api1.py:131] throughput: 0.990533689805606
(INFO) 2023-04-10 17:14:12,174 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:12,174 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 275, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:12,179 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:12,667 [api1.py:131] throughput: 0.02023409303872287
(INFO) 2023-04-10 17:14:12,668 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:12,668 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 364, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:14:12,672 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:12,963 [api1.py:131] throughput: 0.7263016167486499
(INFO) 2023-04-10 17:14:17,068 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:17,068 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 109, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:14:17,072 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:17,072 [api2.py:131] throughput: 1.1400874512526467
(INFO) 2023-04-10 17:14:17,073 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:17,073 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 10, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:14:17,077 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:17,077 [api2.py:131] throughput: 2.3867757565262586
(INFO) 2023-04-10 17:14:25,384 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:25,384 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 145, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:14:25,385 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:25,437 [api1.py:131] throughput: 0.5110100556313462
(INFO) 2023-04-10 17:14:25,437 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:25,438 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 160, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:14:25,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:25,499 [api1.py:131] throughput: 0.2890243459786657
(INFO) 2023-04-10 17:14:25,500 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:25,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:25,502 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:25,565 [api1.py:131] throughput: 0.25057352298872726
(INFO) 2023-04-10 17:14:25,566 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:14:25,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 144, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:25,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:25,632 [api1.py:131] throughput: 0.09114607213604231
(INFO) 2023-04-10 17:14:29,778 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,778 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 343}
(INFO) 2023-04-10 17:14:29,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,782 [api2.py:131] throughput: 41.104645894905936
(INFO) 2023-04-10 17:14:29,783 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,783 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 50}
(INFO) 2023-04-10 17:14:29,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,787 [api2.py:131] throughput: 0.3989493556289115
(INFO) 2023-04-10 17:14:29,788 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,788 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:14:29,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,792 [api2.py:131] throughput: 0.3355673173704524
(INFO) 2023-04-10 17:14:29,792 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,792 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 398, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:14:29,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,797 [api2.py:131] throughput: 0.41013689815516874
(INFO) 2023-04-10 17:14:29,797 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,797 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:29,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,801 [api2.py:131] throughput: 0.6130811529265533
(INFO) 2023-04-10 17:14:29,802 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,802 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:14:29,806 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,806 [api2.py:131] throughput: 2.3205676479663717
(INFO) 2023-04-10 17:14:29,807 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,807 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 84, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 187}
(INFO) 2023-04-10 17:14:29,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,811 [api2.py:131] throughput: 1.4716991717733061
(INFO) 2023-04-10 17:14:29,812 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,812 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 92}
(INFO) 2023-04-10 17:14:29,816 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,816 [api2.py:131] throughput: 1.1875476448071496
(INFO) 2023-04-10 17:14:29,816 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,817 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:14:29,820 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,821 [api2.py:131] throughput: 11.890264027126383
(INFO) 2023-04-10 17:14:29,821 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,821 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:14:29,825 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,825 [api2.py:131] throughput: 0.4011297117232475
(INFO) 2023-04-10 17:14:29,826 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,826 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:29,830 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,830 [api2.py:131] throughput: 2.8565043307470064
(INFO) 2023-04-10 17:14:29,831 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,831 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 76, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:14:29,834 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,835 [api2.py:131] throughput: 0.6363733054468214
(INFO) 2023-04-10 17:14:29,835 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,835 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:14:29,839 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,840 [api2.py:131] throughput: 0.37407820834449357
(INFO) 2023-04-10 17:14:29,840 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,840 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:14:29,844 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,845 [api2.py:131] throughput: 0.846787544031132
(INFO) 2023-04-10 17:14:29,845 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,845 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:14:29,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,850 [api2.py:131] throughput: 1.0074530003882454
(INFO) 2023-04-10 17:14:29,850 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,850 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 152, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 17:14:29,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,855 [api2.py:131] throughput: 7.2100296635511585
(INFO) 2023-04-10 17:14:29,855 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:29,855 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 17:14:29,859 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:29,859 [api2.py:131] throughput: 7.765346362970018
(INFO) 2023-04-10 17:14:33,993 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:33,994 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 228, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:33,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:33,998 [api2.py:131] throughput: 1.641555155450806
(INFO) 2023-04-10 17:14:33,999 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:33,999 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:14:34,003 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,004 [api2.py:131] throughput: 55.04792807475775
(INFO) 2023-04-10 17:14:34,004 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,004 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:34,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,009 [api2.py:131] throughput: 2.0947271566713344
(INFO) 2023-04-10 17:14:34,009 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,010 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 279, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:14:34,014 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,014 [api2.py:131] throughput: 0.6058419501549149
(INFO) 2023-04-10 17:14:34,015 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,015 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 244, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:14:34,125 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,125 [api2.py:131] throughput: 0.551513894822458
(INFO) 2023-04-10 17:14:34,126 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,126 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 508, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:14:34,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,131 [api2.py:131] throughput: 0.295059390135525
(INFO) 2023-04-10 17:14:34,131 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,131 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:34,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,136 [api2.py:131] throughput: 1.1977448053028583
(INFO) 2023-04-10 17:14:34,137 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,137 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-10 17:14:34,141 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,141 [api2.py:131] throughput: 0.986084395115431
(INFO) 2023-04-10 17:14:34,142 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,142 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:34,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,146 [api2.py:131] throughput: 24.811003445212876
(INFO) 2023-04-10 17:14:34,147 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,147 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:14:34,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,151 [api2.py:131] throughput: 3.3276893311272318
(INFO) 2023-04-10 17:14:34,152 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,152 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 284, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:34,156 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,157 [api2.py:131] throughput: 2.633130867739572
(INFO) 2023-04-10 17:14:34,157 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,158 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 506, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:14:34,162 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,162 [api2.py:131] throughput: 0.19403437948264696
(INFO) 2023-04-10 17:14:34,163 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,163 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:14:34,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,167 [api2.py:131] throughput: 1.9884092884105573
(INFO) 2023-04-10 17:14:34,168 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,168 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 53}
(INFO) 2023-04-10 17:14:34,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,173 [api2.py:131] throughput: 222.76544037200304
(INFO) 2023-04-10 17:14:34,174 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,174 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 181, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:34,178 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,178 [api2.py:131] throughput: 0.8404670401878528
(INFO) 2023-04-10 17:14:34,179 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,179 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:34,183 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,184 [api2.py:131] throughput: 0.7586789986101017
(INFO) 2023-04-10 17:14:34,184 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,184 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 167, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:34,189 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,189 [api2.py:131] throughput: 1.153087012287226
(INFO) 2023-04-10 17:14:34,190 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,190 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 394, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 78}
(INFO) 2023-04-10 17:14:34,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,194 [api2.py:131] throughput: 57.24952516655544
(INFO) 2023-04-10 17:14:34,195 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,195 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 433, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 164}
(INFO) 2023-04-10 17:14:34,199 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,200 [api2.py:131] throughput: 122.11582073360795
(INFO) 2023-04-10 17:14:34,200 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:14:34,200 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 490, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:14:34,205 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:34,205 [api2.py:131] throughput: 0.21919829335159854
(INFO) 2023-04-10 17:14:38,403 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:38,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:14:38,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:38,615 [api1.py:131] throughput: 0.30499689104225086
(INFO) 2023-04-10 17:14:38,616 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:38,617 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:38,619 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:38,728 [api1.py:131] throughput: 0.23865948600879133
(INFO) 2023-04-10 17:14:38,729 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:38,729 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 171, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:14:38,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:38,826 [api1.py:131] throughput: 0.3776727925107287
(INFO) 2023-04-10 17:14:38,827 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:38,827 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 436, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:14:38,829 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:38,938 [api1.py:131] throughput: 0.08866928512598607
(INFO) 2023-04-10 17:14:38,939 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:38,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 153}
(INFO) 2023-04-10 17:14:38,942 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,019 [api1.py:131] throughput: 3.192948783519956
(INFO) 2023-04-10 17:14:39,020 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 510, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:14:39,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,123 [api1.py:131] throughput: 0.27290632195759973
(INFO) 2023-04-10 17:14:39,124 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,124 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:39,127 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,237 [api1.py:131] throughput: 0.4124057003965852
(INFO) 2023-04-10 17:14:39,238 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 486, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:39,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,356 [api1.py:131] throughput: 0.2702522021471351
(INFO) 2023-04-10 17:14:39,357 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:39,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,474 [api1.py:131] throughput: 0.567472610075159
(INFO) 2023-04-10 17:14:39,475 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:14:39,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,578 [api1.py:131] throughput: 0.37486581260530183
(INFO) 2023-04-10 17:14:39,579 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 154, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:14:39,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,703 [api1.py:131] throughput: 0.3845714498206179
(INFO) 2023-04-10 17:14:39,705 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 168, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:39,707 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,817 [api1.py:131] throughput: 0.2745900477911959
(INFO) 2023-04-10 17:14:39,818 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,818 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:14:39,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:39,921 [api1.py:131] throughput: 1.2784040935112706
(INFO) 2023-04-10 17:14:39,923 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:39,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 502, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:14:39,925 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,031 [api1.py:131] throughput: 0.0810420668218282
(INFO) 2023-04-10 17:14:40,032 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:40,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:14:40,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,122 [api1.py:131] throughput: 0.6839895200160716
(INFO) 2023-04-10 17:14:40,123 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:40,123 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:40,125 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,229 [api1.py:131] throughput: 0.12709695538165602
(INFO) 2023-04-10 17:14:40,230 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:40,231 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:40,233 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,339 [api1.py:131] throughput: 0.13551502021597892
(INFO) 2023-04-10 17:14:40,340 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:40,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 290, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:40,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,582 [api1.py:131] throughput: 0.1651560284530972
(INFO) 2023-04-10 17:14:40,584 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:40,584 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 507, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:40,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,693 [api1.py:131] throughput: 0.16322548834670894
(INFO) 2023-04-10 17:14:40,694 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:14:40,694 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 142, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:14:40,696 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:40,800 [api1.py:131] throughput: 0.15838806326965885
(INFO) 2023-04-10 17:14:45,000 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,000 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 285, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 314}
(INFO) 2023-04-10 17:14:45,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,006 [api2.py:131] throughput: 237.6160430442741
(INFO) 2023-04-10 17:14:45,007 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,007 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 472, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:14:45,013 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,013 [api2.py:131] throughput: 0.4363995469727825
(INFO) 2023-04-10 17:14:45,014 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,014 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 202, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 44}
(INFO) 2023-04-10 17:14:45,019 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,019 [api2.py:131] throughput: 0.5317707645849039
(INFO) 2023-04-10 17:14:45,020 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,020 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:14:45,026 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,026 [api2.py:131] throughput: 3.411755892347155
(INFO) 2023-04-10 17:14:45,027 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,027 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 108, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 70}
(INFO) 2023-04-10 17:14:45,033 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,033 [api2.py:131] throughput: 0.6963983711382493
(INFO) 2023-04-10 17:14:45,034 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,034 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 153, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:14:45,040 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,040 [api2.py:131] throughput: 1.6566638326847416
(INFO) 2023-04-10 17:14:45,040 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,041 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 102, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:45,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,047 [api2.py:131] throughput: 1.1708447803682953
(INFO) 2023-04-10 17:14:45,047 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,047 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 320, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:45,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,167 [api2.py:131] throughput: 0.5919125536626432
(INFO) 2023-04-10 17:14:45,168 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,168 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 309, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:14:45,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,174 [api2.py:131] throughput: 0.5725162102252849
(INFO) 2023-04-10 17:14:45,175 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,175 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 269}
(INFO) 2023-04-10 17:14:45,181 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,181 [api2.py:131] throughput: 0.5480110154592347
(INFO) 2023-04-10 17:14:45,182 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,182 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 420, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:14:45,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,188 [api2.py:131] throughput: 0.508506538199701
(INFO) 2023-04-10 17:14:45,188 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,188 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 92}
(INFO) 2023-04-10 17:14:45,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,194 [api2.py:131] throughput: 0.3981229021274406
(INFO) 2023-04-10 17:14:45,195 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,195 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 388, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:45,201 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,201 [api2.py:131] throughput: 0.4410021981412349
(INFO) 2023-04-10 17:14:45,201 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,201 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 416, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:14:45,207 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,208 [api2.py:131] throughput: 13.78239977908407
(INFO) 2023-04-10 17:14:45,208 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,208 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 426, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:14:45,214 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,214 [api2.py:131] throughput: 0.34468052048968967
(INFO) 2023-04-10 17:14:45,215 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,215 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 134, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:14:45,221 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,221 [api2.py:131] throughput: 0.6117201881087202
(INFO) 2023-04-10 17:14:45,222 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,222 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:14:45,228 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,228 [api2.py:131] throughput: 0.2951129717117038
(INFO) 2023-04-10 17:14:45,229 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,229 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 505, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 75}
(INFO) 2023-04-10 17:14:45,235 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,235 [api2.py:131] throughput: 0.24797036252604537
(INFO) 2023-04-10 17:14:45,236 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,236 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 352, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:14:45,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,242 [api2.py:131] throughput: 0.6492606519468547
(INFO) 2023-04-10 17:14:45,242 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 30}
(INFO) 2023-04-10 17:14:45,242 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 331, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 101}
(INFO) 2023-04-10 17:14:45,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:45,248 [api2.py:131] throughput: 70.6942022157768
(INFO) 2023-04-10 17:14:49,401 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,401 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 401, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:14:49,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,405 [api2.py:131] throughput: 0.352478497540892
(INFO) 2023-04-10 17:14:49,406 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,406 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:14:49,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,410 [api2.py:131] throughput: 1.310669041509379
(INFO) 2023-04-10 17:14:49,410 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,410 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 253, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:14:49,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,415 [api2.py:131] throughput: 0.5392179523723412
(INFO) 2023-04-10 17:14:49,415 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,415 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 233, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:14:49,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,420 [api2.py:131] throughput: 0.7961460242895024
(INFO) 2023-04-10 17:14:49,420 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,420 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 196, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 17:14:49,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,425 [api2.py:131] throughput: 1.1441003135038759
(INFO) 2023-04-10 17:14:49,425 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,425 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:14:49,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,430 [api2.py:131] throughput: 22.251271797377452
(INFO) 2023-04-10 17:14:49,430 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,430 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 415, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:14:49,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,435 [api2.py:131] throughput: 0.7536733722640218
(INFO) 2023-04-10 17:14:49,435 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,435 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:14:49,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,440 [api2.py:131] throughput: 1.621680642269756
(INFO) 2023-04-10 17:14:49,440 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,440 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 494, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:14:49,444 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,445 [api2.py:131] throughput: 0.4844304344453928
(INFO) 2023-04-10 17:14:49,445 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,445 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 381, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:14:49,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,450 [api2.py:131] throughput: 0.5100421512257841
(INFO) 2023-04-10 17:14:49,451 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,451 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 223, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:14:49,455 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,455 [api2.py:131] throughput: 0.6362278238964072
(INFO) 2023-04-10 17:14:49,456 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,456 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 141, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 507}
(INFO) 2023-04-10 17:14:49,460 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,460 [api2.py:131] throughput: 257.8413049931258
(INFO) 2023-04-10 17:14:49,461 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,461 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-10 17:14:49,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,465 [api2.py:131] throughput: 0.7095860100402304
(INFO) 2023-04-10 17:14:49,466 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,466 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 190, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:14:49,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,470 [api2.py:131] throughput: 0.7763642049981382
(INFO) 2023-04-10 17:14:49,471 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,471 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 131, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:14:49,476 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,476 [api2.py:131] throughput: 1.6332123800041283
(INFO) 2023-04-10 17:14:49,476 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,477 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 74}
(INFO) 2023-04-10 17:14:49,481 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,481 [api2.py:131] throughput: 88.3596107676899
(INFO) 2023-04-10 17:14:49,482 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,482 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 112, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:14:49,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,486 [api2.py:131] throughput: 1.0720127937892951
(INFO) 2023-04-10 17:14:49,487 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,487 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 311, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:14:49,491 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,491 [api2.py:131] throughput: 9.488517764152284
(INFO) 2023-04-10 17:14:49,492 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,492 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:14:49,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,496 [api2.py:131] throughput: 1.1023152662542692
(INFO) 2023-04-10 17:14:49,497 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:14:49,497 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:14:49,501 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:14:49,501 [api2.py:131] throughput: 5.752514286342681
