(DEBUG) 2023-04-10 15:57:37,609 [logger.py:40] logger init.
(INFO) 2023-04-10 15:57:37,609 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-15-57-37-609570.log
(INFO) 2023-04-10 15:57:41,678 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:57:41,679 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:41,687 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:41,688 [api2.py:131] throughput: 0.700674441190112
(INFO) 2023-04-10 15:57:41,788 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:41,789 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:41,795 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:57:41,795 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:57:41,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:41,800 [api2.py:131] throughput: 0.6995761996484899
(INFO) 2023-04-10 15:57:41,804 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:41,805 [api2.py:131] throughput: 0.394189744203154
(INFO) 2023-04-10 15:57:41,904 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:41,905 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:57:41,904 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:57:41,905 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:57:41,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:41,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:41,915 [api2.py:131] throughput: 0.3935325035892898
(INFO) 2023-04-10 15:57:41,915 [api2.py:131] throughput: 2.27519067189922
(INFO) 2023-04-10 15:57:42,016 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,017 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:57:42,020 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:57:42,021 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:42,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,028 [api2.py:131] throughput: 0.330967406335876
(INFO) 2023-04-10 15:57:42,029 [api2.py:131] throughput: 2.1178533259771646
(INFO) 2023-04-10 15:57:42,136 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:57:42,137 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:42,137 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,138 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:42,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,147 [api2.py:131] throughput: 0.9933193720990079
(INFO) 2023-04-10 15:57:42,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,148 [api2.py:131] throughput: 0.32990957922709396
(INFO) 2023-04-10 15:57:42,245 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:57:42,246 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:42,250 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,251 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:42,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,255 [api2.py:131] throughput: 0.6702244149711424
(INFO) 2023-04-10 15:57:42,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,263 [api2.py:131] throughput: 0.9013537254215694
(INFO) 2023-04-10 15:57:42,355 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,355 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:42,366 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,367 [api2.py:131] throughput: 0.5971292547704609
(INFO) 2023-04-10 15:57:42,448 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:42,449 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:57:42,465 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,465 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:42,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,475 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,476 [api2.py:131] throughput: 1.1294578436094478
(INFO) 2023-04-10 15:57:42,575 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,575 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 15:57:42,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,586 [api2.py:131] throughput: 0.3939654504719718
(INFO) 2023-04-10 15:57:42,689 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,690 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:57:42,700 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,701 [api2.py:131] throughput: 1.021638866974054
(INFO) 2023-04-10 15:57:42,804 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-10 15:57:42,805 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 142, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:57:42,814 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:42,815 [api2.py:131] throughput: 1.0386650078870738
(INFO) 2023-04-10 15:57:43,028 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,029 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:57:43,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,038 [api2.py:131] throughput: 0.7025142816251339
(INFO) 2023-04-10 15:57:43,119 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,120 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:43,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,131 [api2.py:131] throughput: 0.39056414213595836
(INFO) 2023-04-10 15:57:43,237 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,238 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:57:43,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,250 [api2.py:131] throughput: 2.2305473366182715
(INFO) 2023-04-10 15:57:43,354 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,354 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:43,363 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,365 [api2.py:131] throughput: 0.3308168087350567
(INFO) 2023-04-10 15:57:43,473 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,474 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:43,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,486 [api2.py:131] throughput: 0.9923011907258648
(INFO) 2023-04-10 15:57:43,584 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,584 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:43,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,595 [api2.py:131] throughput: 0.6679775699758339
(INFO) 2023-04-10 15:57:43,694 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,694 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:43,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,706 [api2.py:131] throughput: 6.674349589481625
(INFO) 2023-04-10 15:57:43,809 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,810 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 15:57:43,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,822 [api2.py:131] throughput: 0.39668896431851114
(INFO) 2023-04-10 15:57:43,928 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:43,928 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:43,938 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:43,939 [api2.py:131] throughput: 1.0002827248086839
(INFO) 2023-04-10 15:57:44,044 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:57:44,044 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 141, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:44,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,057 [api2.py:131] throughput: 1.0415602013587701
(INFO) 2023-04-10 15:57:44,229 [api1.py:131] throughput: 0.2749445639862451
(INFO) 2023-04-10 15:57:44,267 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,268 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:57:44,278 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,279 [api2.py:131] throughput: 0.7009874309283479
(INFO) 2023-04-10 15:57:44,325 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:44,325 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 15:57:44,347 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,381 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,382 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:44,393 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,394 [api2.py:131] throughput: 0.3717625070537966
(INFO) 2023-04-10 15:57:44,498 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,498 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:44,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,508 [api2.py:131] throughput: 2.000825921239848
(INFO) 2023-04-10 15:57:44,599 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,599 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:57:44,608 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,609 [api2.py:131] throughput: 0.3328752912514996
(INFO) 2023-04-10 15:57:44,711 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,711 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:44,720 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,721 [api2.py:131] throughput: 1.256682525718644
(INFO) 2023-04-10 15:57:44,827 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,827 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:44,838 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,839 [api2.py:131] throughput: 0.650793444034905
(INFO) 2023-04-10 15:57:44,945 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:44,946 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:44,954 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:44,955 [api2.py:131] throughput: 1.0811059437307364
(INFO) 2023-04-10 15:57:45,059 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:45,060 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:57:45,072 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:45,073 [api2.py:131] throughput: 0.3586701432884583
(INFO) 2023-04-10 15:57:45,199 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:45,200 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:57:45,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:45,207 [api2.py:131] throughput: 1.0247282182145505
(INFO) 2023-04-10 15:57:45,304 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:57:45,305 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 74, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:45,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:45,312 [api2.py:131] throughput: 1.3415562950718005
(INFO) 2023-04-10 15:57:45,645 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:45,646 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:57:45,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:45,658 [api2.py:131] throughput: 0.7026266367279153
(INFO) 2023-04-10 15:57:45,762 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:45,763 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 15:57:45,773 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:45,774 [api2.py:131] throughput: 0.3944133073358651
(INFO) 2023-04-10 15:57:45,884 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:45,884 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:57:45,895 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:45,896 [api2.py:131] throughput: 2.2492763016479183
(INFO) 2023-04-10 15:57:45,998 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:45,999 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:46,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,011 [api2.py:131] throughput: 0.3314444772696657
(INFO) 2023-04-10 15:57:46,075 [api1.py:131] throughput: 0.4892947685034509
(INFO) 2023-04-10 15:57:46,118 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:46,119 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:57:46,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,131 [api2.py:131] throughput: 0.9895739125896855
(INFO) 2023-04-10 15:57:46,181 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:46,181 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 15:57:46,207 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,232 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:46,233 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:46,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,245 [api2.py:131] throughput: 0.634628057235999
(INFO) 2023-04-10 15:57:46,346 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:46,347 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:46,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,357 [api2.py:131] throughput: 0.9170653626672172
(INFO) 2023-04-10 15:57:46,461 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:46,461 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 222, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:57:46,474 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,475 [api2.py:131] throughput: 0.3070477736350575
(INFO) 2023-04-10 15:57:46,583 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:46,584 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:46,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,596 [api2.py:131] throughput: 0.976966130683462
(INFO) 2023-04-10 15:57:46,703 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:57:46,704 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:46,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:46,716 [api2.py:131] throughput: 0.7840875667813838
(INFO) 2023-04-10 15:57:46,934 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:46,935 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:46,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:47,110 [api1.py:131] throughput: 0.29766136656076203
(INFO) 2023-04-10 15:57:47,219 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:47,220 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:57:47,230 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:47,381 [api1.py:131] throughput: 0.222381097523117
(INFO) 2023-04-10 15:57:47,488 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:47,488 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:47,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:47,666 [api1.py:131] throughput: 0.06659062780181832
(INFO) 2023-04-10 15:57:47,773 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:47,774 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:47,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:47,944 [api1.py:131] throughput: 0.07587741303562563
(INFO) 2023-04-10 15:57:47,950 [api1.py:131] throughput: 0.5293728875899902
(INFO) 2023-04-10 15:57:48,049 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:48,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:48,056 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:48,057 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:57:48,059 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:48,080 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:48,222 [api1.py:131] throughput: 0.21029944810964415
(INFO) 2023-04-10 15:57:48,331 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:48,331 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:48,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:48,509 [api1.py:131] throughput: 0.026092915943925016
(INFO) 2023-04-10 15:57:48,622 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:48,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 441, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:48,632 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:48,802 [api1.py:131] throughput: 0.03051553071388044
(INFO) 2023-04-10 15:57:48,906 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:48,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 383, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 15:57:48,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:49,058 [api1.py:131] throughput: 0.28035868238124984
(INFO) 2023-04-10 15:57:49,166 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:49,167 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:57:49,176 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:49,322 [api1.py:131] throughput: 0.34238225082541124
(INFO) 2023-04-10 15:57:49,425 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:57:49,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:49,435 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:49,597 [api1.py:131] throughput: 0.04537662929590372
(INFO) 2023-04-10 15:57:49,712 [api1.py:131] throughput: 0.07781943663818329
(INFO) 2023-04-10 15:57:49,815 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:49,816 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 15:57:49,843 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:51,646 [api1.py:131] throughput: 0.7329937540123277
(INFO) 2023-04-10 15:57:51,753 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:51,754 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:51,777 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:53,597 [api1.py:131] throughput: 0.02236620145809129
(INFO) 2023-04-10 15:57:53,693 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:53,693 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 441, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:57:53,718 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,382 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:55,382 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 56}
(INFO) 2023-04-10 15:57:55,397 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,398 [api2.py:131] throughput: 0.6713231879150322
(INFO) 2023-04-10 15:57:55,499 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:55,499 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 130, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 185}
(INFO) 2023-04-10 15:57:55,509 [api1.py:131] throughput: 0.05069800037695961
(INFO) 2023-04-10 15:57:55,514 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,515 [api2.py:131] throughput: 1.6256745034068623
(INFO) 2023-04-10 15:57:55,606 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:55,606 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 383, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 77}
(INFO) 2023-04-10 15:57:55,607 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:55,607 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 192, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:57:55,623 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,624 [api2.py:131] throughput: 0.7143000445467468
(INFO) 2023-04-10 15:57:55,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,730 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:55,730 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 125}
(INFO) 2023-04-10 15:57:55,744 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,745 [api2.py:131] throughput: 0.9049321699583228
(INFO) 2023-04-10 15:57:55,840 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:55,840 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:57:55,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,855 [api2.py:131] throughput: 0.9712239007789812
(INFO) 2023-04-10 15:57:55,951 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:55,952 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 86, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 61}
(INFO) 2023-04-10 15:57:55,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:55,968 [api2.py:131] throughput: 1.1036574036262212
(INFO) 2023-04-10 15:57:56,066 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:56,067 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 15:57:56,079 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:56,080 [api2.py:131] throughput: 0.7575973848923404
(INFO) 2023-04-10 15:57:56,178 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:56,179 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 103, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 15:57:56,192 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:56,193 [api2.py:131] throughput: 0.3958543557165469
(INFO) 2023-04-10 15:57:56,269 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:56,269 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:57:56,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:56,284 [api2.py:131] throughput: 2.885521128758405
(INFO) 2023-04-10 15:57:56,383 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 61}
(INFO) 2023-04-10 15:57:56,384 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 181, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 112}
(INFO) 2023-04-10 15:57:56,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:56,400 [api2.py:131] throughput: 0.768730607497937
(INFO) 2023-04-10 15:57:56,933 [api1.py:131] throughput: 0.7266576890313936
(INFO) 2023-04-10 15:57:57,035 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:57,036 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 15:57:57,060 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:57:58,892 [api1.py:131] throughput: 0.3154241774143924
(INFO) 2023-04-10 15:57:58,991 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 29}
(INFO) 2023-04-10 15:57:58,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:57:59,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:00,764 [api1.py:131] throughput: 0.2690386582455611
(INFO) 2023-04-10 15:58:00,976 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:00,977 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:00,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:00,987 [api2.py:131] throughput: 0.7022895852653084
(INFO) 2023-04-10 15:58:01,087 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,088 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:58:01,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,099 [api2.py:131] throughput: 0.39421285369066406
(INFO) 2023-04-10 15:58:01,201 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,202 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 15:58:01,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,214 [api2.py:131] throughput: 2.3129822686170085
(INFO) 2023-04-10 15:58:01,309 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,310 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:01,321 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,322 [api2.py:131] throughput: 0.33143627612066223
(INFO) 2023-04-10 15:58:01,412 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,413 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:01,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,426 [api2.py:131] throughput: 0.9964644396772311
(INFO) 2023-04-10 15:58:01,529 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,530 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:01,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,542 [api2.py:131] throughput: 0.6770596091646283
(INFO) 2023-04-10 15:58:01,639 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,640 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 441, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:01,646 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,648 [api2.py:131] throughput: 0.4543431424958689
(INFO) 2023-04-10 15:58:01,753 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,754 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 383, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:58:01,762 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,764 [api2.py:131] throughput: 0.26774564429685466
(INFO) 2023-04-10 15:58:01,897 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:01,897 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:01,905 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:01,906 [api2.py:131] throughput: 1.00610157650255
(INFO) 2023-04-10 15:58:02,022 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:02,023 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:58:02,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:02,035 [api2.py:131] throughput: 0.8465673597405067
(INFO) 2023-04-10 15:58:02,244 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:58:02,245 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:02,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:02,334 [api1.py:131] throughput: 0.6348593733232457
(INFO) 2023-04-10 15:58:02,435 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:58:02,436 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:02,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:02,525 [api1.py:131] throughput: 0.11984563845826582
(INFO) 2023-04-10 15:58:02,633 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:58:02,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 15:58:02,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:02,704 [api1.py:131] throughput: 2.534755501389308
(INFO) 2023-04-10 15:58:02,805 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:58:02,806 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:02,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:02,885 [api1.py:131] throughput: 0.6150822368690394
(INFO) 2023-04-10 15:58:02,983 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:58:02,984 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:02,992 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:03,075 [api1.py:131] throughput: 0.8647207968619202
(INFO) 2023-04-10 15:58:03,177 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:58:03,178 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 222, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:03,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:03,281 [api1.py:131] throughput: 0.07160363157834161
(INFO) 2023-04-10 15:58:03,501 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:03,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:03,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:03,597 [api1.py:131] throughput: 0.24064703171704768
(INFO) 2023-04-10 15:58:03,701 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:03,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:58:03,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:03,788 [api1.py:131] throughput: 0.32485180362196286
(INFO) 2023-04-10 15:58:03,886 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:03,887 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:03,895 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:03,986 [api1.py:131] throughput: 0.1001186019375631
(INFO) 2023-04-10 15:58:04,098 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:04,099 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:04,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:04,191 [api1.py:131] throughput: 0.05700094522706638
(INFO) 2023-04-10 15:58:04,296 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:04,297 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:04,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:04,397 [api1.py:131] throughput: 0.23924800333186338
(INFO) 2023-04-10 15:58:04,500 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:04,500 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:04,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:04,596 [api1.py:131] throughput: 0.03937229989177041
(INFO) 2023-04-10 15:58:04,805 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:04,806 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:04,816 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:04,817 [api2.py:131] throughput: 0.694647153170619
(INFO) 2023-04-10 15:58:04,917 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:04,918 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 15:58:04,928 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:04,929 [api2.py:131] throughput: 0.39463581475237897
(INFO) 2023-04-10 15:58:05,033 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,034 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 15:58:05,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,047 [api2.py:131] throughput: 2.305035789134634
(INFO) 2023-04-10 15:58:05,147 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,147 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:05,161 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,162 [api2.py:131] throughput: 0.3299118747358695
(INFO) 2023-04-10 15:58:05,264 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,264 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:58:05,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,276 [api2.py:131] throughput: 1.0072155558808396
(INFO) 2023-04-10 15:58:05,371 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,372 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 332, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:05,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,387 [api2.py:131] throughput: 0.6886977946474296
(INFO) 2023-04-10 15:58:05,488 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,489 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 441, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:05,499 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,501 [api2.py:131] throughput: 0.44772167674733643
(INFO) 2023-04-10 15:58:05,601 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,601 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 383, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:05,614 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,615 [api2.py:131] throughput: 0.2643620134902103
(INFO) 2023-04-10 15:58:05,718 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,719 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:05,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,732 [api2.py:131] throughput: 1.0216046460984973
(INFO) 2023-04-10 15:58:05,833 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:05,833 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 199, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:58:05,844 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:05,845 [api2.py:131] throughput: 0.8409278944237606
(INFO) 2023-04-10 15:58:10,897 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:10,898 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:10,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:10,917 [api2.py:131] throughput: 0.42021921788221717
(INFO) 2023-04-10 15:58:11,028 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,029 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 236, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 71}
(INFO) 2023-04-10 15:58:11,049 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,050 [api2.py:131] throughput: 1.0452060600719435
(INFO) 2023-04-10 15:58:11,158 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,159 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 192, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:58:11,181 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,182 [api2.py:131] throughput: 0.7617064905065472
(INFO) 2023-04-10 15:58:11,301 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,302 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 233, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 266}
(INFO) 2023-04-10 15:58:11,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,328 [api2.py:131] throughput: 0.6048917443160406
(INFO) 2023-04-10 15:58:11,439 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,440 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 372, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 15:58:11,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,463 [api2.py:131] throughput: 0.3944561272352167
(INFO) 2023-04-10 15:58:11,570 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,571 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 60, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 15:58:11,575 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:11,576 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 229, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:11,584 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,585 [api2.py:131] throughput: 13.846539401978374
(INFO) 2023-04-10 15:58:11,592 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,593 [api2.py:131] throughput: 1.189558798829541
(INFO) 2023-04-10 15:58:11,688 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:11,689 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:11,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,700 [api2.py:131] throughput: 8.481080541225921
(INFO) 2023-04-10 15:58:11,703 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,704 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 84}
(INFO) 2023-04-10 15:58:11,724 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,725 [api2.py:131] throughput: 0.7617578130202779
(INFO) 2023-04-10 15:58:11,808 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:11,809 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 15:58:11,818 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,819 [api2.py:131] throughput: 1.5467582086185905
(INFO) 2023-04-10 15:58:11,827 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,828 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 174, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 71}
(INFO) 2023-04-10 15:58:11,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,850 [api2.py:131] throughput: 0.39303445985688334
(INFO) 2023-04-10 15:58:11,925 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:11,926 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:11,933 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,934 [api2.py:131] throughput: 0.8698035126946666
(INFO) 2023-04-10 15:58:11,951 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:11,951 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:11,970 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:11,971 [api2.py:131] throughput: 2.9877508789928107
(INFO) 2023-04-10 15:58:12,043 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:12,043 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:12,053 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,054 [api2.py:131] throughput: 0.7365659330454262
(INFO) 2023-04-10 15:58:12,073 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:12,073 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 181, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 79}
(INFO) 2023-04-10 15:58:12,091 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,091 [api2.py:131] throughput: 0.7742236027810828
(INFO) 2023-04-10 15:58:12,164 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:12,165 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:12,171 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,173 [api2.py:131] throughput: 0.842846794586482
(INFO) 2023-04-10 15:58:12,287 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:12,288 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 144, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:12,296 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,297 [api2.py:131] throughput: 1.135119116530508
(INFO) 2023-04-10 15:58:12,423 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:12,424 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 364, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:12,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,430 [api2.py:131] throughput: 0.697287443403019
(INFO) 2023-04-10 15:58:12,534 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:12,534 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:58:12,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,544 [api2.py:131] throughput: 16.79610506654375
(INFO) 2023-04-10 15:58:12,641 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-10 15:58:12,641 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:12,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,653 [api2.py:131] throughput: 0.7670761509804594
(INFO) 2023-04-10 15:58:12,880 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:12,881 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:58:12,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:12,893 [api2.py:131] throughput: 1.7163523428456473
(INFO) 2023-04-10 15:58:13,002 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,003 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 400, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:13,015 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,016 [api2.py:131] throughput: 0.25051933516518876
(INFO) 2023-04-10 15:58:13,123 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,123 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 195, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:58:13,133 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,134 [api2.py:131] throughput: 0.8594278641846683
(INFO) 2023-04-10 15:58:13,241 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,242 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 15:58:13,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,256 [api2.py:131] throughput: 2.780026943829043
(INFO) 2023-04-10 15:58:13,370 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,370 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:58:13,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,380 [api2.py:131] throughput: 3.6157038063369003
(INFO) 2023-04-10 15:58:13,488 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,488 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 394, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:13,500 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,501 [api2.py:131] throughput: 0.5217601863028817
(INFO) 2023-04-10 15:58:13,599 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,599 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:13,612 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,613 [api2.py:131] throughput: 1.3753350883538802
(INFO) 2023-04-10 15:58:13,717 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,718 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 138, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 15:58:13,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,731 [api2.py:131] throughput: 17.68353113640042
(INFO) 2023-04-10 15:58:13,834 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,835 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 133, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 15:58:13,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,848 [api2.py:131] throughput: 0.7020871020775535
(INFO) 2023-04-10 15:58:13,943 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 49}
(INFO) 2023-04-10 15:58:13,944 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 15:58:13,953 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:13,954 [api2.py:131] throughput: 0.42039276615414356
(INFO) 2023-04-10 15:58:14,178 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:14,179 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 221, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:14,192 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:14,506 [api1.py:131] throughput: 0.13204446145330476
(INFO) 2023-04-10 15:58:14,602 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:14,602 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 474, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 226}
(INFO) 2023-04-10 15:58:14,615 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:14,825 [api1.py:131] throughput: 2.004626266502581
(INFO) 2023-04-10 15:58:14,932 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:14,933 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:58:14,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:15,294 [api1.py:131] throughput: 0.4065095665015928
(INFO) 2023-04-10 15:58:15,393 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:15,394 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 15:58:15,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:15,735 [api1.py:131] throughput: 1.3979868273916305
(INFO) 2023-04-10 15:58:15,824 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:15,825 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:15,838 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:16,205 [api1.py:131] throughput: 0.4905909052590162
(INFO) 2023-04-10 15:58:16,304 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:16,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 447, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:16,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:16,565 [api1.py:131] throughput: 0.04632435828860263
(INFO) 2023-04-10 15:58:16,657 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:16,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 15:58:16,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,045 [api1.py:131] throughput: 0.9170607280485412
(INFO) 2023-04-10 15:58:17,139 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:17,140 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 287, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:58:17,153 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,466 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:17,467 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:58:17,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,485 [api2.py:131] throughput: 0.4636317921083964
(INFO) 2023-04-10 15:58:17,524 [api1.py:131] throughput: 0.09384111360823534
(INFO) 2023-04-10 15:58:17,588 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:17,589 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 186}
(INFO) 2023-04-10 15:58:17,607 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,608 [api2.py:131] throughput: 45.24011177615992
(INFO) 2023-04-10 15:58:17,621 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:17,622 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 471, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 15:58:17,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,716 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:17,717 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 108, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:58:17,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,737 [api2.py:131] throughput: 1.3795374814597434
(INFO) 2023-04-10 15:58:17,844 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:17,845 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 144, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 189}
(INFO) 2023-04-10 15:58:17,864 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,865 [api2.py:131] throughput: 0.4852573638438554
(INFO) 2023-04-10 15:58:17,891 [api1.py:131] throughput: 0.1381345987601459
(INFO) 2023-04-10 15:58:17,977 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:17,977 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 180}
(INFO) 2023-04-10 15:58:17,980 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 9}
(INFO) 2023-04-10 15:58:17,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:58:17,992 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,993 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:17,995 [api2.py:131] throughput: 1.0831905972559257
(INFO) 2023-04-10 15:58:18,102 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:18,103 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 15:58:18,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:18,120 [api2.py:131] throughput: 3.394188632247673
(INFO) 2023-04-10 15:58:18,226 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:18,227 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 208, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 80}
(INFO) 2023-04-10 15:58:18,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:18,243 [api2.py:131] throughput: 0.6849530567066089
(INFO) 2023-04-10 15:58:18,345 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:18,345 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:18,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:18,363 [api2.py:131] throughput: 2.8203477483411725
(INFO) 2023-04-10 15:58:18,370 [api1.py:131] throughput: 0.640462566981698
(INFO) 2023-04-10 15:58:18,461 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:18,462 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 165, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:18,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:18,479 [api2.py:131] throughput: 0.541383208100111
(INFO) 2023-04-10 15:58:18,582 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 16}
(INFO) 2023-04-10 15:58:18,582 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 146, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:58:18,597 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:18,598 [api2.py:131] throughput: 0.904337659038916
(INFO) 2023-04-10 15:58:23,703 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:23,704 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 78, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 15:58:23,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:23,732 [api2.py:131] throughput: 0.6734510488187372
(INFO) 2023-04-10 15:58:23,836 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:23,836 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:58:23,865 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:23,866 [api2.py:131] throughput: 4.574390553105911
(INFO) 2023-04-10 15:58:23,966 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:23,967 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 15:58:23,993 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:23,994 [api2.py:131] throughput: 0.7908508833755931
(INFO) 2023-04-10 15:58:24,088 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,088 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 410}
(INFO) 2023-04-10 15:58:24,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,113 [api2.py:131] throughput: 94.64832679826878
(INFO) 2023-04-10 15:58:24,197 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,197 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 379, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 15:58:24,220 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,220 [api2.py:131] throughput: 0.4485491203668631
(INFO) 2023-04-10 15:58:24,310 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,311 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 86}
(INFO) 2023-04-10 15:58:24,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,335 [api2.py:131] throughput: 0.9006763900482401
(INFO) 2023-04-10 15:58:24,411 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,412 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 140, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:24,432 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,433 [api2.py:131] throughput: 1.0674407328655438
(INFO) 2023-04-10 15:58:24,523 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,523 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 692}
(INFO) 2023-04-10 15:58:24,547 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,548 [api2.py:131] throughput: 1.1246942946906586
(INFO) 2023-04-10 15:58:24,625 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,626 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 193}
(INFO) 2023-04-10 15:58:24,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,646 [api2.py:131] throughput: 0.6720288988956438
(INFO) 2023-04-10 15:58:24,732 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:24,733 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 15:58:24,760 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:24,761 [api2.py:131] throughput: 2.438101016940632
(INFO) 2023-04-10 15:58:27,822 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:27,823 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 15:58:27,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:27,855 [api2.py:131] throughput: 2.645919962109774
(INFO) 2023-04-10 15:58:27,967 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:27,967 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:58:27,995 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:27,996 [api2.py:131] throughput: 4.109521806028062
(INFO) 2023-04-10 15:58:28,105 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,106 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 853}
(INFO) 2023-04-10 15:58:28,135 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,136 [api2.py:131] throughput: 1.5326305792539772
(INFO) 2023-04-10 15:58:28,247 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,248 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 11, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 142}
(INFO) 2023-04-10 15:58:28,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,280 [api2.py:131] throughput: 1.0760377609410456
(INFO) 2023-04-10 15:58:28,389 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,390 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 128}
(INFO) 2023-04-10 15:58:28,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,420 [api2.py:131] throughput: 0.7157669586114914
(INFO) 2023-04-10 15:58:28,520 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,520 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 15:58:28,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,544 [api2.py:131] throughput: 0.8125123269238583
(INFO) 2023-04-10 15:58:28,631 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,632 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 11, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 457}
(INFO) 2023-04-10 15:58:28,662 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,664 [api2.py:131] throughput: 2.3565297799871496
(INFO) 2023-04-10 15:58:28,774 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,775 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 15:58:28,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,800 [api2.py:131] throughput: 3.480666287135571
(INFO) 2023-04-10 15:58:28,890 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:28,891 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 142}
(INFO) 2023-04-10 15:58:28,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:28,920 [api2.py:131] throughput: 10.497882379268022
(INFO) 2023-04-10 15:58:29,025 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 39, 'reticle_array_w': 48}
(INFO) 2023-04-10 15:58:29,026 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 9, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 56}
(INFO) 2023-04-10 15:58:29,055 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:29,056 [api2.py:131] throughput: 1.6708347670637944
(INFO) 2023-04-10 15:58:29,278 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:29,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 122}
(INFO) 2023-04-10 15:58:29,293 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:29,988 [api1.py:131] throughput: 7.480989714606309
(INFO) 2023-04-10 15:58:30,070 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:30,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:30,083 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:30,087 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:30,088 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 400, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 15:58:30,104 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:30,414 [api1.py:131] throughput: 0.17826375059905397
(INFO) 2023-04-10 15:58:30,513 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:30,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 177}
(INFO) 2023-04-10 15:58:30,528 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:30,784 [api1.py:131] throughput: 7.518514170497406
(INFO) 2023-04-10 15:58:30,877 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:30,878 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:58:30,889 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:30,899 [api1.py:131] throughput: 0.16128477455259127
(INFO) 2023-04-10 15:58:30,997 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:30,997 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 140, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 15:58:31,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:31,213 [api1.py:131] throughput: 0.542035156413778
(INFO) 2023-04-10 15:58:31,314 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:31,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 15:58:31,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:31,499 [api1.py:131] throughput: 10.337759795764907
(INFO) 2023-04-10 15:58:31,609 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:31,610 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:31,623 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:31,738 [api1.py:131] throughput: 0.5337599408639584
(INFO) 2023-04-10 15:58:31,828 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:31,829 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 15:58:31,846 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:31,957 [api1.py:131] throughput: 0.4602211373772241
(INFO) 2023-04-10 15:58:32,047 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:32,048 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 7, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:58:32,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:32,268 [api1.py:131] throughput: 1.94641246508237
(INFO) 2023-04-10 15:58:32,388 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:32,388 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 74, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:32,398 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:32,607 [api1.py:131] throughput: 2.2889752858396255
(INFO) 2023-04-10 15:58:32,708 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:32,709 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:32,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:32,749 [api1.py:131] throughput: 0.1905432168914371
(INFO) 2023-04-10 15:58:32,863 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:32,864 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 15:58:32,873 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:33,164 [api1.py:131] throughput: 5.2421151362297005
(INFO) 2023-04-10 15:58:33,267 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:33,268 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 177, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 235}
(INFO) 2023-04-10 15:58:33,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:33,415 [api1.py:131] throughput: 9.285700836245521
(INFO) 2023-04-10 15:58:33,494 [api1.py:131] throughput: 0.14658588907416611
(INFO) 2023-04-10 15:58:33,499 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 15:58:33,500 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 15:58:33,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:33,593 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:33,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 394, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:58:33,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:33,778 [api1.py:131] throughput: 5.1894619405271865
(INFO) 2023-04-10 15:58:34,435 [api1.py:131] throughput: 0.09330199909285145
(INFO) 2023-04-10 15:58:34,534 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:34,535 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 234, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:34,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:35,340 [api1.py:131] throughput: 0.0668443279627973
(INFO) 2023-04-10 15:58:35,439 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:35,439 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 138, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 97}
(INFO) 2023-04-10 15:58:35,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:36,091 [api1.py:131] throughput: 0.2818917048978836
(INFO) 2023-04-10 15:58:36,185 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:36,185 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 131, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 74}
(INFO) 2023-04-10 15:58:36,203 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:36,938 [api1.py:131] throughput: 0.4099662920270573
(INFO) 2023-04-10 15:58:37,027 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 36}
(INFO) 2023-04-10 15:58:37,028 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 152}
(INFO) 2023-04-10 15:58:37,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:37,685 [api1.py:131] throughput: 1.253754258736859
(INFO) 2023-04-10 15:58:37,903 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:37,904 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 221, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:37,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:37,963 [api1.py:131] throughput: 0.06682980066943742
(INFO) 2023-04-10 15:58:38,073 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 15:58:38,074 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 474, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 15:58:38,080 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:38,126 [api1.py:131] throughput: 0.3247656055321207
(INFO) 2023-04-10 15:58:38,547 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:38,548 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:38,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:38,573 [api2.py:131] throughput: 2.6000483926139393
(INFO) 2023-04-10 15:58:38,673 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:38,674 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 51, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 15:58:38,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:38,698 [api2.py:131] throughput: 0.3539980293367745
(INFO) 2023-04-10 15:58:38,788 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:38,789 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 15:58:38,813 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:38,814 [api2.py:131] throughput: 1.0670850085137795
(INFO) 2023-04-10 15:58:38,907 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:38,908 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 15:58:38,932 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:38,933 [api2.py:131] throughput: 4.031227799557326
(INFO) 2023-04-10 15:58:39,026 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:39,027 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 96}
(INFO) 2023-04-10 15:58:39,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:39,052 [api2.py:131] throughput: 2.8343474060010134
(INFO) 2023-04-10 15:58:39,145 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:39,146 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 209, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 15:58:39,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:39,171 [api2.py:131] throughput: 0.3092963002456913
(INFO) 2023-04-10 15:58:39,260 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:39,261 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:39,291 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:39,292 [api2.py:131] throughput: 1.9797634239273079
(INFO) 2023-04-10 15:58:39,401 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:39,402 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 98}
(INFO) 2023-04-10 15:58:39,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:39,425 [api2.py:131] throughput: 0.37976315104049146
(INFO) 2023-04-10 15:58:39,513 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:39,514 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:58:39,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:39,544 [api2.py:131] throughput: 2.0153667491249627
(INFO) 2023-04-10 15:58:39,644 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 56}
(INFO) 2023-04-10 15:58:39,645 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 176}
(INFO) 2023-04-10 15:58:39,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:39,675 [api2.py:131] throughput: 0.4399099720773139
(INFO) 2023-04-10 15:58:42,884 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:42,885 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 78, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 15:58:42,905 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:42,906 [api2.py:131] throughput: 0.6844016255004736
(INFO) 2023-04-10 15:58:43,021 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,022 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:43,041 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,042 [api2.py:131] throughput: 4.1814628600396135
(INFO) 2023-04-10 15:58:43,154 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,154 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 120}
(INFO) 2023-04-10 15:58:43,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,175 [api2.py:131] throughput: 0.7993056957983616
(INFO) 2023-04-10 15:58:43,284 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,284 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 151}
(INFO) 2023-04-10 15:58:43,305 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,306 [api2.py:131] throughput: 104.24973145499122
(INFO) 2023-04-10 15:58:43,414 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,414 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 379, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:43,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,435 [api2.py:131] throughput: 0.4472348964650534
(INFO) 2023-04-10 15:58:43,545 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,546 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 15:58:43,566 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,567 [api2.py:131] throughput: 0.893363281978051
(INFO) 2023-04-10 15:58:43,673 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,673 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:43,688 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,689 [api2.py:131] throughput: 0.7840167406766864
(INFO) 2023-04-10 15:58:43,777 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,778 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 236, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 185}
(INFO) 2023-04-10 15:58:43,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,797 [api2.py:131] throughput: 0.7908286353567131
(INFO) 2023-04-10 15:58:43,906 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:43,906 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 15:58:43,926 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:43,927 [api2.py:131] throughput: 0.6485605791994393
(INFO) 2023-04-10 15:58:44,034 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:58:44,035 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 15:58:44,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,057 [api2.py:131] throughput: 3.2711575423523818
(INFO) 2023-04-10 15:58:44,404 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:44,404 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 288, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:58:44,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,413 [api2.py:131] throughput: 0.35466382314577605
(INFO) 2023-04-10 15:58:44,537 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:44,537 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 300, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:44,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,546 [api2.py:131] throughput: 0.7329375895202433
(INFO) 2023-04-10 15:58:44,653 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:44,653 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 464, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:44,660 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,661 [api2.py:131] throughput: 0.28108955369725536
(INFO) 2023-04-10 15:58:44,752 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:44,753 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 15:58:44,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,764 [api2.py:131] throughput: 0.3798470320179233
(INFO) 2023-04-10 15:58:44,850 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:44,850 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 236, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:44,861 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,862 [api2.py:131] throughput: 0.4927491544077615
(INFO) 2023-04-10 15:58:44,959 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:44,959 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:58:44,969 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:44,970 [api2.py:131] throughput: 2.6402256108088022
(INFO) 2023-04-10 15:58:45,074 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:45,074 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:45,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:45,083 [api2.py:131] throughput: 1.0301540651232854
(INFO) 2023-04-10 15:58:45,176 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:45,177 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 149, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 15:58:45,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:45,186 [api2.py:131] throughput: 0.6051045224265126
(INFO) 2023-04-10 15:58:45,277 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:45,277 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 205, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 15:58:45,286 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:45,286 [api2.py:131] throughput: 0.405796613752734
(INFO) 2023-04-10 15:58:45,364 [api2.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-10 15:58:45,364 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 15:58:45,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:45,374 [api2.py:131] throughput: 0.42549154214545326
(INFO) 2023-04-10 15:58:49,275 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:49,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 15:58:49,308 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:50,044 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:50,045 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 384, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:50,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:50,261 [api1.py:131] throughput: 0.0407446153689534
(INFO) 2023-04-10 15:58:50,367 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:50,367 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 193, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 15:58:50,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:50,578 [api1.py:131] throughput: 0.4071650627620514
(INFO) 2023-04-10 15:58:50,681 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:50,681 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:50,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:50,900 [api1.py:131] throughput: 0.46813052474912165
(INFO) 2023-04-10 15:58:51,014 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:51,015 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 410, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 15:58:51,024 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:51,231 [api1.py:131] throughput: 0.22929580239384206
(INFO) 2023-04-10 15:58:51,341 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:51,342 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:58:51,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:51,569 [api1.py:131] throughput: 0.06148410085764233
(INFO) 2023-04-10 15:58:51,673 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:51,673 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 256, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:51,682 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:51,892 [api1.py:131] throughput: 0.061116613236078705
(INFO) 2023-04-10 15:58:51,997 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-10 15:58:51,998 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 401, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 15:58:52,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:52,079 [api1.py:131] throughput: 0.2184713224996458
(INFO) 2023-04-10 15:58:52,204 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:52,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 916}
(INFO) 2023-04-10 15:58:52,207 [api1.py:131] throughput: 0.12214636999093409
(INFO) 2023-04-10 15:58:52,237 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:55,489 [api1.py:131] throughput: 2.8237707487872856
(INFO) 2023-04-10 15:58:55,617 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:55,618 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 421, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 15:58:55,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:57,489 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:57,490 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 126, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:58:57,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:57,510 [api2.py:131] throughput: 1.2085483421098069
(INFO) 2023-04-10 15:58:57,615 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:57,616 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 55}
(INFO) 2023-04-10 15:58:57,636 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:57,637 [api2.py:131] throughput: 15.049016822096021
(INFO) 2023-04-10 15:58:57,749 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:57,749 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:58:57,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:57,771 [api2.py:131] throughput: 0.9393728613423705
(INFO) 2023-04-10 15:58:57,881 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:57,882 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 128, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 232}
(INFO) 2023-04-10 15:58:57,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:57,903 [api2.py:131] throughput: 0.48695140193921044
(INFO) 2023-04-10 15:58:58,015 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:58,016 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 364, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 88}
(INFO) 2023-04-10 15:58:58,035 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:58,036 [api2.py:131] throughput: 0.8379210860231494
(INFO) 2023-04-10 15:58:58,141 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:58,142 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:58:58,160 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:58,161 [api2.py:131] throughput: 0.7460671911243666
(INFO) 2023-04-10 15:58:58,273 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:58,273 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:58:58,292 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:58,293 [api2.py:131] throughput: 2.3585485161937383
(INFO) 2023-04-10 15:58:58,401 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:58,401 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:58:58,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:58,422 [api2.py:131] throughput: 3.827757160410297
(INFO) 2023-04-10 15:58:58,526 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:58,527 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 74}
(INFO) 2023-04-10 15:58:58,547 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:58,548 [api2.py:131] throughput: 0.9864879349051173
(INFO) 2023-04-10 15:58:58,630 [api1.py:131] throughput: 0.30481648542661505
(INFO) 2023-04-10 15:58:58,657 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 34}
(INFO) 2023-04-10 15:58:58,658 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 285, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:58:58,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:58:58,679 [api2.py:131] throughput: 0.569158291084109
(INFO) 2023-04-10 15:58:58,751 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:58:58,752 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 424, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 15:58:58,784 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:01,538 [api1.py:131] throughput: 0.31973138944800317
(INFO) 2023-04-10 15:59:01,657 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:59:01,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:59:01,690 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:04,688 [api1.py:131] throughput: 0.07615043101952096
(INFO) 2023-04-10 15:59:04,804 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:59:04,805 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 141, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:59:04,838 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:07,785 [api1.py:131] throughput: 0.06902719353776879
(INFO) 2023-04-10 15:59:07,909 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:59:07,910 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 221, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 15:59:07,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:08,524 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:08,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 15:59:08,547 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:09,318 [api1.py:131] throughput: 1.1774614729595794
(INFO) 2023-04-10 15:59:09,409 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:09,409 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 143, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 15:59:09,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:10,587 [api1.py:131] throughput: 0.3029790190774568
(INFO) 2023-04-10 15:59:10,676 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:10,677 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 165}
(INFO) 2023-04-10 15:59:10,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:11,061 [api1.py:131] throughput: 0.13504372921359295
(INFO) 2023-04-10 15:59:11,182 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:59:11,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 87}
(INFO) 2023-04-10 15:59:11,215 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:11,850 [api1.py:131] throughput: 1.546567062146115
(INFO) 2023-04-10 15:59:11,951 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:11,951 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 160}
(INFO) 2023-04-10 15:59:11,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:12,961 [api1.py:131] throughput: 0.7890467905646232
(INFO) 2023-04-10 15:59:13,049 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:13,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 88, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:59:13,070 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:13,331 [api1.py:131] throughput: 1.5454403836914417
(INFO) 2023-04-10 15:59:13,445 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:59:13,446 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 299, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1096}
(INFO) 2023-04-10 15:59:13,479 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:14,242 [api1.py:131] throughput: 0.06846703438572113
(INFO) 2023-04-10 15:59:14,331 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:14,332 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 118}
(INFO) 2023-04-10 15:59:14,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:15,147 [api1.py:131] throughput: 2.2843942763741585
(INFO) 2023-04-10 15:59:15,256 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 41}
(INFO) 2023-04-10 15:59:15,257 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 154, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 69}
(INFO) 2023-04-10 15:59:15,289 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:15,374 [api1.py:131] throughput: 1.3278728017550645
(INFO) 2023-04-10 15:59:15,475 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:15,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 15:59:15,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:16,610 [api1.py:131] throughput: 0.5531419002431466
(INFO) 2023-04-10 15:59:16,709 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:16,709 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 114}
(INFO) 2023-04-10 15:59:16,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:17,653 [api1.py:131] throughput: 1.4358030944911142
(INFO) 2023-04-10 15:59:17,757 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:17,758 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 448, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:59:17,779 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:17,864 [api1.py:131] throughput: 0.8863056830122332
(INFO) 2023-04-10 15:59:18,914 [api1.py:131] throughput: 0.1012453489116978
(INFO) 2023-04-10 15:59:19,012 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:19,012 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:59:19,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:20,174 [api1.py:131] throughput: 0.0932276272332264
(INFO) 2023-04-10 15:59:23,130 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,131 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:59:23,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,147 [api2.py:131] throughput: 0.8755678364862245
(INFO) 2023-04-10 15:59:23,225 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,226 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 340, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 15:59:23,241 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,242 [api2.py:131] throughput: 0.3014705568616511
(INFO) 2023-04-10 15:59:23,335 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,336 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 493, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:59:23,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,355 [api2.py:131] throughput: 0.3886648295951636
(INFO) 2023-04-10 15:59:23,455 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,456 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:59:23,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,474 [api2.py:131] throughput: 32.087277439036036
(INFO) 2023-04-10 15:59:23,577 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,577 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 418, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 15:59:23,593 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,594 [api2.py:131] throughput: 0.7808835844339088
(INFO) 2023-04-10 15:59:23,694 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,695 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 209, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 15:59:23,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,710 [api2.py:131] throughput: 0.3128758881739608
(INFO) 2023-04-10 15:59:23,811 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,812 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 396, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:59:23,830 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,831 [api2.py:131] throughput: 0.5062151690909182
(INFO) 2023-04-10 15:59:23,939 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:23,939 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 342, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 15:59:23,955 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:23,956 [api2.py:131] throughput: 0.30007517950800616
(INFO) 2023-04-10 15:59:24,063 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:24,063 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 460, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 15:59:24,081 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:24,082 [api2.py:131] throughput: 0.7534912488021663
(INFO) 2023-04-10 15:59:24,194 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-10 15:59:24,195 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 314, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 123}
(INFO) 2023-04-10 15:59:24,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:24,210 [api2.py:131] throughput: 0.39925142153904275
(INFO) 2023-04-10 15:59:29,235 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:29,236 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 288, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:59:29,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:29,256 [api2.py:131] throughput: 0.24269825335033773
(INFO) 2023-04-10 15:59:29,371 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:29,372 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 15:59:29,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:29,393 [api2.py:131] throughput: 1.406126624495505
(INFO) 2023-04-10 15:59:29,501 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:29,502 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 15:59:29,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:29,522 [api2.py:131] throughput: 0.5583200105044183
(INFO) 2023-04-10 15:59:29,635 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:29,636 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 15:59:29,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:29,658 [api2.py:131] throughput: 0.383905349278413
(INFO) 2023-04-10 15:59:29,765 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:29,766 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:59:29,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:29,785 [api2.py:131] throughput: 0.8662045132992438
(INFO) 2023-04-10 15:59:29,872 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:29,873 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 15:59:29,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:29,895 [api2.py:131] throughput: 3.3387340929775697
(INFO) 2023-04-10 15:59:30,003 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:30,004 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 77}
(INFO) 2023-04-10 15:59:30,023 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:30,025 [api2.py:131] throughput: 0.8009981123413957
(INFO) 2023-04-10 15:59:30,132 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:30,133 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 15:59:30,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:30,152 [api2.py:131] throughput: 0.6803594047491729
(INFO) 2023-04-10 15:59:30,256 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:30,257 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 15:59:30,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:30,278 [api2.py:131] throughput: 0.46112730737775354
(INFO) 2023-04-10 15:59:30,390 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 37}
(INFO) 2023-04-10 15:59:30,391 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 102, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 15:59:30,411 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:30,412 [api2.py:131] throughput: 0.5684179978750359
(INFO) 2023-04-10 15:59:35,521 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:35,521 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 384, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 15:59:35,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:35,538 [api2.py:131] throughput: 0.4249962979309147
(INFO) 2023-04-10 15:59:35,641 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:35,641 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 193, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 15:59:35,655 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:35,656 [api2.py:131] throughput: 0.870366428465363
(INFO) 2023-04-10 15:59:35,732 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:35,733 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 15:59:35,749 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:35,750 [api2.py:131] throughput: 0.39035285130520603
(INFO) 2023-04-10 15:59:35,865 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:35,866 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 410, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 15:59:35,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:35,882 [api2.py:131] throughput: 0.4872330430948487
(INFO) 2023-04-10 15:59:35,978 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:35,979 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 299, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:59:35,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:36,000 [api2.py:131] throughput: 8.982589700316408
(INFO) 2023-04-10 15:59:36,101 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:36,102 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 256, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:59:36,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:36,119 [api2.py:131] throughput: 0.6064784060858893
(INFO) 2023-04-10 15:59:36,226 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:36,226 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 401, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 15:59:36,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:36,243 [api2.py:131] throughput: 30.850017187161573
(INFO) 2023-04-10 15:59:36,345 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:36,346 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 331, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 15:59:36,364 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:36,365 [api2.py:131] throughput: 0.5405336570310889
(INFO) 2023-04-10 15:59:36,476 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:36,477 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 211, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 15:59:36,494 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:36,495 [api2.py:131] throughput: 1.323203145362038
(INFO) 2023-04-10 15:59:36,602 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 15}
(INFO) 2023-04-10 15:59:36,603 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 134, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 15:59:36,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:36,624 [api2.py:131] throughput: 1.0163760856652893
(INFO) 2023-04-10 15:59:41,754 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:41,755 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 419, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 15:59:41,779 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:43,263 [api1.py:131] throughput: 0.016520890194810366
(INFO) 2023-04-10 15:59:43,357 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:43,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 155}
(INFO) 2023-04-10 15:59:43,382 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:44,500 [api1.py:131] throughput: 1.9802867299962768
(INFO) 2023-04-10 15:59:44,592 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:44,593 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:59:44,616 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:45,872 [api1.py:131] throughput: 0.24609628767860875
(INFO) 2023-04-10 15:59:45,975 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:45,976 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 500, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 104}
(INFO) 2023-04-10 15:59:46,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:46,957 [api1.py:131] throughput: 0.40175524593032597
(INFO) 2023-04-10 15:59:47,053 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:47,054 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 364, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 15:59:47,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:48,449 [api1.py:131] throughput: 0.2231208868458441
(INFO) 2023-04-10 15:59:48,554 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:48,555 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 15:59:48,580 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:50,043 [api1.py:131] throughput: 0.058782393132375946
(INFO) 2023-04-10 15:59:50,151 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:50,151 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 153, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 15:59:50,175 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:51,681 [api1.py:131] throughput: 0.21366673121462817
(INFO) 2023-04-10 15:59:51,790 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:51,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 422, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 15:59:51,816 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:53,302 [api1.py:131] throughput: 0.0653868102962656
(INFO) 2023-04-10 15:59:53,406 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:53,407 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 15:59:53,428 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:54,761 [api1.py:131] throughput: 0.3136158072776189
(INFO) 2023-04-10 15:59:55,037 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 43}
(INFO) 2023-04-10 15:59:55,038 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 285, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 124}
(INFO) 2023-04-10 15:59:55,061 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 15:59:55,759 [api1.py:131] throughput: 3.5016824245803595
(INFO) 2023-04-10 16:00:00,207 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,208 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 298, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 16:00:00,219 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,220 [api2.py:131] throughput: 0.6029128369613581
(INFO) 2023-04-10 16:00:00,330 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,331 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 16:00:00,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,343 [api2.py:131] throughput: 1.42605398779377
(INFO) 2023-04-10 16:00:00,425 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,426 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 359, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 16:00:00,440 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,441 [api2.py:131] throughput: 5.9791593864421975
(INFO) 2023-04-10 16:00:00,553 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,554 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 16:00:00,565 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,566 [api2.py:131] throughput: 0.6292668646010448
(INFO) 2023-04-10 16:00:00,647 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,648 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 16:00:00,661 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,662 [api2.py:131] throughput: 33.69040434920605
(INFO) 2023-04-10 16:00:00,768 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,769 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 460, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 16:00:00,784 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,785 [api2.py:131] throughput: 0.36369193112445986
(INFO) 2023-04-10 16:00:00,896 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:00,897 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 286, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 100}
(INFO) 2023-04-10 16:00:00,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:00,911 [api2.py:131] throughput: 0.984733906797217
(INFO) 2023-04-10 16:00:01,019 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:01,020 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 202, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 16:00:01,033 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:01,034 [api2.py:131] throughput: 0.6361681563066423
(INFO) 2023-04-10 16:00:01,144 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:01,144 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 171}
(INFO) 2023-04-10 16:00:01,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:01,158 [api2.py:131] throughput: 0.6812213431126165
(INFO) 2023-04-10 16:00:01,268 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 26}
(INFO) 2023-04-10 16:00:01,269 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 137, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 16:00:01,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:01,282 [api2.py:131] throughput: 0.8203831474361867
(INFO) 2023-04-10 16:00:06,024 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,025 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 16:00:06,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,039 [api2.py:131] throughput: 0.5474194803968538
(INFO) 2023-04-10 16:00:06,144 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,145 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 268, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 16:00:06,160 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,161 [api2.py:131] throughput: 0.6396543126312951
(INFO) 2023-04-10 16:00:06,260 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,261 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 495, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 16:00:06,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,274 [api2.py:131] throughput: 0.22803360869203523
(INFO) 2023-04-10 16:00:06,376 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,377 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 305, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 16:00:06,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,393 [api2.py:131] throughput: 34.122243791584644
(INFO) 2023-04-10 16:00:06,507 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,507 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 88, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 16:00:06,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,520 [api2.py:131] throughput: 1.8766864354713892
(INFO) 2023-04-10 16:00:06,631 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,632 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 16:00:06,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,646 [api2.py:131] throughput: 0.40715748269890506
(INFO) 2023-04-10 16:00:06,749 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,750 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 16:00:06,762 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,763 [api2.py:131] throughput: 2.267866229815242
(INFO) 2023-04-10 16:00:06,871 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:06,872 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 72}
(INFO) 2023-04-10 16:00:06,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:06,886 [api2.py:131] throughput: 0.5984668278246228
(INFO) 2023-04-10 16:00:07,000 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:07,001 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 448, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 16:00:07,013 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:07,014 [api2.py:131] throughput: 0.44486392081802417
(INFO) 2023-04-10 16:00:07,120 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 10}
(INFO) 2023-04-10 16:00:07,121 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 442, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 16:00:07,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:00:07,135 [api2.py:131] throughput: 0.39250711703592017
