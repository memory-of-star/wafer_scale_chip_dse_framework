(DEBUG) 2023-04-09 20:10:07,602 [logger.py:40] logger init.
(INFO) 2023-04-09 20:10:07,602 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-09-20-10-07-602601.log
(INFO) 2023-04-09 20:10:10,122 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:10:10,123 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,128 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,128 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,129 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:10:10,129 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,129 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,130 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,132 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,133 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,135 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,138 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,138 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,139 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:10,140 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,141 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,142 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,148 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,158 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:10,159 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,163 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:10,164 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,176 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,195 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:10,197 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:10,211 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,212 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:10,212 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:10,211 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,213 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:10,214 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,214 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:10:10,220 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,234 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:10:10,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,234 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:10:10,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,237 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,237 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,241 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:10:10,241 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:10,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,243 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,249 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:10:10,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,250 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,251 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 20:10:10,255 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,256 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:10,256 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,259 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,261 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:10,261 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,261 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,270 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,270 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,278 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:10:10,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,287 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,290 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:10,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,313 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:10:10,320 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:10,338 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:10:10,342 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,343 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,349 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:10,350 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,355 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:10:10,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,366 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:10:10,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,366 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:10,374 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,381 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:10:10,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,399 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:10:10,403 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:10:10,406 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:10,407 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,412 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:10,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,417 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:10,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,421 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:10:10,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,442 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:10,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,446 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,446 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,447 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:10,448 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,452 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:10,452 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,453 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,454 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:10:10,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,464 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:10:10,495 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:10:10,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,499 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:10:10,499 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,501 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 20:10:10,508 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,539 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:10,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,565 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 20:10:10,590 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 20:10:10,603 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:10:10,603 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,622 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:10:10,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,624 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:10:10,627 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,635 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,658 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,664 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,686 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:10:10,694 [api1.py:131] throughput: 0.011575340385444667
(INFO) 2023-04-09 20:10:10,716 [api1.py:131] throughput: 0.005788071437064676
(INFO) 2023-04-09 20:10:10,723 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:10,724 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,732 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:10:10,734 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:10,735 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,737 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:10,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,748 [api1.py:131] throughput: 0.004961253977952574
(INFO) 2023-04-09 20:10:10,750 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:10:10,751 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,762 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,769 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,769 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,774 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:10,774 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,775 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,776 [api1.py:131] throughput: 0.011575342090005696
(INFO) 2023-04-09 20:10:10,783 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:10:10,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,816 [api1.py:131] throughput: 0.004341129851340191
(INFO) 2023-04-09 20:10:10,820 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,821 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,829 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,846 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 41}
(INFO) 2023-04-09 20:10:10,847 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,859 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,860 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:10:10,860 [api1.py:131] throughput: 0.004341129621918651
(INFO) 2023-04-09 20:10:10,901 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:10:10,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,906 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:10,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,914 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,925 [api1.py:131] throughput: 0.011575341204519385
(INFO) 2023-04-09 20:10:10,942 [api1.py:131] throughput: 0.00496125469960527
(INFO) 2023-04-09 20:10:10,962 [api1.py:131] throughput: 0.006945590319684866
(INFO) 2023-04-09 20:10:10,970 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:10,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,978 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:10,987 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:10,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:10,996 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,000 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:10:11,000 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,068 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 20:10:11,096 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:11,102 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,103 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,112 [api1.py:131] throughput: 0.005788072109178651
(INFO) 2023-04-09 20:10:11,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,120 [api1.py:131] throughput: 0.011575341966037603
(INFO) 2023-04-09 20:10:11,131 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:11,132 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,141 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:10:11,152 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 20:10:11,156 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:11,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,165 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:11,165 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,167 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:10:11,169 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,180 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:11,180 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,184 [api1.py:131] throughput: 0.0034729402433456894
(INFO) 2023-04-09 20:10:11,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,190 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 20:10:11,191 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:10:11,192 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,204 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:11,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,215 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,220 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:10:11,220 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,229 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,232 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:11,232 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,239 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,241 [api1.py:131] throughput: 0.00578807160311635
(INFO) 2023-04-09 20:10:11,251 [api1.py:131] throughput: 0.004961254504920039
(INFO) 2023-04-09 20:10:11,257 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:10:11,261 [api1.py:131] throughput: 0.008681807437366683
(INFO) 2023-04-09 20:10:11,281 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:11,281 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,290 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,290 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:11,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,291 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:11,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,294 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,295 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,302 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:10:11,303 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,342 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,343 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,346 [api1.py:131] throughput: 0.0028941369354155344
(INFO) 2023-04-09 20:10:11,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,362 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 20:10:11,387 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,388 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,400 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 20:10:11,400 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,404 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,405 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,408 [api1.py:131] throughput: 0.004341130186648638
(INFO) 2023-04-09 20:10:11,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,433 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:10:11,434 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,448 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:10:11,455 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 20:10:11,460 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,461 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,464 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,467 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,482 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:11,483 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,491 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,492 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,499 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,536 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:11,539 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 20:10:11,541 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 20:10:11,543 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 20:10:11,555 [api1.py:131] throughput: 0.004961254676560895
(INFO) 2023-04-09 20:10:11,568 [api1.py:131] throughput: 0.006945591389563201
(INFO) 2023-04-09 20:10:11,571 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:11,572 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,573 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:11,573 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,578 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:11,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,579 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:11,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,584 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:10:11,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,587 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,590 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,597 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,612 [api1.py:131] throughput: 0.0057880730392393265
(INFO) 2023-04-09 20:10:11,612 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:11,613 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,620 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 41, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,621 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,624 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:10:11,641 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,649 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 20:10:11,655 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,656 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,661 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,664 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:10:11,665 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,673 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,680 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:10:11,681 [api1.py:131] throughput: 0.017361803286854736
(INFO) 2023-04-09 20:10:11,687 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:10:11,688 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,688 [api1.py:131] throughput: 0.006945590910933379
(INFO) 2023-04-09 20:10:11,704 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,707 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:10:11,719 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:11,719 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,724 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:10:11,725 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,725 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:10:11,726 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,729 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,732 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,738 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:10:11,741 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:10:11,743 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:11,744 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,761 [api1.py:131] throughput: 0.006945590910933379
(INFO) 2023-04-09 20:10:11,772 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:10:11,773 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,776 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:11,777 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,780 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,786 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,795 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 20:10:11,800 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,800 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,803 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:10:11,808 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,826 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 20:10:11,835 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:11,836 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,841 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 58}
(INFO) 2023-04-09 20:10:11,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,848 [api1.py:131] throughput: 0.003472940297389044
(INFO) 2023-04-09 20:10:11,855 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,860 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,861 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,881 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:11,896 [api1.py:131] throughput: 0.008681806023730315
(INFO) 2023-04-09 20:10:11,896 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:11,896 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,916 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:11,916 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,918 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:10:11,923 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,927 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 20:10:11,933 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:11,938 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:11,938 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,953 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:11,953 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,964 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,965 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:11,966 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,969 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:11,970 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:11,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:11,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,030 [api1.py:131] throughput: 0.0069455902685893215
(INFO) 2023-04-09 20:10:12,030 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:10:12,032 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:12,054 [api1.py:131] throughput: 0.005788072522787328
(INFO) 2023-04-09 20:10:12,055 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:10:12,058 [api1.py:131] throughput: 0.008681806193698297
(INFO) 2023-04-09 20:10:12,134 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:12,134 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,136 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:10:12,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,141 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,154 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:12,155 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,155 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 20:10:12,165 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,166 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:12,166 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,175 [api1.py:131] throughput: 0.008681806023730315
(INFO) 2023-04-09 20:10:12,182 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 67}
(INFO) 2023-04-09 20:10:12,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,199 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,223 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,223 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,225 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:12,226 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,229 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,233 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,242 [api1.py:131] throughput: 0.0069455906312028875
(INFO) 2023-04-09 20:10:12,267 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 20:10:12,267 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:12,276 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:12,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,286 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,297 [api1.py:131] throughput: 0.003858804827156706
(INFO) 2023-04-09 20:10:12,323 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:10:12,323 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,325 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:12,325 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,331 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,333 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,342 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:10:12,347 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:12,347 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,357 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:10:12,361 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:12,361 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,370 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,372 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:10:12,375 [api1.py:131] throughput: 0.011575332580659792
(INFO) 2023-04-09 20:10:12,377 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 55}
(INFO) 2023-04-09 20:10:12,378 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,387 [api1.py:131] throughput: 0.00385880439348466
(INFO) 2023-04-09 20:10:12,398 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:12,400 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,454 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:12,454 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,457 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:12,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,470 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:10:12,470 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,490 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,490 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,498 [api1.py:131] throughput: 0.004341130085695552
(INFO) 2023-04-09 20:10:12,544 [api1.py:131] throughput: 0.0057880730735931215
(INFO) 2023-04-09 20:10:12,550 [api1.py:131] throughput: 0.004341129735099941
(INFO) 2023-04-09 20:10:12,553 [api1.py:131] throughput: 0.011575342059471682
(INFO) 2023-04-09 20:10:12,580 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:12,614 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:10:12,630 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,630 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,635 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,649 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:10:12,650 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,670 [api1.py:131] throughput: 0.006945590841688053
(INFO) 2023-04-09 20:10:12,671 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,671 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:12,671 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,671 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,672 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:12,673 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,677 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,684 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:12,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,686 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,689 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 46}
(INFO) 2023-04-09 20:10:12,690 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,690 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,702 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,703 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,709 [api1.py:131] throughput: 0.006945591455809547
(INFO) 2023-04-09 20:10:12,721 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:12,734 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 20:10:12,753 [api1.py:131] throughput: 0.004961253570431144
(INFO) 2023-04-09 20:10:12,757 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:12,758 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,764 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,780 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,781 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,781 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:12,781 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,786 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,803 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:12,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,812 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:10:12,829 [api1.py:131] throughput: 0.006945590160502592
(INFO) 2023-04-09 20:10:12,865 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:10:12,891 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:12,899 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:12,899 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,903 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:12,904 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,906 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:12,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:12,908 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:12,914 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:10:12,918 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 20:10:13,007 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:13,007 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,017 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:13,023 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 53}
(INFO) 2023-04-09 20:10:13,023 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,039 [api1.py:131] throughput: 0.005788072001640405
(INFO) 2023-04-09 20:10:13,043 [api1.py:131] throughput: 0.0024807011189285225
(INFO) 2023-04-09 20:10:13,044 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:10:13,092 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:13,093 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,099 [api1.py:131] throughput: 0.005788072378024285
(INFO) 2023-04-09 20:10:13,100 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:10:13,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,159 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:13,159 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,185 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:13,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,187 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:13,187 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,192 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,193 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:13,193 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,221 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:13,222 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,226 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,231 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:10:13,268 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:13,297 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:10:13,315 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:13,327 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 20:10:13,359 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:10:13,366 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:13,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,371 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,374 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 56}
(INFO) 2023-04-09 20:10:13,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,375 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:13,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,378 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:13,378 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,378 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:13,379 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,379 [api1.py:131] throughput: 0.005788071882782347
(INFO) 2023-04-09 20:10:13,382 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,383 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,385 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,399 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:13,400 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,404 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,468 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:13,469 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,477 [api1.py:131] throughput: 0.0115753414162661
(INFO) 2023-04-09 20:10:13,493 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:13,497 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,504 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:10:13,513 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:10:13,520 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 20:10:13,546 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 45}
(INFO) 2023-04-09 20:10:13,546 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,556 [api1.py:131] throughput: 0.004341129863975001
(INFO) 2023-04-09 20:10:13,576 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,616 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:13,616 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:13,616 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,616 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,619 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:10:13,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,623 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,654 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:13,657 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:13,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,663 [api1.py:131] throughput: 0.0021706215644502124
(INFO) 2023-04-09 20:10:13,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,680 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 52}
(INFO) 2023-04-09 20:10:13,681 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,695 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,714 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 28}
(INFO) 2023-04-09 20:10:13,715 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,748 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:13,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,753 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,765 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:10:13,766 [api1.py:131] throughput: 0.00496125462628226
(INFO) 2023-04-09 20:10:13,774 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 20:10:13,795 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:13,796 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,804 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,841 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:13,841 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,858 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:10:13,859 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:10:13,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,917 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:10:13,937 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:13,938 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,938 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:13,938 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,942 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:13,943 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,946 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,948 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,954 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:13,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,961 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:13,964 [api1.py:131] throughput: 0.008681809169407528
(INFO) 2023-04-09 20:10:13,968 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:13,968 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:13,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,010 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:10:14,010 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,015 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,021 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:14,025 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:14,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,040 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:14,110 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:14,111 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,115 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,125 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:10:14,128 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 20:10:14,154 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 20:10:14,175 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:10:14,218 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:10:14,218 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:10:14,218 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,218 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,226 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,227 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,228 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:10:14,248 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:14,273 [api1.py:131] throughput: 0.017361804016257653
(INFO) 2023-04-09 20:10:14,319 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 20:10:14,335 [api1.py:131] throughput: 0.011575331866671992
(INFO) 2023-04-09 20:10:14,379 [api1.py:131] throughput: 0.008681808608527177
(INFO) 2023-04-09 20:10:14,409 [api1.py:131] throughput: 0.017361798545737262
(INFO) 2023-04-09 20:10:14,444 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:14,445 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,445 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:14,445 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,449 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,449 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:10:14,450 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,458 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,477 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:14,478 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,479 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:14,479 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,510 [api1.py:131] throughput: 0.011575338091036336
(INFO) 2023-04-09 20:10:14,538 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 20:10:14,540 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:10:14,551 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:14,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,552 [api1.py:131] throughput: 0.005788073375573406
(INFO) 2023-04-09 20:10:14,553 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:14,555 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,559 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:14,560 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,576 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:10:14,592 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:14,592 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,598 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,617 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:14,622 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:14,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,627 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:14,628 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,646 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:14,647 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,700 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:14,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,708 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,718 [api1.py:131] throughput: 0.0049612541863442395
(INFO) 2023-04-09 20:10:14,724 [api1.py:131] throughput: 0.00868180560220974
(INFO) 2023-04-09 20:10:14,729 [api1.py:131] throughput: 0.004961254621254395
(INFO) 2023-04-09 20:10:14,740 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:14,740 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,745 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,772 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 20:10:14,846 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 51}
(INFO) 2023-04-09 20:10:14,846 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,848 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:10:14,862 [api1.py:131] throughput: 0.006945587560526473
(INFO) 2023-04-09 20:10:14,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,905 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:10:14,908 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:14,908 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,909 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:14,909 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,913 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 20:10:14,914 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,915 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:14,917 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:14,917 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,923 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,923 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 52}
(INFO) 2023-04-09 20:10:14,924 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,937 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:14,937 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,947 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:14,973 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:14,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:14,980 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,012 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:15,012 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,044 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:15,058 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:15,059 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,059 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 20:10:15,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,079 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:10:15,082 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:10:15,100 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:15,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,110 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,124 [api1.py:131] throughput: 0.006945589917307465
(INFO) 2023-04-09 20:10:15,151 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:15,152 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,156 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,184 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 20:10:15,231 [api1.py:131] throughput: 0.005788071437064676
(INFO) 2023-04-09 20:10:15,237 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:15,237 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,241 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,243 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:15,244 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,248 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:10:15,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,251 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:10:15,251 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,258 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,260 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:15,261 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,266 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,281 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:10:15,305 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:10:15,308 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 20:10:15,309 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,310 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:15,314 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,324 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:15,324 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,347 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:15,353 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:15,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,364 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:10:15,365 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,369 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:15,370 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,390 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,419 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:10:15,437 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 20:10:15,452 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:10:15,476 [api1.py:131] throughput: 0.003157230346767118
(INFO) 2023-04-09 20:10:15,484 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:15,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,503 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:10:15,534 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:10:15,534 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,557 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:15,558 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,569 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:15,570 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,575 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,582 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:15,605 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:15,606 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,612 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:10:15,624 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:15,625 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,627 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:15,627 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,632 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,646 [api1.py:131] throughput: 0.008681804420119654
(INFO) 2023-04-09 20:10:15,675 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:15,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,683 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,718 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 20:10:15,719 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:10:15,719 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,741 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:10:15,743 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:15,763 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 20:10:15,764 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:15,770 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:15,778 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:15,778 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,784 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,815 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:15,816 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,824 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,839 [api1.py:131] throughput: 0.011575342209205787
(INFO) 2023-04-09 20:10:15,860 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:15,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,864 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:15,865 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,868 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,870 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,890 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:15,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:15,896 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:15,906 [api1.py:131] throughput: 0.008681808193322147
(INFO) 2023-04-09 20:10:15,984 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:15,990 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 39}
(INFO) 2023-04-09 20:10:15,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,014 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,030 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:16,044 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:16,045 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,048 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,082 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:16,083 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,088 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,106 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:10:16,158 [api1.py:131] throughput: 0.008681804420119654
(INFO) 2023-04-09 20:10:16,161 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:16,166 [api1.py:131] throughput: 0.006945590043768928
(INFO) 2023-04-09 20:10:16,211 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:10:16,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,219 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,220 [api1.py:131] throughput: 0.0038588044573588313
(INFO) 2023-04-09 20:10:16,252 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:16,253 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,274 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 20:10:16,319 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:10:16,320 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,328 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:16,329 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,344 [api1.py:131] throughput: 0.006945589895198816
(INFO) 2023-04-09 20:10:16,352 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:16,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,352 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:16,353 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,358 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,358 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:16,358 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:16,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,361 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,371 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 20:10:16,382 [api1.py:131] throughput: 0.008681805213113862
(INFO) 2023-04-09 20:10:16,382 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 48}
(INFO) 2023-04-09 20:10:16,383 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,393 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,405 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:16,405 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,436 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:16,444 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:16,445 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,464 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:16,465 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:10:16,466 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,499 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:10:16,507 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:16,529 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 20:10:16,534 [api1.py:131] throughput: 0.011575332580659792
(INFO) 2023-04-09 20:10:16,538 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:16,538 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,662 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:16,668 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:10:16,668 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,674 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:16,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,674 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:16,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,676 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,678 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,679 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,695 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 20:10:16,743 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:16,744 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:16,745 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,797 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:16,797 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,803 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:10:16,839 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 20:10:16,865 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:16,867 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,893 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,899 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:16,928 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 20:10:16,943 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 24}
(INFO) 2023-04-09 20:10:16,943 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,970 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:16,993 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:16,993 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:16,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,002 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:17,003 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,007 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 57}
(INFO) 2023-04-09 20:10:17,007 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,011 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,011 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:17,011 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,040 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:17,040 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,044 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,048 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,083 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:17,084 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,084 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:17,084 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,089 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,089 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,097 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:17,098 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,111 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 20:10:17,119 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 20:10:17,137 [api1.py:131] throughput: 0.008681807447786608
(INFO) 2023-04-09 20:10:17,137 [api1.py:131] throughput: 0.011575339886586523
(INFO) 2023-04-09 20:10:17,152 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:17,160 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:17,178 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:17,183 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:17,189 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:17,194 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 20:10:17,195 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:10:17,196 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,237 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:17,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,250 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:17,251 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,377 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:17,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,480 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:17,536 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:10:17,546 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:17,546 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,555 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,593 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:17,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,602 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 20:10:17,646 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:17,646 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,675 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:17,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,680 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,702 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:17,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,708 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,717 [api1.py:131] throughput: 0.008681807984291658
(INFO) 2023-04-09 20:10:17,718 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:17,718 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,730 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:17,731 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,732 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:17,733 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,759 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:10:17,761 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 47}
(INFO) 2023-04-09 20:10:17,761 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,782 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 35}
(INFO) 2023-04-09 20:10:17,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,795 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:10:17,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,801 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:17,801 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,805 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:10:17,805 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:17,806 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,811 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:17,815 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:17,824 [api1.py:131] throughput: 0.003858804423740846
(INFO) 2023-04-09 20:10:17,832 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 20:10:17,840 [api1.py:131] throughput: 0.0057880730392393265
(INFO) 2023-04-09 20:10:17,845 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:10:17,845 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,852 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,894 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:17,895 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,927 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:17,935 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 33}
(INFO) 2023-04-09 20:10:17,943 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:17,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,956 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,957 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:17,984 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:17,984 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:17,990 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,068 [api1.py:131] throughput: 0.005788071827924784
(INFO) 2023-04-09 20:10:18,156 [api1.py:131] throughput: 0.0026715182019872064
(INFO) 2023-04-09 20:10:18,191 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:18,228 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 20:10:18,264 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:18,264 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,268 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,290 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:18,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,291 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:18,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,297 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:18,298 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,309 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,314 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,303 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,361 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,361 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,365 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,368 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 20:10:18,373 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:18,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,375 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:10:18,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,413 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:18,417 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:18,418 [api1.py:131] throughput: 0.006945589316354288
(INFO) 2023-04-09 20:10:18,425 [api1.py:131] throughput: 0.011575340601635167
(INFO) 2023-04-09 20:10:18,451 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 20:10:18,455 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:10:18,465 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:18,465 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,470 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,470 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,475 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,479 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:18,481 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:10:18,517 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 22}
(INFO) 2023-04-09 20:10:18,518 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,528 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 23}
(INFO) 2023-04-09 20:10:18,528 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,531 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,540 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,602 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:18,609 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 20:10:18,718 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,718 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,771 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,771 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,914 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:18,915 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,916 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:18,920 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,922 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:18,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,927 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:18,927 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 20:10:18,927 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:18,932 [api1.py:131] throughput: 0.0021706215752560743
(INFO) 2023-04-09 20:10:18,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,093 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:10:19,099 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:10:19,103 [api1.py:131] throughput: 0.005788072145144621
(INFO) 2023-04-09 20:10:19,111 [api1.py:131] throughput: 0.006945588480245695
(INFO) 2023-04-09 20:10:19,113 [api1.py:131] throughput: 0.006945589767853012
(INFO) 2023-04-09 20:10:19,125 [api1.py:131] throughput: 0.0057880720490173935
(INFO) 2023-04-09 20:10:19,315 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 20:10:19,320 [api1.py:131] throughput: 0.004961254669087043
(INFO) 2023-04-09 20:10:19,396 [api1.py:131] throughput: 0.0023153256586876656
(INFO) 2023-04-09 20:10:19,398 [api1.py:131] throughput: 0.004341130304677223
(INFO) 2023-04-09 20:10:19,434 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:19,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,438 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:19,439 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,441 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,442 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:19,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,442 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:19,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,513 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:19,513 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,522 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,537 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:19,574 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:10:19,575 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,587 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,598 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:19,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,599 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:19,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,599 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 40}
(INFO) 2023-04-09 20:10:19,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,601 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:19,601 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,604 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,605 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,606 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,607 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 20:10:19,611 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:19,614 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,616 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:19,677 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 25}
(INFO) 2023-04-09 20:10:19,678 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,694 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:19,694 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,713 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,735 [api1.py:131] throughput: 0.003157230393023477
(INFO) 2023-04-09 20:10:19,760 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:10:19,767 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:10:19,787 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:19,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,793 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:19,794 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,804 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:19,805 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,863 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:19,879 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:19,880 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,886 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,897 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:19,897 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,901 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,961 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 20:10:19,984 [api1.py:131] throughput: 0.0049612539575765
(INFO) 2023-04-09 20:10:19,989 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:19,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:19,992 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:19,992 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:10:19,997 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 20:10:19,997 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,004 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:10:20,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,012 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:20,013 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,021 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,027 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 20:10:20,032 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:20,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,044 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,062 [api1.py:131] throughput: 0.008681806211909154
(INFO) 2023-04-09 20:10:20,062 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:20,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,076 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,143 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:20,143 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,154 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,164 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:20,165 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,169 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,177 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:20,178 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,184 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 20:10:20,197 [api1.py:131] throughput: 0.00315723047302932
(INFO) 2023-04-09 20:10:20,210 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:20,211 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,215 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,233 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:20,234 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,238 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,255 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:20,290 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:10:20,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,291 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 20:10:20,295 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:20,304 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:10:20,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,312 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:20,362 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:20,362 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,363 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:10:20,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,371 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 20:10:20,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,375 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,405 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 20:10:20,459 [api1.py:131] throughput: 0.006945591153849464
(INFO) 2023-04-09 20:10:20,510 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:20,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,550 [api1.py:131] throughput: 0.005788071398745061
(INFO) 2023-04-09 20:10:20,641 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 20:10:20,642 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,648 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,649 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:20,650 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,659 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:20,660 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,663 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,683 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:20,683 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:20,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:20,724 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:20,736 [api1.py:131] throughput: 0.005788073130792191
(INFO) 2023-04-09 20:10:20,741 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 20:10:20,742 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:20,815 [api1.py:131] throughput: 0.006945588480245695
(INFO) 2023-04-09 20:10:20,841 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:10:20,900 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 20:10:20,977 [api1.py:131] throughput: 0.008681807984291658
(INFO) 2023-04-09 20:10:21,091 [api1.py:131] throughput: 0.005788072638597768
(INFO) 2023-04-09 20:10:21,102 [api1.py:131] throughput: 0.0019294469775901562
(INFO) 2023-04-09 20:10:21,196 [api1.py:131] throughput: 0.0049612545382946485
(INFO) 2023-04-09 20:10:21,384 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:21,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,385 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:21,386 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,389 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:21,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,390 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:21,391 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,390 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 20:10:21,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,392 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,392 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:21,393 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,393 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:21,394 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,395 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,419 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 37}
(INFO) 2023-04-09 20:10:21,419 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,420 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:21,420 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,421 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:21,422 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:21,422 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,422 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,423 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:21,424 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,442 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,463 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:21,463 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,469 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,511 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:21,512 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:10:21,516 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:21,521 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 20:10:21,542 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 20:10:21,543 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:10:21,549 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:21,610 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:21,611 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,618 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,645 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 20:10:21,708 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:21,723 [api1.py:131] throughput: 0.002894136993621754
(INFO) 2023-04-09 20:10:21,750 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:21,751 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,757 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,770 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 20:10:21,772 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:10:21,790 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 20:10:21,876 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:21,876 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:21,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:21,928 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 20:10:22,059 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 20:10:22,111 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 20:10:22,509 [api1.py:131] throughput: 0.005788073184561335
(INFO) 2023-04-09 20:10:22,825 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:22,826 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,827 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:22,828 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,829 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 20:10:22,829 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,830 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 20:10:22,831 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,832 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,832 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:22,833 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,833 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:22,833 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 36}
(INFO) 2023-04-09 20:10:22,834 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,834 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,833 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:22,833 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:22,834 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,834 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,834 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:22,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,836 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,839 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,842 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,860 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:22,861 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:22,868 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:22,888 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:22,929 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:22,930 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:22,932 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 20:10:22,934 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 20:10:22,958 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 20:10:22,963 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 20:10:22,971 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 20:10:22,992 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 20:10:22,993 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:23,019 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:23,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:23,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:23,108 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 58}
(INFO) 2023-04-09 20:10:23,110 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:23,152 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:23,226 [api1.py:131] throughput: 0.01736179591178423
(INFO) 2023-04-09 20:10:23,359 [api1.py:131] throughput: 0.002042941065981492
(INFO) 2023-04-09 20:10:23,363 [api1.py:131] throughput: 0.006945588020386053
(INFO) 2023-04-09 20:10:23,625 [api1.py:131] throughput: 0.001929446978185097
(INFO) 2023-04-09 20:10:24,302 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:24,303 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,304 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:24,305 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:24,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,307 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:24,307 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 20:10:24,308 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,307 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:24,308 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,308 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,309 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,309 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:24,309 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:24,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,318 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,338 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:24,339 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,359 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:24,359 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,367 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,373 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:24,380 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:24,387 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:24,395 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:24,402 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:24,410 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 20:10:24,563 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 20:10:24,643 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 20:10:24,692 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:24,693 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,842 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 20:10:24,851 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:24,851 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,856 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,867 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:24,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,873 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:24,874 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,874 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,874 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:24,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:24,881 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:24,901 [api1.py:131] throughput: 0.003858804356504878
(INFO) 2023-04-09 20:10:24,954 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 20:10:24,963 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:24,992 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:25,013 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:25,062 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:10:25,099 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 20:10:25,099 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:25,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,100 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,100 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:25,100 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:25,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,161 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 21}
(INFO) 2023-04-09 20:10:25,162 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,174 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:25,174 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,179 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,189 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,265 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 20:10:25,266 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:25,276 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:10:25,328 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 20:10:25,423 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 20:10:25,613 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:25,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:25,618 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:25,715 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:26,066 [api1.py:131] throughput: 0.0034729402645391617
(INFO) 2023-04-09 20:10:26,380 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 57}
(INFO) 2023-04-09 20:10:26,381 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,380 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:26,381 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,381 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:26,382 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,384 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:26,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,387 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:26,388 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,390 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,395 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,445 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 20}
(INFO) 2023-04-09 20:10:26,446 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,451 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:26,455 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,470 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 20:10:26,473 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:26,548 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:26,819 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:26,819 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,822 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,888 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:26,939 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 20:10:26,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,949 [api1.py:131] throughput: 0.004341129735099941
(INFO) 2023-04-09 20:10:26,968 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:26,968 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,973 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:26,993 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:26,994 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:26,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:27,014 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:27,025 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:27,027 [api1.py:131] throughput: 0.017361805615825673
(INFO) 2023-04-09 20:10:27,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:27,084 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:27,101 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:27,107 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 20:10:27,125 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:27,125 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:27,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:27,147 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 34, 'reticle_array_w': 11}
(INFO) 2023-04-09 20:10:27,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:27,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:27,199 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:27,321 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:27,321 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:27,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:27,364 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 20:10:27,399 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:27,521 [api1.py:131] throughput: 0.006945590644291881
(INFO) 2023-04-09 20:10:27,828 [api1.py:131] throughput: 0.0019294469787595218
(INFO) 2023-04-09 20:10:28,102 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 20:10:28,103 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,149 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:28,150 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,158 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,192 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 27}
(INFO) 2023-04-09 20:10:28,192 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,203 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,306 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 20:10:28,336 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:28,336 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,344 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,429 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 20:10:28,429 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,500 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:28,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,505 [api1.py:131] throughput: 0.01736180289982464
(INFO) 2023-04-09 20:10:28,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,507 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 20:10:28,582 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:28,603 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:10:28,661 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 20:10:28,662 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,707 [api1.py:131] throughput: 0.004961254359060638
(INFO) 2023-04-09 20:10:28,735 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:28,735 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:28,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:28,809 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 20:10:28,871 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 20:10:29,487 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:10:29,488 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:29,490 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:29,491 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:29,499 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:29,500 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:29,523 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 34}
(INFO) 2023-04-09 20:10:29,524 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:29,535 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:29,551 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:29,727 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 20:10:29,868 [api1.py:131] throughput: 0.008681807108526096
(INFO) 2023-04-09 20:10:29,897 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:29,898 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:29,903 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:29,938 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 20:10:29,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:29,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:29,975 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:30,044 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 20:10:30,072 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 32}
(INFO) 2023-04-09 20:10:30,073 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:30,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:30,291 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 20:10:30,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:30,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:30,306 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 43}
(INFO) 2023-04-09 20:10:30,306 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:30,315 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:30,342 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 20:10:30,346 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 20:10:30,374 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 20:10:30,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:30,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:30,533 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 20:10:30,575 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 20:10:30,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:30,580 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:30,614 [api1.py:131] throughput: 0.017361800199615158
(INFO) 2023-04-09 20:10:30,634 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 20:10:31,062 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 50}
(INFO) 2023-04-09 20:10:31,063 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 20:10:31,073 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 20:10:31,311 [api1.py:131] throughput: 0.017361803286854736
