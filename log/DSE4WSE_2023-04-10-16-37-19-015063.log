(DEBUG) 2023-04-10 16:37:19,015 [logger.py:40] logger init.
(INFO) 2023-04-10 16:37:19,015 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-16-37-19-015063.log
(INFO) 2023-04-10 16:37:22,697 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,697 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 428, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 16:37:22,701 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,702 [api2.py:131] throughput: 0.3653274695724337
(INFO) 2023-04-10 16:37:22,703 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,703 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 504, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 16:37:22,706 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,707 [api2.py:131] throughput: 0.6722809811770002
(INFO) 2023-04-10 16:37:22,707 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,707 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 484, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 16:37:22,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,711 [api2.py:131] throughput: 0.7030140150947294
(INFO) 2023-04-10 16:37:22,712 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,712 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 416, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 16:37:22,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,715 [api2.py:131] throughput: 0.33869567538897044
(INFO) 2023-04-10 16:37:22,716 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,716 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 374, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 16:37:22,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,719 [api2.py:131] throughput: 0.46007462162507023
(INFO) 2023-04-10 16:37:22,720 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,720 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 387, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 16:37:22,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,723 [api2.py:131] throughput: 0.8595768913526393
(INFO) 2023-04-10 16:37:22,724 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,724 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 319, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 16:37:22,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,727 [api2.py:131] throughput: 0.4100122219437654
(INFO) 2023-04-10 16:37:22,728 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,728 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 16:37:22,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,732 [api2.py:131] throughput: 1.08722599434951
(INFO) 2023-04-10 16:37:22,732 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,732 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 16:37:22,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,736 [api2.py:131] throughput: 8.833512736087995
(INFO) 2023-04-10 16:37:22,736 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-10 16:37:22,736 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 16:37:22,740 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,740 [api2.py:131] throughput: 0.25670604124193597
(INFO) 2023-04-10 16:37:22,745 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,746 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 16:37:22,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,756 [api2.py:131] throughput: 3.384093055579182
(INFO) 2023-04-10 16:37:22,756 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,756 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 223, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 53}
(INFO) 2023-04-10 16:37:22,765 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,766 [api2.py:131] throughput: 0.46962652757729934
(INFO) 2023-04-10 16:37:22,767 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,767 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 250}
(INFO) 2023-04-10 16:37:22,775 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,776 [api2.py:131] throughput: 2.9293455939724833
(INFO) 2023-04-10 16:37:22,776 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,776 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 16:37:22,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,785 [api2.py:131] throughput: 3.9214556224239305
(INFO) 2023-04-10 16:37:22,786 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,786 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 10, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 16:37:22,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,945 [api2.py:131] throughput: 3.923215051851949
(INFO) 2023-04-10 16:37:22,947 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,947 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 142, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 16:37:22,954 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,954 [api2.py:131] throughput: 0.7770410930607863
(INFO) 2023-04-10 16:37:22,955 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,955 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 16:37:22,963 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,963 [api2.py:131] throughput: 0.3703712091187085
(INFO) 2023-04-10 16:37:22,964 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,964 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 16:37:22,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,972 [api2.py:131] throughput: 0.45393304439659826
(INFO) 2023-04-10 16:37:22,972 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,973 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 345}
(INFO) 2023-04-10 16:37:22,980 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,980 [api2.py:131] throughput: 1.4600938632506884
(INFO) 2023-04-10 16:37:22,981 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 44}
(INFO) 2023-04-10 16:37:22,981 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 85}
(INFO) 2023-04-10 16:37:22,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:37:22,989 [api2.py:131] throughput: 0.36425974621484025
