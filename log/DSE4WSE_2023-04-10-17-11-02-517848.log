(DEBUG) 2023-04-10 17:11:02,518 [logger.py:40] logger init.
(INFO) 2023-04-10 17:11:02,518 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-17-11-02-517848.log
(INFO) 2023-04-10 17:11:06,783 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,783 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 214, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:11:06,786 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,786 [api2.py:131] throughput: 0.8720889894932178
(INFO) 2023-04-10 17:11:06,787 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,787 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 165, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:11:06,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,789 [api2.py:131] throughput: 0.7776317571969383
(INFO) 2023-04-10 17:11:06,790 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,790 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:06,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,791 [api2.py:131] throughput: 2.347156464286719
(INFO) 2023-04-10 17:11:06,792 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,792 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:06,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,794 [api2.py:131] throughput: 0.3752578954690733
(INFO) 2023-04-10 17:11:06,795 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,795 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 371, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:06,797 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,797 [api2.py:131] throughput: 0.553134096732728
(INFO) 2023-04-10 17:11:06,798 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,798 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 375, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:06,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,799 [api2.py:131] throughput: 0.5386567304236286
(INFO) 2023-04-10 17:11:06,800 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,800 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 300, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:06,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,802 [api2.py:131] throughput: 0.7616049224323056
(INFO) 2023-04-10 17:11:06,803 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,803 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 329, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:11:06,805 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,805 [api2.py:131] throughput: 0.5078809394960019
(INFO) 2023-04-10 17:11:06,806 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,806 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 412, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:11:06,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,807 [api2.py:131] throughput: 27.787873753571073
(INFO) 2023-04-10 17:11:06,808 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,808 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 373, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:11:06,810 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,810 [api2.py:131] throughput: 0.26317395435737406
(INFO) 2023-04-10 17:11:06,811 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,811 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 329, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:11:06,813 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,813 [api2.py:131] throughput: 1.0282202195672971
(INFO) 2023-04-10 17:11:06,814 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,814 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 360, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:11:06,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,815 [api2.py:131] throughput: 0.9004340559821429
(INFO) 2023-04-10 17:11:06,816 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,816 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 206, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:11:06,818 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,818 [api2.py:131] throughput: 0.5200690654381656
(INFO) 2023-04-10 17:11:06,819 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:11:06,819 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 366, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:11:06,820 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,820 [api2.py:131] throughput: 35.66351967379708
(INFO) 2023-04-10 17:11:06,825 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,825 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 83}
(INFO) 2023-04-10 17:11:06,827 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,827 [api2.py:131] throughput: 132.95762936255028
(INFO) 2023-04-10 17:11:06,828 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,828 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:11:06,829 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,830 [api2.py:131] throughput: 0.9671865157415515
(INFO) 2023-04-10 17:11:06,830 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,830 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 60, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:11:06,831 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,832 [api2.py:131] throughput: 36.00438998440989
(INFO) 2023-04-10 17:11:06,832 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,832 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:11:06,834 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,834 [api2.py:131] throughput: 0.3831316241224701
(INFO) 2023-04-10 17:11:06,835 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,835 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 172, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:11:06,836 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,836 [api2.py:131] throughput: 37.69821246192092
(INFO) 2023-04-10 17:11:06,837 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,837 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:11:06,839 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,839 [api2.py:131] throughput: 0.6699696787435147
(INFO) 2023-04-10 17:11:06,840 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:06,840 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 226, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:11:06,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,841 [api2.py:131] throughput: 0.6691362070897532
(INFO) 2023-04-10 17:11:06,846 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,846 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 482, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:11:06,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,849 [api2.py:131] throughput: 0.29604611484512644
(INFO) 2023-04-10 17:11:06,850 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,850 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:11:06,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,853 [api2.py:131] throughput: 2.310096135686247
(INFO) 2023-04-10 17:11:06,854 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,854 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:11:06,856 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,857 [api2.py:131] throughput: 3.346227458504638
(INFO) 2023-04-10 17:11:06,857 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,857 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:11:06,860 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,861 [api2.py:131] throughput: 109.02801339156257
(INFO) 2023-04-10 17:11:06,861 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,861 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 313, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:06,864 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,864 [api2.py:131] throughput: 0.611962986901754
(INFO) 2023-04-10 17:11:06,865 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,865 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:06,868 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,868 [api2.py:131] throughput: 1.8864690655293461
(INFO) 2023-04-10 17:11:06,869 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,869 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:06,872 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,872 [api2.py:131] throughput: 1.2295892125403776
(INFO) 2023-04-10 17:11:06,873 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,873 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 165, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:11:06,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,876 [api2.py:131] throughput: 52.75439812472419
(INFO) 2023-04-10 17:11:06,877 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,877 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 471, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:11:06,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,880 [api2.py:131] throughput: 0.34697687442369796
(INFO) 2023-04-10 17:11:06,880 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,881 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 345, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:11:06,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,883 [api2.py:131] throughput: 0.9454988757928975
(INFO) 2023-04-10 17:11:06,884 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,884 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:11:06,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,887 [api2.py:131] throughput: 1.0558433253806587
(INFO) 2023-04-10 17:11:06,888 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,888 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 128, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:11:06,891 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,891 [api2.py:131] throughput: 65.24867978160113
(INFO) 2023-04-10 17:11:06,891 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,892 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 146, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 111}
(INFO) 2023-04-10 17:11:06,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,895 [api2.py:131] throughput: 173.52506119659424
(INFO) 2023-04-10 17:11:06,895 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,895 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 410, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:06,898 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,898 [api2.py:131] throughput: 0.4561224460652439
(INFO) 2023-04-10 17:11:06,899 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,899 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 294, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:06,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,902 [api2.py:131] throughput: 0.33386534674008805
(INFO) 2023-04-10 17:11:06,903 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,903 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 213, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:11:06,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,906 [api2.py:131] throughput: 0.5746037245443529
(INFO) 2023-04-10 17:11:06,907 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,907 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:11:06,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,910 [api2.py:131] throughput: 0.9371346267686409
(INFO) 2023-04-10 17:11:06,911 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,911 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:11:06,914 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,914 [api2.py:131] throughput: 0.7318771991567743
(INFO) 2023-04-10 17:11:06,915 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,915 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 331, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:11:06,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,918 [api2.py:131] throughput: 0.5799248966638786
(INFO) 2023-04-10 17:11:06,919 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:11:06,919 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 326, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:11:06,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,922 [api2.py:131] throughput: 0.987004917897362
(INFO) 2023-04-10 17:11:06,927 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:06,927 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 420, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 198}
(INFO) 2023-04-10 17:11:06,939 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:06,939 [api2.py:131] throughput: 0.34045981589082613
(INFO) 2023-04-10 17:11:06,940 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:06,940 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 370, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 98}
(INFO) 2023-04-10 17:11:07,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,047 [api2.py:131] throughput: 0.5336850307914563
(INFO) 2023-04-10 17:11:07,048 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,048 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:07,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,058 [api2.py:131] throughput: 1.2663776356373786
(INFO) 2023-04-10 17:11:07,059 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,059 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 217, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 65}
(INFO) 2023-04-10 17:11:07,069 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,070 [api2.py:131] throughput: 0.8456709644841477
(INFO) 2023-04-10 17:11:07,070 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,070 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:11:07,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,082 [api2.py:131] throughput: 1.0020548554153288
(INFO) 2023-04-10 17:11:07,082 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,082 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:07,093 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,094 [api2.py:131] throughput: 0.9219510438776164
(INFO) 2023-04-10 17:11:07,094 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,094 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 17:11:07,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,106 [api2.py:131] throughput: 4.354836087738778
(INFO) 2023-04-10 17:11:07,106 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,106 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 393, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:07,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,118 [api2.py:131] throughput: 0.19261304996887035
(INFO) 2023-04-10 17:11:07,119 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,119 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 484, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 118}
(INFO) 2023-04-10 17:11:07,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,131 [api2.py:131] throughput: 0.33153418257816514
(INFO) 2023-04-10 17:11:07,131 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,131 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 93}
(INFO) 2023-04-10 17:11:07,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,143 [api2.py:131] throughput: 1.1632040020271461
(INFO) 2023-04-10 17:11:07,144 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,144 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 130}
(INFO) 2023-04-10 17:11:07,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,264 [api2.py:131] throughput: 0.7667349985937141
(INFO) 2023-04-10 17:11:07,265 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,265 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 106}
(INFO) 2023-04-10 17:11:07,276 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,276 [api2.py:131] throughput: 1.018091695250026
(INFO) 2023-04-10 17:11:07,277 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,277 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 368}
(INFO) 2023-04-10 17:11:07,288 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,288 [api2.py:131] throughput: 3.020510421446533
(INFO) 2023-04-10 17:11:07,289 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,289 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1042}
(INFO) 2023-04-10 17:11:07,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,300 [api2.py:131] throughput: 64.20067989366336
(INFO) 2023-04-10 17:11:07,301 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,301 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 321, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 347}
(INFO) 2023-04-10 17:11:07,312 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,312 [api2.py:131] throughput: 0.5190649833141814
(INFO) 2023-04-10 17:11:07,313 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,313 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 136, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:11:07,323 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,323 [api2.py:131] throughput: 0.8982108044106777
(INFO) 2023-04-10 17:11:07,324 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,324 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 151, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 664}
(INFO) 2023-04-10 17:11:07,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,335 [api2.py:131] throughput: 116.08183859329294
(INFO) 2023-04-10 17:11:07,335 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,335 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 192, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:11:07,345 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,346 [api2.py:131] throughput: 1.1271069743440105
(INFO) 2023-04-10 17:11:07,346 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,346 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 114, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:11:07,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,357 [api2.py:131] throughput: 1.1421213052470107
(INFO) 2023-04-10 17:11:07,357 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 37}
(INFO) 2023-04-10 17:11:07,358 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:11:07,479 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,480 [api2.py:131] throughput: 2.308011937845372
(INFO) 2023-04-10 17:11:07,485 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:07,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 134, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:07,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,592 [api1.py:131] throughput: 0.10314741752335793
(INFO) 2023-04-10 17:11:07,593 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:07,593 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:11:07,595 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,664 [api1.py:131] throughput: 0.8171515995256294
(INFO) 2023-04-10 17:11:07,665 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:07,682 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:07,684 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,787 [api1.py:131] throughput: 0.057210529549449024
(INFO) 2023-04-10 17:11:07,788 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:07,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:07,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,883 [api1.py:131] throughput: 0.06503881566468679
(INFO) 2023-04-10 17:11:07,884 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:07,884 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 238, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:11:07,886 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:07,970 [api1.py:131] throughput: 0.24780224388143357
(INFO) 2023-04-10 17:11:07,971 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:07,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 207, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:11:07,973 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,055 [api1.py:131] throughput: 0.4105271510177658
(INFO) 2023-04-10 17:11:08,056 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,056 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:11:08,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,132 [api1.py:131] throughput: 0.4318453035876629
(INFO) 2023-04-10 17:11:08,132 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,133 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 120, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:11:08,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,196 [api1.py:131] throughput: 1.2970494842848237
(INFO) 2023-04-10 17:11:08,197 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,197 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 228, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:08,199 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,292 [api1.py:131] throughput: 0.04931518519368581
(INFO) 2023-04-10 17:11:08,293 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,293 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 234, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 56}
(INFO) 2023-04-10 17:11:08,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,365 [api1.py:131] throughput: 4.157806499810043
(INFO) 2023-04-10 17:11:08,365 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,365 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 157, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 17:11:08,367 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,432 [api1.py:131] throughput: 0.8830868715586458
(INFO) 2023-04-10 17:11:08,433 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,433 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 212, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:11:08,435 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,503 [api1.py:131] throughput: 0.4351906070798952
(INFO) 2023-04-10 17:11:08,504 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,504 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 119, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:08,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,596 [api1.py:131] throughput: 0.12302066055819683
(INFO) 2023-04-10 17:11:08,597 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,597 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 448, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:11:08,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,662 [api1.py:131] throughput: 0.5022977816961183
(INFO) 2023-04-10 17:11:08,663 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,663 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:11:08,664 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,742 [api1.py:131] throughput: 0.9777905438111181
(INFO) 2023-04-10 17:11:08,743 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-10 17:11:08,743 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:11:08,745 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:08,951 [api1.py:131] throughput: 0.17842078229742456
(INFO) 2023-04-10 17:11:18,337 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,337 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 183}
(INFO) 2023-04-10 17:11:18,341 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,341 [api2.py:131] throughput: 216.29914301400154
(INFO) 2023-04-10 17:11:18,342 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,342 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 437, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 89}
(INFO) 2023-04-10 17:11:18,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,346 [api2.py:131] throughput: 47.115734510321744
(INFO) 2023-04-10 17:11:18,347 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,347 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 170, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:18,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,351 [api2.py:131] throughput: 1.1590408229559792
(INFO) 2023-04-10 17:11:18,351 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,351 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 162, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:11:18,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,356 [api2.py:131] throughput: 0.9216644025280153
(INFO) 2023-04-10 17:11:18,357 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,357 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:18,361 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,361 [api2.py:131] throughput: 1.3161529498717761
(INFO) 2023-04-10 17:11:18,362 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,362 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 115, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 44}
(INFO) 2023-04-10 17:11:18,366 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,366 [api2.py:131] throughput: 0.3919775453648041
(INFO) 2023-04-10 17:11:18,367 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,367 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:18,371 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,371 [api2.py:131] throughput: 0.6688042072408836
(INFO) 2023-04-10 17:11:18,372 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,372 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 159, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:11:18,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,376 [api2.py:131] throughput: 0.8164700011207455
(INFO) 2023-04-10 17:11:18,377 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,377 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 78}
(INFO) 2023-04-10 17:11:18,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,381 [api2.py:131] throughput: 0.5837981689155559
(INFO) 2023-04-10 17:11:18,382 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,382 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:11:18,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,386 [api2.py:131] throughput: 2.7456743606427083
(INFO) 2023-04-10 17:11:18,387 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,387 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 210, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:18,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,391 [api2.py:131] throughput: 0.7978813996666277
(INFO) 2023-04-10 17:11:18,392 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,392 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:18,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,396 [api2.py:131] throughput: 1.7282618833185528
(INFO) 2023-04-10 17:11:18,396 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,397 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 61}
(INFO) 2023-04-10 17:11:18,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,401 [api2.py:131] throughput: 0.7093388616400097
(INFO) 2023-04-10 17:11:18,402 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,402 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 254, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:11:18,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,406 [api2.py:131] throughput: 0.423114797884629
(INFO) 2023-04-10 17:11:18,406 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,406 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:11:18,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,411 [api2.py:131] throughput: 0.763116082701642
(INFO) 2023-04-10 17:11:18,411 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,411 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 224, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 525}
(INFO) 2023-04-10 17:11:18,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,415 [api2.py:131] throughput: 436.40960441878343
(INFO) 2023-04-10 17:11:18,416 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,416 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 238, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 17:11:18,420 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,420 [api2.py:131] throughput: 0.5906242272745109
(INFO) 2023-04-10 17:11:18,421 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,421 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 159}
(INFO) 2023-04-10 17:11:18,424 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,425 [api2.py:131] throughput: 0.5464895652403079
(INFO) 2023-04-10 17:11:18,425 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,425 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:11:18,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,430 [api2.py:131] throughput: 1.4714281807405665
(INFO) 2023-04-10 17:11:18,430 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:11:18,430 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 285, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:11:18,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:18,434 [api2.py:131] throughput: 0.810797958655147
(INFO) 2023-04-10 17:11:22,922 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:22,922 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:11:22,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,072 [api1.py:131] throughput: 0.34111694251727104
(INFO) 2023-04-10 17:11:23,073 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,073 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:11:23,076 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,220 [api1.py:131] throughput: 0.20003856308212312
(INFO) 2023-04-10 17:11:23,221 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,221 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 17:11:23,224 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,373 [api1.py:131] throughput: 2.573873208551301
(INFO) 2023-04-10 17:11:23,374 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:11:23,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,523 [api1.py:131] throughput: 0.48250451296893065
(INFO) 2023-04-10 17:11:23,524 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,524 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 251, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:11:23,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,677 [api1.py:131] throughput: 0.08456137044626114
(INFO) 2023-04-10 17:11:23,678 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,678 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 426, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:11:23,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,824 [api1.py:131] throughput: 0.09587201429083893
(INFO) 2023-04-10 17:11:23,826 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,826 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 426, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:11:23,828 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:23,941 [api1.py:131] throughput: 0.47859960123467166
(INFO) 2023-04-10 17:11:23,942 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:23,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 61}
(INFO) 2023-04-10 17:11:23,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,088 [api1.py:131] throughput: 2.0852170021478638
(INFO) 2023-04-10 17:11:24,089 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,089 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 339, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 79}
(INFO) 2023-04-10 17:11:24,091 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,195 [api1.py:131] throughput: 1.2167353203170204
(INFO) 2023-04-10 17:11:24,196 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,196 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:11:24,199 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,486 [api1.py:131] throughput: 0.543024409567561
(INFO) 2023-04-10 17:11:24,487 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 187, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:24,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,635 [api1.py:131] throughput: 0.12654914402358763
(INFO) 2023-04-10 17:11:24,636 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,637 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 160, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:11:24,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,760 [api1.py:131] throughput: 0.5176446789214655
(INFO) 2023-04-10 17:11:24,761 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,761 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 201, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 101}
(INFO) 2023-04-10 17:11:24,764 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,876 [api1.py:131] throughput: 0.7331042273653042
(INFO) 2023-04-10 17:11:24,877 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,877 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 416, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:11:24,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:24,986 [api1.py:131] throughput: 0.5645144041676012
(INFO) 2023-04-10 17:11:24,987 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:24,987 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 131, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 192}
(INFO) 2023-04-10 17:11:24,990 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:25,096 [api1.py:131] throughput: 2.528438691723952
(INFO) 2023-04-10 17:11:25,097 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:25,097 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 466, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:25,099 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:25,250 [api1.py:131] throughput: 0.02099948475099749
(INFO) 2023-04-10 17:11:25,251 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:25,251 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 324, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 17:11:25,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:25,372 [api1.py:131] throughput: 1.3165347363101974
(INFO) 2023-04-10 17:11:25,373 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:25,373 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 246, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:11:25,375 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:25,508 [api1.py:131] throughput: 0.581780178417543
(INFO) 2023-04-10 17:11:25,509 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:25,509 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 472, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:11:25,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:25,663 [api1.py:131] throughput: 0.038778515725904794
(INFO) 2023-04-10 17:11:25,665 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:11:25,665 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 94}
(INFO) 2023-04-10 17:11:25,668 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:25,786 [api1.py:131] throughput: 2.2622580361196825
(INFO) 2023-04-10 17:11:34,668 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:11:34,668 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:11:34,670 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:34,671 [api2.py:131] throughput: 70.36314490829987
(INFO) 2023-04-10 17:11:39,187 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:39,187 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 9, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 78}
(INFO) 2023-04-10 17:11:39,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:39,910 [api1.py:131] throughput: 1.9498881523357012
(INFO) 2023-04-10 17:11:39,911 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:39,911 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 18, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 153}
(INFO) 2023-04-10 17:11:39,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:40,615 [api1.py:131] throughput: 11.520963223801873
(INFO) 2023-04-10 17:11:40,616 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:40,616 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 21, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:11:40,623 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:41,384 [api1.py:131] throughput: 0.512452158895318
(INFO) 2023-04-10 17:11:41,385 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:41,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 13, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 17:11:41,393 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:42,251 [api1.py:131] throughput: 0.35572847435720195
(INFO) 2023-04-10 17:11:42,253 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:42,253 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 369}
(INFO) 2023-04-10 17:11:42,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:42,786 [api1.py:131] throughput: 16.696770236517867
(INFO) 2023-04-10 17:11:42,787 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:42,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 12, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 44}
(INFO) 2023-04-10 17:11:42,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:43,262 [api1.py:131] throughput: 2.449914371609144
(INFO) 2023-04-10 17:11:43,263 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:43,263 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 63}
(INFO) 2023-04-10 17:11:43,270 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:43,912 [api1.py:131] throughput: 3.0132934799538034
(INFO) 2023-04-10 17:11:43,913 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:43,914 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 10, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 121}
(INFO) 2023-04-10 17:11:43,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:44,592 [api1.py:131] throughput: 3.3074322395762366
(INFO) 2023-04-10 17:11:44,594 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:44,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 9, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:11:44,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:45,298 [api1.py:131] throughput: 1.9731661120308557
(INFO) 2023-04-10 17:11:45,300 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:45,300 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 18, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 71}
(INFO) 2023-04-10 17:11:45,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:45,789 [api1.py:131] throughput: 6.910866673856354
(INFO) 2023-04-10 17:11:45,790 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:45,790 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 20, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:11:45,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:46,502 [api1.py:131] throughput: 1.0159475309812762
(INFO) 2023-04-10 17:11:46,503 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:46,503 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 21, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 237}
(INFO) 2023-04-10 17:11:46,511 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:47,155 [api1.py:131] throughput: 11.454981285377658
(INFO) 2023-04-10 17:11:47,156 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:47,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 14, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 165}
(INFO) 2023-04-10 17:11:47,296 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:47,613 [api1.py:131] throughput: 15.474674117910315
(INFO) 2023-04-10 17:11:47,614 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:47,615 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 111}
(INFO) 2023-04-10 17:11:47,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:48,343 [api1.py:131] throughput: 0.5151413248314964
(INFO) 2023-04-10 17:11:48,345 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:48,345 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 198}
(INFO) 2023-04-10 17:11:48,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:49,014 [api1.py:131] throughput: 13.604887009978688
(INFO) 2023-04-10 17:11:49,015 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:49,015 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 13, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 99}
(INFO) 2023-04-10 17:11:49,023 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:49,510 [api1.py:131] throughput: 3.8167472512560456
(INFO) 2023-04-10 17:11:49,511 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:49,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 19, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:11:49,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:50,229 [api1.py:131] throughput: 1.4288886719766731
(INFO) 2023-04-10 17:11:50,231 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:50,231 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 7, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:11:50,238 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:51,138 [api1.py:131] throughput: 0.2246853054017578
(INFO) 2023-04-10 17:11:51,139 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:51,139 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 15, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:11:51,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:51,905 [api1.py:131] throughput: 0.4235901742236954
(INFO) 2023-04-10 17:11:51,906 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:11:51,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 101}
(INFO) 2023-04-10 17:11:51,914 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:11:52,523 [api1.py:131] throughput: 10.644469695143474
(INFO) 2023-04-10 17:12:01,565 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,565 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:12:01,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,568 [api2.py:131] throughput: 0.3594188203425762
(INFO) 2023-04-10 17:12:01,568 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,569 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:12:01,571 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,571 [api2.py:131] throughput: 8.21421784485404
(INFO) 2023-04-10 17:12:01,572 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,572 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:12:01,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,574 [api2.py:131] throughput: 2.834849804668013
(INFO) 2023-04-10 17:12:01,574 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,575 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 168, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 47}
(INFO) 2023-04-10 17:12:01,577 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,577 [api2.py:131] throughput: 0.9352488483278291
(INFO) 2023-04-10 17:12:01,578 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,578 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 467, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:12:01,580 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,580 [api2.py:131] throughput: 0.4034795347331651
(INFO) 2023-04-10 17:12:01,581 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,581 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:12:01,583 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,583 [api2.py:131] throughput: 2.190385623755783
(INFO) 2023-04-10 17:12:01,584 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,584 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 96}
(INFO) 2023-04-10 17:12:01,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,586 [api2.py:131] throughput: 3.226585433374229
(INFO) 2023-04-10 17:12:01,587 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,587 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:01,589 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,589 [api2.py:131] throughput: 1.3953048566233512
(INFO) 2023-04-10 17:12:01,590 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,590 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:12:01,593 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,593 [api2.py:131] throughput: 13.5533410930247
(INFO) 2023-04-10 17:12:01,594 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,594 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 120, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:12:01,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,596 [api2.py:131] throughput: 1.0154060694256
(INFO) 2023-04-10 17:12:01,597 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,597 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:12:01,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,600 [api2.py:131] throughput: 1.4360635994043378
(INFO) 2023-04-10 17:12:01,600 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,600 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:12:01,602 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,603 [api2.py:131] throughput: 1.2399375079809465
(INFO) 2023-04-10 17:12:01,603 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,603 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:12:01,606 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,606 [api2.py:131] throughput: 2.2510818241785775
(INFO) 2023-04-10 17:12:01,607 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,607 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:01,609 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,609 [api2.py:131] throughput: 2.215473493909697
(INFO) 2023-04-10 17:12:01,610 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,610 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 11, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:12:01,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,613 [api2.py:131] throughput: 0.7026678723889386
(INFO) 2023-04-10 17:12:01,613 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,614 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:12:01,616 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,616 [api2.py:131] throughput: 1.525181466084379
(INFO) 2023-04-10 17:12:01,617 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,617 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 70}
(INFO) 2023-04-10 17:12:01,619 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,619 [api2.py:131] throughput: 1.292280938888284
(INFO) 2023-04-10 17:12:01,620 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,620 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 195, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:12:01,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,623 [api2.py:131] throughput: 0.8322474394927065
(INFO) 2023-04-10 17:12:01,623 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,623 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:12:01,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,625 [api2.py:131] throughput: 2.0830676881976293
(INFO) 2023-04-10 17:12:01,626 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-10 17:12:01,626 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:12:01,628 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:01,629 [api2.py:131] throughput: 0.8577077002292628
(INFO) 2023-04-10 17:12:06,113 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:06,114 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:12:06,116 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:06,288 [api1.py:131] throughput: 0.5511984734375526
(INFO) 2023-04-10 17:12:06,289 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:06,289 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 97, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:12:06,292 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:06,440 [api1.py:131] throughput: 2.6034209954679643
(INFO) 2023-04-10 17:12:06,441 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:06,441 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 308}
(INFO) 2023-04-10 17:12:06,443 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:06,566 [api1.py:131] throughput: 14.834271752304126
(INFO) 2023-04-10 17:12:06,567 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:06,568 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 17:12:06,570 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:06,704 [api1.py:131] throughput: 2.8414465563818845
(INFO) 2023-04-10 17:12:06,705 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:06,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 70}
(INFO) 2023-04-10 17:12:06,708 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:06,870 [api1.py:131] throughput: 1.0290189561295022
(INFO) 2023-04-10 17:12:06,871 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:06,871 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:12:06,874 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:07,048 [api1.py:131] throughput: 0.20374810626186798
(INFO) 2023-04-10 17:12:07,049 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:07,049 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:12:07,053 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:07,201 [api1.py:131] throughput: 1.3306168846799835
(INFO) 2023-04-10 17:12:07,202 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:07,202 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:07,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:07,374 [api1.py:131] throughput: 0.24566912197777188
(INFO) 2023-04-10 17:12:07,375 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:07,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:12:07,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:07,557 [api1.py:131] throughput: 0.15128342304530867
(INFO) 2023-04-10 17:12:07,558 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:07,558 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:12:07,561 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:07,677 [api1.py:131] throughput: 0.7539941318219391
(INFO) 2023-04-10 17:12:07,678 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:07,678 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 359, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:07,682 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:07,863 [api1.py:131] throughput: 0.04321481547140116
(INFO) 2023-04-10 17:12:07,864 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:07,864 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:12:07,868 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:08,153 [api1.py:131] throughput: 0.2260592407953343
(INFO) 2023-04-10 17:12:08,154 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:08,154 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:08,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:08,344 [api1.py:131] throughput: 0.4409456267337276
(INFO) 2023-04-10 17:12:08,345 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:08,345 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 73}
(INFO) 2023-04-10 17:12:08,348 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:08,496 [api1.py:131] throughput: 4.246866087212679
(INFO) 2023-04-10 17:12:08,497 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:08,497 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:12:08,500 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:08,624 [api1.py:131] throughput: 1.3060199677427695
(INFO) 2023-04-10 17:12:08,625 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:08,625 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 17:12:08,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:08,801 [api1.py:131] throughput: 1.4647581837094346
(INFO) 2023-04-10 17:12:08,802 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:08,803 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 57}
(INFO) 2023-04-10 17:12:08,805 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:08,911 [api1.py:131] throughput: 3.4081192852655824
(INFO) 2023-04-10 17:12:08,912 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:08,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 17:12:08,915 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:09,044 [api1.py:131] throughput: 1.6975261325471864
(INFO) 2023-04-10 17:12:09,045 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:09,045 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:12:09,048 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:09,181 [api1.py:131] throughput: 1.2504582758883651
(INFO) 2023-04-10 17:12:09,182 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:12:09,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:12:09,186 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:09,360 [api1.py:131] throughput: 0.5409901352759943
(INFO) 2023-04-10 17:12:13,982 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:13,982 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 113, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:12:13,985 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:13,985 [api2.py:131] throughput: 1.5426021598523243
(INFO) 2023-04-10 17:12:13,986 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:13,986 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:12:13,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:13,989 [api2.py:131] throughput: 1.4250137603888748
(INFO) 2023-04-10 17:12:13,990 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:13,990 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 94, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:13,993 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:13,993 [api2.py:131] throughput: 1.2474070698098971
(INFO) 2023-04-10 17:12:13,994 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:13,994 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 142, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:12:13,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:13,997 [api2.py:131] throughput: 0.7528674976332254
(INFO) 2023-04-10 17:12:13,998 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:13,998 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:12:14,001 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,001 [api2.py:131] throughput: 1.5755981363794993
(INFO) 2023-04-10 17:12:14,002 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,002 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:12:14,005 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,005 [api2.py:131] throughput: 1.504758677733832
(INFO) 2023-04-10 17:12:14,006 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,006 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 467, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:14,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,009 [api2.py:131] throughput: 0.38107523926110715
(INFO) 2023-04-10 17:12:14,009 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,010 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 86, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:12:14,013 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,013 [api2.py:131] throughput: 0.6816118680679308
(INFO) 2023-04-10 17:12:14,013 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,014 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 141}
(INFO) 2023-04-10 17:12:14,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,017 [api2.py:131] throughput: 29.107579123308216
(INFO) 2023-04-10 17:12:14,018 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,018 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 86, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:14,021 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,021 [api2.py:131] throughput: 0.5901242426619286
(INFO) 2023-04-10 17:12:14,021 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,022 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 446, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 169}
(INFO) 2023-04-10 17:12:14,025 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,025 [api2.py:131] throughput: 0.41809428495346035
(INFO) 2023-04-10 17:12:14,026 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,026 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 225, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:12:14,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,029 [api2.py:131] throughput: 0.4519360134732639
(INFO) 2023-04-10 17:12:14,030 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,030 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:12:14,033 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,033 [api2.py:131] throughput: 0.8016214880471452
(INFO) 2023-04-10 17:12:14,034 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,034 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:14,037 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,037 [api2.py:131] throughput: 2.04937857332927
(INFO) 2023-04-10 17:12:14,038 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,038 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:14,041 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,042 [api2.py:131] throughput: 2.3215628240901705
(INFO) 2023-04-10 17:12:14,042 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,042 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:12:14,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,046 [api2.py:131] throughput: 1.7933243755489583
(INFO) 2023-04-10 17:12:14,046 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,046 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:12:14,049 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,050 [api2.py:131] throughput: 1.4641891842251922
(INFO) 2023-04-10 17:12:14,050 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,050 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:14,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,054 [api2.py:131] throughput: 1.5351805411617028
(INFO) 2023-04-10 17:12:14,054 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,055 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:12:14,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,058 [api2.py:131] throughput: 0.7887561824350191
(INFO) 2023-04-10 17:12:14,059 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:12:14,059 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 120, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:14,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:14,062 [api2.py:131] throughput: 1.0157662524550664
(INFO) 2023-04-10 17:12:18,590 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,590 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 243, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:12:18,595 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,595 [api2.py:131] throughput: 0.5350417830753221
(INFO) 2023-04-10 17:12:18,596 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,596 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 338, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:18,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,601 [api2.py:131] throughput: 0.5293278076661019
(INFO) 2023-04-10 17:12:18,602 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,602 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 94, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:12:18,607 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,607 [api2.py:131] throughput: 2.424799448381745
(INFO) 2023-04-10 17:12:18,608 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,608 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 401, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 89}
(INFO) 2023-04-10 17:12:18,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,613 [api2.py:131] throughput: 0.7461579533112186
(INFO) 2023-04-10 17:12:18,614 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,614 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 334, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:12:18,619 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,619 [api2.py:131] throughput: 0.7760743721337824
(INFO) 2023-04-10 17:12:18,620 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,620 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 395, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:12:18,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,625 [api2.py:131] throughput: 0.4579334340598147
(INFO) 2023-04-10 17:12:18,626 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,626 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 504, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:12:18,631 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,632 [api2.py:131] throughput: 0.21325166323904843
(INFO) 2023-04-10 17:12:18,632 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,632 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:12:18,637 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,638 [api2.py:131] throughput: 4.240560508302199
(INFO) 2023-04-10 17:12:18,638 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,638 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 508, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:12:18,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,644 [api2.py:131] throughput: 0.42028461271327355
(INFO) 2023-04-10 17:12:18,644 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,644 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 466, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:12:18,650 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,650 [api2.py:131] throughput: 0.3279822235131404
(INFO) 2023-04-10 17:12:18,650 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,650 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:12:18,656 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,656 [api2.py:131] throughput: 1.0406413348851382
(INFO) 2023-04-10 17:12:18,656 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,656 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 244, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 145}
(INFO) 2023-04-10 17:12:18,661 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,662 [api2.py:131] throughput: 114.26897271713128
(INFO) 2023-04-10 17:12:18,662 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,662 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 271, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 233}
(INFO) 2023-04-10 17:12:18,783 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,784 [api2.py:131] throughput: 0.6931853918324521
(INFO) 2023-04-10 17:12:18,785 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,785 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 216, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:12:18,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,790 [api2.py:131] throughput: 0.6568545800957738
(INFO) 2023-04-10 17:12:18,790 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,791 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:12:18,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,796 [api2.py:131] throughput: 1.2778055503570414
(INFO) 2023-04-10 17:12:18,796 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,796 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 195, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:12:18,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,802 [api2.py:131] throughput: 0.8612010025809984
(INFO) 2023-04-10 17:12:18,802 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,802 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 194, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:12:18,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,807 [api2.py:131] throughput: 1.0173712109274047
(INFO) 2023-04-10 17:12:18,808 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,808 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 328, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:12:18,813 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,814 [api2.py:131] throughput: 0.44736255947166764
(INFO) 2023-04-10 17:12:18,814 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,814 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 47}
(INFO) 2023-04-10 17:12:18,819 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,819 [api2.py:131] throughput: 0.9777791668442778
(INFO) 2023-04-10 17:12:18,820 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:12:18,820 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 275, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:18,825 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:18,825 [api2.py:131] throughput: 0.6700301598449564
(INFO) 2023-04-10 17:12:23,264 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:23,264 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 427, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 122}
(INFO) 2023-04-10 17:12:23,268 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:23,592 [api1.py:131] throughput: 1.5030011787621775
(INFO) 2023-04-10 17:12:23,593 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:23,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 208, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:12:23,598 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:24,033 [api1.py:131] throughput: 0.36224948905571075
(INFO) 2023-04-10 17:12:24,034 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:24,034 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 261, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 108}
(INFO) 2023-04-10 17:12:24,040 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:24,276 [api1.py:131] throughput: 2.9149862175098598
(INFO) 2023-04-10 17:12:24,277 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:24,277 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 502, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:12:24,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:24,633 [api1.py:131] throughput: 0.0910834657352798
(INFO) 2023-04-10 17:12:24,634 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:24,635 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 176, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:12:24,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:24,967 [api1.py:131] throughput: 0.2911368954800471
(INFO) 2023-04-10 17:12:24,968 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:24,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 17:12:24,973 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:25,278 [api1.py:131] throughput: 1.2224924093445864
(INFO) 2023-04-10 17:12:25,279 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:25,279 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 391, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 187}
(INFO) 2023-04-10 17:12:25,284 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:25,620 [api1.py:131] throughput: 2.6728920675063916
(INFO) 2023-04-10 17:12:25,621 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:25,621 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:12:25,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:25,947 [api1.py:131] throughput: 0.5369231485751482
(INFO) 2023-04-10 17:12:25,948 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:25,949 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:25,953 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:26,279 [api1.py:131] throughput: 0.14662439089630372
(INFO) 2023-04-10 17:12:26,281 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:26,281 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 445, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:12:26,286 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:26,606 [api1.py:131] throughput: 0.1441047734008491
(INFO) 2023-04-10 17:12:26,607 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:26,607 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:12:26,612 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:26,947 [api1.py:131] throughput: 0.03424271228281953
(INFO) 2023-04-10 17:12:26,948 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:26,948 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:12:26,953 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:27,396 [api1.py:131] throughput: 0.7177639771913396
(INFO) 2023-04-10 17:12:27,397 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:27,397 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 57}
(INFO) 2023-04-10 17:12:27,402 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:27,718 [api1.py:131] throughput: 1.5772373386259766
(INFO) 2023-04-10 17:12:27,719 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:27,719 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 482, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:12:27,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:28,011 [api1.py:131] throughput: 0.26406548596527074
(INFO) 2023-04-10 17:12:28,012 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:28,012 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 120, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:12:28,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:28,339 [api1.py:131] throughput: 0.5696336809236563
(INFO) 2023-04-10 17:12:28,340 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:28,340 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:12:28,345 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:28,798 [api1.py:131] throughput: 0.3685981331513255
(INFO) 2023-04-10 17:12:28,799 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:28,799 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 308, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 63}
(INFO) 2023-04-10 17:12:28,804 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:29,079 [api1.py:131] throughput: 0.9146163027655089
(INFO) 2023-04-10 17:12:29,080 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:29,080 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 47}
(INFO) 2023-04-10 17:12:29,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:29,341 [api1.py:131] throughput: 2.5411378599713528
(INFO) 2023-04-10 17:12:29,342 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:29,342 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 384, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:12:29,347 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:29,696 [api1.py:131] throughput: 0.07762677319088204
(INFO) 2023-04-10 17:12:29,697 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:12:29,697 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 210, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:12:29,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:30,017 [api1.py:131] throughput: 0.5487939668661335
(INFO) 2023-04-10 17:12:34,737 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,737 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 15, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 72}
(INFO) 2023-04-10 17:12:34,745 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,746 [api2.py:131] throughput: 1.606249990894857
(INFO) 2023-04-10 17:12:34,746 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,746 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 14, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 665}
(INFO) 2023-04-10 17:12:34,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,755 [api2.py:131] throughput: 11.698109451088232
(INFO) 2023-04-10 17:12:34,755 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,755 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 16, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 140}
(INFO) 2023-04-10 17:12:34,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,763 [api2.py:131] throughput: 1.5646636705692747
(INFO) 2023-04-10 17:12:34,764 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,764 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 16, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 392}
(INFO) 2023-04-10 17:12:34,772 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,772 [api2.py:131] throughput: 13.684995195894166
(INFO) 2023-04-10 17:12:34,773 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,773 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:12:34,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,781 [api2.py:131] throughput: 0.5976712544343847
(INFO) 2023-04-10 17:12:34,782 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,782 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 15, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 243}
(INFO) 2023-04-10 17:12:34,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,791 [api2.py:131] throughput: 3.6589105805921283
(INFO) 2023-04-10 17:12:34,791 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,792 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 8, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 177}
(INFO) 2023-04-10 17:12:34,800 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,800 [api2.py:131] throughput: 3.722303760091835
(INFO) 2023-04-10 17:12:34,801 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,801 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 13, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 94}
(INFO) 2023-04-10 17:12:34,809 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,809 [api2.py:131] throughput: 0.6020325742865742
(INFO) 2023-04-10 17:12:34,810 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,810 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 9, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 114}
(INFO) 2023-04-10 17:12:34,817 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,818 [api2.py:131] throughput: 1.8876815975766743
(INFO) 2023-04-10 17:12:34,818 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,818 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:12:34,826 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,827 [api2.py:131] throughput: 0.366911529541379
(INFO) 2023-04-10 17:12:34,827 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,827 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 18, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:12:34,835 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,835 [api2.py:131] throughput: 1.996275026544846
(INFO) 2023-04-10 17:12:34,836 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,836 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 8, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:12:34,844 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,844 [api2.py:131] throughput: 1.120530123523255
(INFO) 2023-04-10 17:12:34,845 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,845 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 89}
(INFO) 2023-04-10 17:12:34,852 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,853 [api2.py:131] throughput: 3.639598831397131
(INFO) 2023-04-10 17:12:34,853 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,853 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 451, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 114}
(INFO) 2023-04-10 17:12:34,861 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,862 [api2.py:131] throughput: 0.35011930563445093
(INFO) 2023-04-10 17:12:34,862 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,862 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:12:34,871 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,871 [api2.py:131] throughput: 0.4768142354651867
(INFO) 2023-04-10 17:12:34,872 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,872 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 96}
(INFO) 2023-04-10 17:12:34,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:34,880 [api2.py:131] throughput: 0.3783799941959432
(INFO) 2023-04-10 17:12:34,881 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:34,881 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 47}
(INFO) 2023-04-10 17:12:35,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:35,022 [api2.py:131] throughput: 1.1110988628435639
(INFO) 2023-04-10 17:12:35,023 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:35,024 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 16, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 463}
(INFO) 2023-04-10 17:12:35,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:35,032 [api2.py:131] throughput: 15.978905585205348
(INFO) 2023-04-10 17:12:35,033 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:35,033 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 11, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 90}
(INFO) 2023-04-10 17:12:35,042 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:35,042 [api2.py:131] throughput: 0.3871175403287038
(INFO) 2023-04-10 17:12:35,043 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 40}
(INFO) 2023-04-10 17:12:35,043 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 16, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 82}
(INFO) 2023-04-10 17:12:35,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:12:35,051 [api2.py:131] throughput: 1.0823145165486645
