(DEBUG) 2023-04-09 19:56:21,745 [logger.py:40] logger init.
(INFO) 2023-04-09 19:56:21,746 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-09-19-56-21-745776.log
(INFO) 2023-04-09 19:56:24,200 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:24,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,214 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,276 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:24,277 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,290 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,326 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 32, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:56:24,327 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,331 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:56:24,332 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,357 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:24,358 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,365 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,416 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:56:24,416 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,423 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:56:24,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,433 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:24,433 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,460 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:56:24,461 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,469 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,530 [api1.py:131] throughput: 0.008681804420119654
(INFO) 2023-04-09 19:56:24,531 [api1.py:131] throughput: 0.005788072233261248
(INFO) 2023-04-09 19:56:24,547 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:24,548 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,547 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:24,548 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,548 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:56:24,549 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,554 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 19:56:24,555 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,558 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,559 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,564 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:24,565 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,566 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:56:24,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,590 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:24,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,603 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,643 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 19:56:24,662 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 19:56:24,681 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:56:24,682 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,690 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,696 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:24,696 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,709 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:56:24,722 [api1.py:131] throughput: 0.0069455879054211534
(INFO) 2023-04-09 19:56:24,742 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:24,743 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,758 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:24,758 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,764 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,764 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 19:56:24,805 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:24,805 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,812 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,850 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:56:24,863 [api1.py:131] throughput: 0.005788071943735196
(INFO) 2023-04-09 19:56:24,883 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:56:24,886 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:24,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,891 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,899 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:24,899 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,904 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,915 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:56:24,918 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:56:24,918 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,949 [api1.py:131] throughput: 0.005788071750717845
(INFO) 2023-04-09 19:56:24,948 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:56:24,949 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,959 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:56:24,959 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,981 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:24,982 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,987 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:24,988 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:24,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:24,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,017 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:56:25,024 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 19:56:25,057 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:25,058 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,061 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:56:25,061 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,070 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,112 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:56:25,112 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:56:25,116 [api1.py:131] throughput: 0.005788072969484766
(INFO) 2023-04-09 19:56:25,128 [api1.py:131] throughput: 0.005788073225042196
(INFO) 2023-04-09 19:56:25,145 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:25,145 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,146 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:25,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,156 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:56:25,160 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 19:56:25,165 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:56:25,165 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,171 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,172 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:25,172 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,188 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,193 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:25,193 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,195 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:56:25,195 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,203 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,244 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:56:25,253 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 19:56:25,273 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:25,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,277 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:56:25,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,285 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:25,286 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,291 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,298 [api1.py:131] throughput: 0.008681804420119654
(INFO) 2023-04-09 19:56:25,319 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:56:25,319 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,332 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,333 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:25,333 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,348 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:56:25,372 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 19:56:25,379 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:56:25,384 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:25,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,405 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:25,405 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,411 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,415 [api1.py:131] throughput: 0.0069455907076916915
(INFO) 2023-04-09 19:56:25,415 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:25,416 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,450 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:25,450 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,481 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:56:25,496 [api1.py:131] throughput: 0.00868180376558486
(INFO) 2023-04-09 19:56:25,519 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:25,520 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,524 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 19:56:25,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,535 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:56:25,535 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,543 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:56:25,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,558 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:25,559 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,565 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,575 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:56:25,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,588 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,632 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:56:25,640 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:56:25,671 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:56:25,672 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,679 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 34, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:25,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,692 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,697 [api1.py:131] throughput: 0.005788071398745061
(INFO) 2023-04-09 19:56:25,729 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:56:25,730 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,748 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,750 [api1.py:131] throughput: 0.004341129735099941
(INFO) 2023-04-09 19:56:25,787 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:25,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,820 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 19:56:25,864 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:25,865 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:25,874 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:25,983 [api1.py:131] throughput: 0.00434112949236297
(INFO) 2023-04-09 19:56:25,996 [api1.py:131] throughput: 0.00868180734098237
(INFO) 2023-04-09 19:56:25,997 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 19:56:26,018 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:26,019 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,026 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,031 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:26,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,037 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:56:26,038 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,040 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,042 [api1.py:131] throughput: 0.006945590847171279
(INFO) 2023-04-09 19:56:26,052 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,058 [api1.py:131] throughput: 0.0043411303502960494
(INFO) 2023-04-09 19:56:26,058 [api1.py:131] throughput: 0.002480701131444747
(INFO) 2023-04-09 19:56:26,083 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:56:26,083 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,087 [api1.py:131] throughput: 0.004341130189065275
(INFO) 2023-04-09 19:56:26,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,100 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:56:26,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,106 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:26,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,121 [api1.py:131] throughput: 0.00496125450707324
(INFO) 2023-04-09 19:56:26,128 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:26,129 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,129 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 19:56:26,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,138 [api1.py:131] throughput: 0.011575338091036336
(INFO) 2023-04-09 19:56:26,156 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:26,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,161 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,165 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:26,166 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,176 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:26,177 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,243 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:56:26,254 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:56:26,256 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:56:26,273 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:26,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,289 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:56:26,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,292 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:56:26,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,295 [api1.py:131] throughput: 0.006945589137188145
(INFO) 2023-04-09 19:56:26,296 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 19:56:26,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,329 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:56:26,329 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,332 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:26,332 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,336 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,385 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:56:26,404 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:56:26,419 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:56:26,420 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,426 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:56:26,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,443 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:26,444 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,453 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 19:56:26,454 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,463 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:26,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,471 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,488 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:56:26,494 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:26,494 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,505 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,523 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:26,524 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,528 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,594 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:56:26,615 [api1.py:131] throughput: 0.0038588045044240116
(INFO) 2023-04-09 19:56:26,618 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:56:26,629 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:26,630 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,642 [api1.py:131] throughput: 0.004341129918401875
(INFO) 2023-04-09 19:56:26,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,654 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 51}
(INFO) 2023-04-09 19:56:26,655 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,659 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:26,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,680 [api1.py:131] throughput: 0.011575337387769596
(INFO) 2023-04-09 19:56:26,681 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 44}
(INFO) 2023-04-09 19:56:26,682 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,714 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:26,714 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,722 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,787 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 19:56:26,822 [api1.py:131] throughput: 0.008681806387110151
(INFO) 2023-04-09 19:56:26,825 [api1.py:131] throughput: 0.004341129683097727
(INFO) 2023-04-09 19:56:26,830 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:56:26,831 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,851 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,864 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 32, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:56:26,864 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,870 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:26,871 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:26,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,884 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:26,954 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 19:56:26,986 [api1.py:131] throughput: 0.017361800700790336
(INFO) 2023-04-09 19:56:26,992 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:56:26,993 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,002 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,026 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:56:27,028 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:56:27,029 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,067 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 38}
(INFO) 2023-04-09 19:56:27,068 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,087 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,140 [api1.py:131] throughput: 0.0028941369584901426
(INFO) 2023-04-09 19:56:27,145 [api1.py:131] throughput: 0.002480701134635157
(INFO) 2023-04-09 19:56:27,182 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:27,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,187 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:27,187 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,190 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,195 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 19:56:27,231 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:27,232 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,353 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 19:56:27,364 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:56:27,368 [api1.py:131] throughput: 0.0043411296769798186
(INFO) 2023-04-09 19:56:27,384 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:27,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,402 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:27,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,404 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:27,405 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,415 [api1.py:131] throughput: 0.005788071437064676
(INFO) 2023-04-09 19:56:27,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,451 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:27,452 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,463 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:56:27,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,470 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:56:27,480 [api1.py:131] throughput: 0.004341130296137121
(INFO) 2023-04-09 19:56:27,495 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:27,496 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,501 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:27,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,503 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,517 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:27,517 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,599 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 19:56:27,608 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:56:27,608 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 19:56:27,638 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:56:27,638 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,645 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:56:27,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,649 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:27,649 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,655 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,655 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,655 [api1.py:131] throughput: 0.005788073374334792
(INFO) 2023-04-09 19:56:27,697 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 53}
(INFO) 2023-04-09 19:56:27,698 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,782 [api1.py:131] throughput: 0.004961254936826791
(INFO) 2023-04-09 19:56:27,827 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:56:27,827 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,829 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 19:56:27,831 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 19:56:27,836 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,861 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 49}
(INFO) 2023-04-09 19:56:27,861 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,867 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:27,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,874 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,878 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,934 [api1.py:131] throughput: 0.008681805842040405
(INFO) 2023-04-09 19:56:27,936 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:56:27,970 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:27,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,974 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:56:27,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:27,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:27,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,005 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 19:56:28,038 [api1.py:131] throughput: 0.006945591317678867
(INFO) 2023-04-09 19:56:28,043 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:56:28,044 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,072 [api1.py:131] throughput: 0.0031572304148685866
(INFO) 2023-04-09 19:56:28,078 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:56:28,079 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,091 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:56:28,097 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,120 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:56:28,121 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,124 [api1.py:131] throughput: 0.008681808706134924
(INFO) 2023-04-09 19:56:28,127 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:28,127 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,128 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,135 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,160 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:56:28,161 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,169 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,202 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:56:28,221 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:56:28,235 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:28,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,243 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,248 [api1.py:131] throughput: 0.002480701131444747
(INFO) 2023-04-09 19:56:28,257 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:56:28,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,274 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,287 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:28,288 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,296 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,315 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 19:56:28,323 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 19:56:28,344 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:28,345 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,352 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,353 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:28,353 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,363 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,377 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:56:28,414 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:28,415 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,492 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 19:56:28,525 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:28,525 [api1.py:131] throughput: 0.003858804423740846
(INFO) 2023-04-09 19:56:28,525 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,533 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,554 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 19:56:28,565 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:56:28,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,572 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,589 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:28,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,637 [api1.py:131] throughput: 0.002671518205535664
(INFO) 2023-04-09 19:56:28,637 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 19:56:28,673 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:56:28,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,684 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:56:28,685 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,690 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,698 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:56:28,728 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:56:28,728 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,745 [api1.py:131] throughput: 0.005788072378024285
(INFO) 2023-04-09 19:56:28,752 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 19:56:28,778 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:28,779 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,787 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 44}
(INFO) 2023-04-09 19:56:28,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,839 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 19:56:28,861 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:56:28,872 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:56:28,876 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:28,876 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,892 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:56:28,893 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,901 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:56:28,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,905 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,920 [api1.py:131] throughput: 0.008681808658234827
(INFO) 2023-04-09 19:56:28,955 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:56:28,955 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:28,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:28,987 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:56:29,027 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:56:29,028 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,041 [api1.py:131] throughput: 0.0043411300644473315
(INFO) 2023-04-09 19:56:29,079 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 41, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:56:29,080 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,137 [api1.py:131] throughput: 0.004341130319859627
(INFO) 2023-04-09 19:56:29,147 [api1.py:131] throughput: 0.0031572303817637057
(INFO) 2023-04-09 19:56:29,162 [api1.py:131] throughput: 0.011575340077537005
(INFO) 2023-04-09 19:56:29,173 [api1.py:131] throughput: 0.006945589316354288
(INFO) 2023-04-09 19:56:29,183 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:29,184 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,189 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:29,189 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,197 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 53}
(INFO) 2023-04-09 19:56:29,198 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,198 [api1.py:131] throughput: 0.005788072829344386
(INFO) 2023-04-09 19:56:29,199 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,203 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:29,203 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,212 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,232 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:56:29,233 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,313 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 19:56:29,344 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:56:29,344 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,358 [api1.py:131] throughput: 0.006945590013111603
(INFO) 2023-04-09 19:56:29,362 [api1.py:131] throughput: 0.0049612545695160585
(INFO) 2023-04-09 19:56:29,400 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:29,400 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,403 [api1.py:131] throughput: 0.004341129813435761
(INFO) 2023-04-09 19:56:29,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,408 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:29,408 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,416 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,442 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:56:29,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,485 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:56:29,510 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 19:56:29,521 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:56:29,522 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,547 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 39}
(INFO) 2023-04-09 19:56:29,548 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,612 [api1.py:131] throughput: 0.0069455905705175566
(INFO) 2023-04-09 19:56:29,634 [api1.py:131] throughput: 0.0069455906312028875
(INFO) 2023-04-09 19:56:29,658 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:29,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,667 [api1.py:131] throughput: 0.003858804426942559
(INFO) 2023-04-09 19:56:29,675 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 44}
(INFO) 2023-04-09 19:56:29,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,687 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,707 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 51}
(INFO) 2023-04-09 19:56:29,707 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,719 [api1.py:131] throughput: 0.011575342323907762
(INFO) 2023-04-09 19:56:29,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,743 [api1.py:131] throughput: 0.008681806502731015
(INFO) 2023-04-09 19:56:29,754 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:29,755 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,760 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,777 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:29,778 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,784 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 19:56:29,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,817 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:56:29,818 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,828 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,833 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:56:29,867 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:56:29,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:29,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:29,983 [api1.py:131] throughput: 0.017361800700790336
(INFO) 2023-04-09 19:56:30,013 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:56:30,014 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,033 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 19:56:30,068 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:30,069 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,117 [api1.py:131] throughput: 0.011575337340580706
(INFO) 2023-04-09 19:56:30,147 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:56:30,156 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:56:30,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,164 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,187 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:30,188 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,306 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:56:30,342 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 64}
(INFO) 2023-04-09 19:56:30,343 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,357 [api1.py:131] throughput: 0.0069455907076916915
(INFO) 2023-04-09 19:56:30,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,360 [api1.py:131] throughput: 0.004341130186648638
(INFO) 2023-04-09 19:56:30,395 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:30,395 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,402 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,402 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:30,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,439 [api1.py:131] throughput: 0.0031572304355389517
(INFO) 2023-04-09 19:56:30,460 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:56:30,489 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:30,490 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,503 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:56:30,503 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,513 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,530 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 19:56:30,552 [api1.py:131] throughput: 0.002894136973135579
(INFO) 2023-04-09 19:56:30,562 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:56:30,562 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,579 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:56:30,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,592 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 68}
(INFO) 2023-04-09 19:56:30,592 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,602 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 19:56:30,605 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,616 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:56:30,617 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,627 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,635 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:30,635 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,693 [api1.py:131] throughput: 0.0021706215842952387
(INFO) 2023-04-09 19:56:30,750 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:30,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,756 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,768 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 19:56:30,788 [api1.py:131] throughput: 0.00496125469960527
(INFO) 2023-04-09 19:56:30,801 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:30,802 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,808 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,822 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:56:30,827 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:30,827 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,835 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,842 [api1.py:131] throughput: 0.004961254123495958
(INFO) 2023-04-09 19:56:30,859 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:56:30,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,883 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:30,884 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,928 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:56:30,937 [api1.py:131] throughput: 0.0115753389908042
(INFO) 2023-04-09 19:56:30,945 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:56:30,957 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:56:30,970 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:30,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:30,975 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:56:30,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:30,994 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:56:30,995 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:31,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:31,061 [api1.py:131] throughput: 0.0038588048060630635
(INFO) 2023-04-09 19:56:31,104 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:31,105 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:31,113 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:31,228 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:56:31,272 [api1.py:131] throughput: 0.011575342975492143
(INFO) 2023-04-09 19:56:31,338 [api1.py:131] throughput: 0.008681806643613585
(INFO) 2023-04-09 19:56:31,352 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 19:56:31,572 [api1.py:131] throughput: 0.0057880734999380635
(INFO) 2023-04-09 19:56:31,847 [api1.py:131] throughput: 0.0031572304319762022
(INFO) 2023-04-09 19:56:32,024 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:32,025 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,033 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,052 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:56:32,052 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,054 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:56:32,055 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,068 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,071 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:32,072 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,075 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:32,076 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,081 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,083 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,166 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 19:56:32,169 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:56:32,188 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 19:56:32,200 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 50}
(INFO) 2023-04-09 19:56:32,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,203 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:56:32,204 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,216 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,221 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:32,222 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,229 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,300 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 19:56:32,351 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 19:56:32,554 [api1.py:131] throughput: 0.005788072314328548
(INFO) 2023-04-09 19:56:32,593 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:56:32,593 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,593 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:56:32,593 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:32,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,605 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,606 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,608 [api1.py:131] throughput: 0.004961254669087043
(INFO) 2023-04-09 19:56:32,644 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:56:32,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,655 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,682 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:56:32,714 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:32,714 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,729 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,796 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 19:56:32,832 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:56:32,833 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,869 [api1.py:131] throughput: 0.0069455903689555724
(INFO) 2023-04-09 19:56:32,908 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:56:32,909 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,911 [api1.py:131] throughput: 0.005788072109178651
(INFO) 2023-04-09 19:56:32,920 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:32,942 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:56:32,943 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:32,949 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,025 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:56:33,214 [api1.py:131] throughput: 0.004961254886725603
(INFO) 2023-04-09 19:56:33,249 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:56:33,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,270 [api1.py:131] throughput: 0.008681806901898306
(INFO) 2023-04-09 19:56:33,337 [api1.py:131] throughput: 0.004341129932708369
(INFO) 2023-04-09 19:56:33,409 [api1.py:131] throughput: 0.006945591591062511
(INFO) 2023-04-09 19:56:33,606 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:33,607 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,613 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:56:33,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,613 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:33,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,626 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,636 [api1.py:131] throughput: 0.006945590408690013
(INFO) 2023-04-09 19:56:33,672 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:56:33,673 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,693 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,710 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 19:56:33,747 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:56:33,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,752 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:56:33,759 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,787 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:56:33,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:33,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:33,951 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:56:33,988 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:56:33,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:34,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:34,011 [api1.py:131] throughput: 0.011575337662575496
(INFO) 2023-04-09 19:56:34,098 [api1.py:131] throughput: 0.00434112994820707
(INFO) 2023-04-09 19:56:34,133 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:56:34,134 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:34,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:34,588 [api1.py:131] throughput: 0.006945591059947248
(INFO) 2023-04-09 19:56:34,627 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:34,628 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:34,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:34,684 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:56:35,061 [api1.py:131] throughput: 0.002894136985023107
(INFO) 2023-04-09 19:56:35,429 [api1.py:131] throughput: 0.002671518230676864
(INFO) 2023-04-09 19:56:35,458 [api1.py:131] throughput: 0.001389210799150999
(INFO) 2023-04-09 19:56:35,892 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 51}
(INFO) 2023-04-09 19:56:35,893 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:35,905 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:36,291 [api1.py:131] throughput: 0.011575341204519385
(INFO) 2023-04-09 19:56:36,329 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:56:36,330 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:36,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:36,678 [api1.py:131] throughput: 0.004341129621918651
(INFO) 2023-04-09 19:56:36,716 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:56:36,717 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:36,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:36,971 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 19:56:37,005 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:37,006 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:37,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:37,274 [api1.py:131] throughput: 0.011575339507337669
(INFO) 2023-04-09 19:56:37,314 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:56:37,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:37,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:37,473 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 19:56:37,512 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:56:37,513 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:37,528 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:38,116 [api1.py:131] throughput: 0.003858804647860758
(INFO) 2023-04-09 19:56:38,158 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:56:38,158 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:38,166 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:38,278 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:56:38,314 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:56:38,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:38,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:38,596 [api1.py:131] throughput: 0.0049612537981637
(INFO) 2023-04-09 19:56:38,640 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:56:38,641 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:38,647 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:38,700 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:56:38,741 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:56:38,742 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:56:38,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:56:39,088 [api1.py:131] throughput: 0.011575340578199882
