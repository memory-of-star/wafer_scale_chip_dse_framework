(DEBUG) 2023-04-10 17:20:20,536 [logger.py:40] logger init.
(INFO) 2023-04-10 17:20:20,536 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-17-20-20-536237.log
(INFO) 2023-04-10 17:20:24,134 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,134 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 488, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:24,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,137 [api2.py:131] throughput: 0.4088694169196066
(INFO) 2023-04-10 17:20:24,137 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,137 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 257, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,139 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,139 [api2.py:131] throughput: 0.7734837283379012
(INFO) 2023-04-10 17:20:24,140 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,140 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,141 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,141 [api2.py:131] throughput: 1.1957601658210555
(INFO) 2023-04-10 17:20:24,142 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,142 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,143 [api2.py:131] throughput: 2.871313692628856
(INFO) 2023-04-10 17:20:24,144 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,144 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 222, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:20:24,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,145 [api2.py:131] throughput: 15.269395839252624
(INFO) 2023-04-10 17:20:24,146 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,146 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:20:24,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,147 [api2.py:131] throughput: 151.4980190996666
(INFO) 2023-04-10 17:20:24,148 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,148 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 301, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:24,149 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,149 [api2.py:131] throughput: 0.6183607475771181
(INFO) 2023-04-10 17:20:24,150 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,150 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:24,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,151 [api2.py:131] throughput: 0.8123987225685764
(INFO) 2023-04-10 17:20:24,152 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,152 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 200, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:20:24,153 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,153 [api2.py:131] throughput: 0.7016870140833509
(INFO) 2023-04-10 17:20:24,154 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,154 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,155 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,155 [api2.py:131] throughput: 2.2659477641891854
(INFO) 2023-04-10 17:20:24,156 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,156 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 510, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,157 [api2.py:131] throughput: 0.304976043725363
(INFO) 2023-04-10 17:20:24,158 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,158 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,159 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,160 [api2.py:131] throughput: 5.089762232368086
(INFO) 2023-04-10 17:20:24,160 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,160 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:24,161 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,161 [api2.py:131] throughput: 1.2025492694358437
(INFO) 2023-04-10 17:20:24,162 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,162 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 146, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:20:24,163 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,163 [api2.py:131] throughput: 0.9588656136153206
(INFO) 2023-04-10 17:20:24,164 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,164 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 472, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,165 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,165 [api2.py:131] throughput: 0.4710388201552104
(INFO) 2023-04-10 17:20:24,166 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,166 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 211, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:24,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,167 [api2.py:131] throughput: 0.7327757219278944
(INFO) 2023-04-10 17:20:24,167 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,167 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 112, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:20:24,169 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,169 [api2.py:131] throughput: 47.778900348350234
(INFO) 2023-04-10 17:20:24,169 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,170 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:24,171 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,171 [api2.py:131] throughput: 1.545848124008184
(INFO) 2023-04-10 17:20:24,172 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,172 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 300, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:24,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,173 [api2.py:131] throughput: 0.7748921647665238
(INFO) 2023-04-10 17:20:24,174 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:24,174 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:24,175 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,176 [api2.py:131] throughput: 0.5315149989212542
(INFO) 2023-04-10 17:20:24,180 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:24,180 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 8, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 55}
(INFO) 2023-04-10 17:20:24,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:24,890 [api1.py:131] throughput: 0.7942569590877318
(INFO) 2023-04-10 17:20:24,891 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:24,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 137}
(INFO) 2023-04-10 17:20:24,991 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:25,545 [api1.py:131] throughput: 7.23529695606839
(INFO) 2023-04-10 17:20:25,546 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:25,546 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-10 17:20:25,553 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:26,229 [api1.py:131] throughput: 0.30803181649888456
(INFO) 2023-04-10 17:20:26,246 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:26,246 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 17:20:26,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:26,763 [api1.py:131] throughput: 0.8873161114156634
(INFO) 2023-04-10 17:20:26,764 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:26,764 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 368}
(INFO) 2023-04-10 17:20:26,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:27,233 [api1.py:131] throughput: 21.59097808060645
(INFO) 2023-04-10 17:20:27,234 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:27,234 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:27,241 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:27,910 [api1.py:131] throughput: 0.3269972727336217
(INFO) 2023-04-10 17:20:27,912 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:27,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 76, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 105}
(INFO) 2023-04-10 17:20:27,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:28,727 [api1.py:131] throughput: 0.5802538176992887
(INFO) 2023-04-10 17:20:28,728 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:28,728 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 835}
(INFO) 2023-04-10 17:20:28,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:29,183 [api1.py:131] throughput: 21.76118231353484
(INFO) 2023-04-10 17:20:29,184 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:29,184 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:20:29,190 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:29,867 [api1.py:131] throughput: 0.65379627850706
(INFO) 2023-04-10 17:20:29,868 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:29,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 84}
(INFO) 2023-04-10 17:20:29,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:30,237 [api1.py:131] throughput: 4.261484153349248
(INFO) 2023-04-10 17:20:30,238 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:30,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 173}
(INFO) 2023-04-10 17:20:30,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:30,683 [api1.py:131] throughput: 1.3384896323793072
(INFO) 2023-04-10 17:20:30,685 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:30,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 127, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:20:30,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:31,476 [api1.py:131] throughput: 0.24130677195583788
(INFO) 2023-04-10 17:20:31,478 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:31,478 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:31,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:32,148 [api1.py:131] throughput: 0.23623918323903056
(INFO) 2023-04-10 17:20:32,150 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:32,150 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 268}
(INFO) 2023-04-10 17:20:32,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:32,786 [api1.py:131] throughput: 12.290572782418181
(INFO) 2023-04-10 17:20:32,787 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:32,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:20:32,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:33,464 [api1.py:131] throughput: 1.5397864535583043
(INFO) 2023-04-10 17:20:33,465 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:33,466 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:33,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:34,138 [api1.py:131] throughput: 0.1103738128972744
(INFO) 2023-04-10 17:20:34,139 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:34,139 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 17:20:34,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:34,793 [api1.py:131] throughput: 0.6572450079496992
(INFO) 2023-04-10 17:20:34,794 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:34,795 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 111, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 252}
(INFO) 2023-04-10 17:20:34,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:35,457 [api1.py:131] throughput: 1.8021873312823116
(INFO) 2023-04-10 17:20:35,458 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:35,458 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:20:35,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:36,023 [api1.py:131] throughput: 3.3067176648845376
(INFO) 2023-04-10 17:20:36,025 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:20:36,025 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:36,140 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:36,816 [api1.py:131] throughput: 0.10417354378733332
(INFO) 2023-04-10 17:20:36,858 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:36,859 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 219, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:36,861 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:36,916 [api1.py:131] throughput: 0.267763122809312
(INFO) 2023-04-10 17:20:36,917 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:36,917 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 114, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:36,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:36,970 [api1.py:131] throughput: 0.21203737177336796
(INFO) 2023-04-10 17:20:36,971 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:36,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 470, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:36,974 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,035 [api1.py:131] throughput: 0.043887088377591206
(INFO) 2023-04-10 17:20:37,037 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,037 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 250, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,094 [api1.py:131] throughput: 0.07475175814221506
(INFO) 2023-04-10 17:20:37,095 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,095 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 431, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:37,096 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,146 [api1.py:131] throughput: 0.11922707100512703
(INFO) 2023-04-10 17:20:37,147 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,149 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,202 [api1.py:131] throughput: 0.1876572838264955
(INFO) 2023-04-10 17:20:37,203 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,203 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 150, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,205 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,245 [api1.py:131] throughput: 0.17127803029366298
(INFO) 2023-04-10 17:20:37,245 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,246 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 173, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,279 [api1.py:131] throughput: 0.08492305672131889
(INFO) 2023-04-10 17:20:37,280 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,280 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 488, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:37,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,313 [api1.py:131] throughput: 0.08522011299821561
(INFO) 2023-04-10 17:20:37,314 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,314 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 352, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:20:37,314 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,340 [api1.py:131] throughput: 0.4868321939746346
(INFO) 2023-04-10 17:20:37,341 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,341 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 211, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,373 [api1.py:131] throughput: 0.12888660974286092
(INFO) 2023-04-10 17:20:37,374 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:37,374 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,407 [api1.py:131] throughput: 0.3731847339189265
(INFO) 2023-04-10 17:20:37,408 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,408 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,409 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,447 [api1.py:131] throughput: 0.10933197568093005
(INFO) 2023-04-10 17:20:37,447 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,448 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:37,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,472 [api1.py:131] throughput: 0.69415567568336
(INFO) 2023-04-10 17:20:37,473 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,473 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 160, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:37,474 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,503 [api1.py:131] throughput: 0.4684629259189967
(INFO) 2023-04-10 17:20:37,504 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,504 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,505 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,536 [api1.py:131] throughput: 0.32990357483302546
(INFO) 2023-04-10 17:20:37,537 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,537 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 134, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:20:37,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,565 [api1.py:131] throughput: 0.3228222545005451
(INFO) 2023-04-10 17:20:37,565 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,566 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,599 [api1.py:131] throughput: 0.1812916570519241
(INFO) 2023-04-10 17:20:37,600 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,600 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 237, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,633 [api1.py:131] throughput: 0.15743182179439189
(INFO) 2023-04-10 17:20:37,633 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:20:37,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,667 [api1.py:131] throughput: 0.3571177160556469
(INFO) 2023-04-10 17:20:37,673 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,673 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:37,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,675 [api2.py:131] throughput: 0.9623240089534679
(INFO) 2023-04-10 17:20:37,675 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,675 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 422, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:37,676 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,677 [api2.py:131] throughput: 0.3221727405738068
(INFO) 2023-04-10 17:20:37,677 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,678 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:20:37,679 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,679 [api2.py:131] throughput: 1.9988808679503134
(INFO) 2023-04-10 17:20:37,680 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,680 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,682 [api2.py:131] throughput: 0.4202095896245788
(INFO) 2023-04-10 17:20:37,682 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,682 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,683 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,684 [api2.py:131] throughput: 2.0485144867797986
(INFO) 2023-04-10 17:20:37,684 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,685 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,685 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,686 [api2.py:131] throughput: 5.379584040085665
(INFO) 2023-04-10 17:20:37,686 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,687 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 115, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:37,687 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,688 [api2.py:131] throughput: 1.0396781524878072
(INFO) 2023-04-10 17:20:37,688 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,688 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,690 [api2.py:131] throughput: 0.5252342561589965
(INFO) 2023-04-10 17:20:37,690 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,691 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,691 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,692 [api2.py:131] throughput: 0.9651975198206874
(INFO) 2023-04-10 17:20:37,692 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,692 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 360, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:37,693 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,694 [api2.py:131] throughput: 0.47610829671145316
(INFO) 2023-04-10 17:20:37,694 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,694 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:37,696 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,696 [api2.py:131] throughput: 0.3325021433354161
(INFO) 2023-04-10 17:20:37,697 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,697 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 200, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:37,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,698 [api2.py:131] throughput: 0.6712707231690365
(INFO) 2023-04-10 17:20:37,699 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,699 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,700 [api2.py:131] throughput: 3.9081040045397546
(INFO) 2023-04-10 17:20:37,700 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,701 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 237, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:37,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,702 [api2.py:131] throughput: 0.5424573445567166
(INFO) 2023-04-10 17:20:37,702 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,702 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 118, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,703 [api2.py:131] throughput: 1.4092683661548533
(INFO) 2023-04-10 17:20:37,704 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,704 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 378, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:20:37,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,706 [api2.py:131] throughput: 0.5920620390485927
(INFO) 2023-04-10 17:20:37,706 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,706 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,707 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,707 [api2.py:131] throughput: 1.595236662850136
(INFO) 2023-04-10 17:20:37,708 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,708 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 286, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:20:37,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,709 [api2.py:131] throughput: 7.893458923154235
(INFO) 2023-04-10 17:20:37,710 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,710 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:37,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,712 [api2.py:131] throughput: 1.3478141275556454
(INFO) 2023-04-10 17:20:37,712 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:20:37,713 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 150, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:37,713 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,714 [api2.py:131] throughput: 8.056477769056524
(INFO) 2023-04-10 17:20:37,719 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,719 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 380, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:20:37,720 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,720 [api2.py:131] throughput: 0.29074060634586385
(INFO) 2023-04-10 17:20:37,721 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,721 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 248, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:20:37,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,723 [api2.py:131] throughput: 0.5393708854179574
(INFO) 2023-04-10 17:20:37,724 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,724 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,726 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,726 [api2.py:131] throughput: 1.9612849954453733
(INFO) 2023-04-10 17:20:37,727 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,727 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:37,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,730 [api2.py:131] throughput: 0.7024626071822141
(INFO) 2023-04-10 17:20:37,731 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,731 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:20:37,733 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,733 [api2.py:131] throughput: 0.4177359816627137
(INFO) 2023-04-10 17:20:37,734 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,734 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 10, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:37,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,735 [api2.py:131] throughput: 4.746564189794721
(INFO) 2023-04-10 17:20:37,736 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,736 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 6, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:20:37,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,739 [api2.py:131] throughput: 1.7941416349286585
(INFO) 2023-04-10 17:20:37,740 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,740 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,741 [api2.py:131] throughput: 1.5800777966598338
(INFO) 2023-04-10 17:20:37,742 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,742 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:20:37,744 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,744 [api2.py:131] throughput: 2.2744454570345543
(INFO) 2023-04-10 17:20:37,745 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,745 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,747 [api2.py:131] throughput: 2.1395682758874974
(INFO) 2023-04-10 17:20:37,747 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,747 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 10, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:20:37,750 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,750 [api2.py:131] throughput: 1.2768222807590994
(INFO) 2023-04-10 17:20:37,751 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,751 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 9, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:37,752 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,753 [api2.py:131] throughput: 1.731221327390172
(INFO) 2023-04-10 17:20:37,753 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,754 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 17:20:37,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,755 [api2.py:131] throughput: 0.5058502239504453
(INFO) 2023-04-10 17:20:37,756 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,756 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:20:37,758 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,758 [api2.py:131] throughput: 0.9763066215204598
(INFO) 2023-04-10 17:20:37,759 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,759 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:37,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,761 [api2.py:131] throughput: 4.783237040851741
(INFO) 2023-04-10 17:20:37,762 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,762 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 164, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:20:37,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,764 [api2.py:131] throughput: 0.6029709602137576
(INFO) 2023-04-10 17:20:37,764 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,764 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:20:37,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,766 [api2.py:131] throughput: 1.9963399349769109
(INFO) 2023-04-10 17:20:37,767 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,767 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:20:37,768 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,769 [api2.py:131] throughput: 1.5219911374974138
(INFO) 2023-04-10 17:20:37,769 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,769 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 128}
(INFO) 2023-04-10 17:20:37,772 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,772 [api2.py:131] throughput: 157.20015472342007
(INFO) 2023-04-10 17:20:37,773 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:37,773 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 121}
(INFO) 2023-04-10 17:20:37,774 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,774 [api2.py:131] throughput: 30.4751161637636
(INFO) 2023-04-10 17:20:37,780 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:37,780 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 173, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:20:37,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,838 [api1.py:131] throughput: 0.7627412416852845
(INFO) 2023-04-10 17:20:37,839 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:37,839 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:20:37,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,890 [api1.py:131] throughput: 1.645985677136558
(INFO) 2023-04-10 17:20:37,891 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:37,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 9, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:20:37,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:37,943 [api1.py:131] throughput: 2.911044145862551
(INFO) 2023-04-10 17:20:37,944 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:37,944 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:20:37,946 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,007 [api1.py:131] throughput: 0.8510228296704417
(INFO) 2023-04-10 17:20:38,008 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,008 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:38,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,072 [api1.py:131] throughput: 0.6892156169198631
(INFO) 2023-04-10 17:20:38,073 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,073 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:38,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,137 [api1.py:131] throughput: 0.38191360020347176
(INFO) 2023-04-10 17:20:38,138 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:38,139 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,201 [api1.py:131] throughput: 0.5110030533714155
(INFO) 2023-04-10 17:20:38,201 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 9, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:20:38,203 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,394 [api1.py:131] throughput: 0.3332607555707422
(INFO) 2023-04-10 17:20:38,395 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,395 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 11, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:38,397 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,450 [api1.py:131] throughput: 2.1388196495094403
(INFO) 2023-04-10 17:20:38,451 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,451 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:38,452 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,506 [api1.py:131] throughput: 1.1789732790971452
(INFO) 2023-04-10 17:20:38,507 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,507 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:38,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,568 [api1.py:131] throughput: 0.6052016918357273
(INFO) 2023-04-10 17:20:38,568 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,569 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:38,570 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,636 [api1.py:131] throughput: 0.8634993267549718
(INFO) 2023-04-10 17:20:38,637 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,637 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:38,638 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,704 [api1.py:131] throughput: 0.9173120848595162
(INFO) 2023-04-10 17:20:38,705 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 10, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:38,706 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,782 [api1.py:131] throughput: 0.14193509795746231
(INFO) 2023-04-10 17:20:38,783 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:38,784 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,848 [api1.py:131] throughput: 0.32534946028967665
(INFO) 2023-04-10 17:20:38,849 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,849 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 90, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:38,851 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,905 [api1.py:131] throughput: 0.3286567211041224
(INFO) 2023-04-10 17:20:38,906 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:38,908 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:38,968 [api1.py:131] throughput: 0.31515778716764675
(INFO) 2023-04-10 17:20:38,969 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:38,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 63}
(INFO) 2023-04-10 17:20:38,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:39,019 [api1.py:131] throughput: 10.787929545112021
(INFO) 2023-04-10 17:20:39,020 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:39,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 10, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:39,021 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:39,084 [api1.py:131] throughput: 0.404546831946508
(INFO) 2023-04-10 17:20:39,084 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-10 17:20:39,084 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 7, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:39,086 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:39,139 [api1.py:131] throughput: 1.7948131134164451
(INFO) 2023-04-10 17:20:44,371 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,371 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 7, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:20:44,372 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,372 [api2.py:131] throughput: 0.9644783305444191
(INFO) 2023-04-10 17:20:44,373 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,373 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 8, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:20:44,374 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,375 [api2.py:131] throughput: 2.516007809794978
(INFO) 2023-04-10 17:20:44,375 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,375 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:44,377 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,377 [api2.py:131] throughput: 1.7659593125405777
(INFO) 2023-04-10 17:20:44,378 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,378 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 10, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:44,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,379 [api2.py:131] throughput: 0.3621043488981596
(INFO) 2023-04-10 17:20:44,380 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,380 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:44,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,381 [api2.py:131] throughput: 0.3655600575104094
(INFO) 2023-04-10 17:20:44,382 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,382 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 10, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:20:44,383 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,383 [api2.py:131] throughput: 1.4422645887017362
(INFO) 2023-04-10 17:20:44,384 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,384 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 17:20:44,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,386 [api2.py:131] throughput: 1.766626652268295
(INFO) 2023-04-10 17:20:44,386 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,387 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 99}
(INFO) 2023-04-10 17:20:44,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,388 [api2.py:131] throughput: 70.03602086033264
(INFO) 2023-04-10 17:20:44,388 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,389 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:20:44,390 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,390 [api2.py:131] throughput: 8.879629588028855
(INFO) 2023-04-10 17:20:44,390 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,391 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 150, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:20:44,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,392 [api2.py:131] throughput: 0.9385453938181347
(INFO) 2023-04-10 17:20:44,393 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,393 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 51, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:44,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,394 [api2.py:131] throughput: 1.5443708327307932
(INFO) 2023-04-10 17:20:44,395 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,395 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 159, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:44,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,397 [api2.py:131] throughput: 0.3029951986445214
(INFO) 2023-04-10 17:20:44,397 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,397 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 72}
(INFO) 2023-04-10 17:20:44,398 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,399 [api2.py:131] throughput: 18.65917953258177
(INFO) 2023-04-10 17:20:44,399 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,399 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:20:44,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,401 [api2.py:131] throughput: 1.2359933389334798
(INFO) 2023-04-10 17:20:44,401 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,401 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 10, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:44,403 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,403 [api2.py:131] throughput: 3.3024679259377425
(INFO) 2023-04-10 17:20:44,403 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,404 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 9, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:44,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,405 [api2.py:131] throughput: 1.0371739262508155
(INFO) 2023-04-10 17:20:44,406 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,406 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:20:44,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,407 [api2.py:131] throughput: 12.611959950702948
(INFO) 2023-04-10 17:20:44,408 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,408 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:20:44,409 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,410 [api2.py:131] throughput: 0.6102814692865667
(INFO) 2023-04-10 17:20:44,410 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,410 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:20:44,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,412 [api2.py:131] throughput: 0.9562862989229853
(INFO) 2023-04-10 17:20:44,412 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:20:44,412 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 138, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:20:44,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:44,414 [api2.py:131] throughput: 1.0689322517015187
(INFO) 2023-04-10 17:20:49,247 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,247 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 312, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:49,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,249 [api2.py:131] throughput: 0.6334092189718655
(INFO) 2023-04-10 17:20:49,249 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,250 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:49,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,251 [api2.py:131] throughput: 11.368120777243329
(INFO) 2023-04-10 17:20:49,252 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,252 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 425, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:20:49,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,253 [api2.py:131] throughput: 0.3358260935890345
(INFO) 2023-04-10 17:20:49,254 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,254 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:20:49,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,256 [api2.py:131] throughput: 0.6765256936930861
(INFO) 2023-04-10 17:20:49,256 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,256 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 446, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:49,258 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,259 [api2.py:131] throughput: 0.4818271512180764
(INFO) 2023-04-10 17:20:49,259 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,259 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 88, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 17:20:49,261 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,261 [api2.py:131] throughput: 1.5941075902146369
(INFO) 2023-04-10 17:20:49,262 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,262 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:20:49,263 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,263 [api2.py:131] throughput: 1.7112095891695163
(INFO) 2023-04-10 17:20:49,264 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,264 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 162, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:49,265 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,265 [api2.py:131] throughput: 0.8439260227141134
(INFO) 2023-04-10 17:20:49,266 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,266 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 141, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:49,268 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,268 [api2.py:131] throughput: 0.6063766068339748
(INFO) 2023-04-10 17:20:49,269 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,269 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:20:49,270 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,271 [api2.py:131] throughput: 1.542749108809379
(INFO) 2023-04-10 17:20:49,271 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,271 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:49,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,273 [api2.py:131] throughput: 2.4189073876605227
(INFO) 2023-04-10 17:20:49,274 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,274 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:49,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,275 [api2.py:131] throughput: 1.3102408931339857
(INFO) 2023-04-10 17:20:49,276 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,276 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:20:49,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,277 [api2.py:131] throughput: 104.19593414683592
(INFO) 2023-04-10 17:20:49,278 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,278 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 254, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:49,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,280 [api2.py:131] throughput: 0.7614121673908001
(INFO) 2023-04-10 17:20:49,281 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,281 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 175, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:20:49,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,282 [api2.py:131] throughput: 0.7134094941457777
(INFO) 2023-04-10 17:20:49,283 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,283 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:49,284 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,285 [api2.py:131] throughput: 2.1853000147691315
(INFO) 2023-04-10 17:20:49,285 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,285 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 356, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:49,286 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,287 [api2.py:131] throughput: 0.6988661988391475
(INFO) 2023-04-10 17:20:49,287 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,287 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 201, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:20:49,289 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,290 [api2.py:131] throughput: 1.1105276563797173
(INFO) 2023-04-10 17:20:49,290 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,290 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 262, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:49,291 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,292 [api2.py:131] throughput: 0.6806342840694377
(INFO) 2023-04-10 17:20:49,292 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:49,292 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 243, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:49,294 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:49,294 [api2.py:131] throughput: 0.6482244830632736
(INFO) 2023-04-10 17:20:54,193 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,193 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:20:54,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,315 [api1.py:131] throughput: 0.24036602510643973
(INFO) 2023-04-10 17:20:54,316 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,316 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 169, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:54,319 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,438 [api1.py:131] throughput: 0.13565374035241018
(INFO) 2023-04-10 17:20:54,439 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,440 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:20:54,442 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,566 [api1.py:131] throughput: 0.2870807581010386
(INFO) 2023-04-10 17:20:54,567 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:20:54,569 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,669 [api1.py:131] throughput: 3.2560203174345648
(INFO) 2023-04-10 17:20:54,671 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,671 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:20:54,673 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,763 [api1.py:131] throughput: 0.5169981582755402
(INFO) 2023-04-10 17:20:54,764 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,764 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 222, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:54,767 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,894 [api1.py:131] throughput: 0.08792468735359835
(INFO) 2023-04-10 17:20:54,895 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,896 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:20:54,898 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:54,993 [api1.py:131] throughput: 0.794509699496448
(INFO) 2023-04-10 17:20:54,994 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:54,994 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:54,996 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,097 [api1.py:131] throughput: 0.5401584302479538
(INFO) 2023-04-10 17:20:55,098 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,098 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:55,100 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,225 [api1.py:131] throughput: 0.12248876090165652
(INFO) 2023-04-10 17:20:55,226 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,226 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 126, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:20:55,228 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,353 [api1.py:131] throughput: 0.7163585990008225
(INFO) 2023-04-10 17:20:55,354 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:20:55,357 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,457 [api1.py:131] throughput: 1.1161715651061268
(INFO) 2023-04-10 17:20:55,458 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,458 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 214, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:55,460 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,576 [api1.py:131] throughput: 0.08558828248439529
(INFO) 2023-04-10 17:20:55,577 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,577 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:55,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,694 [api1.py:131] throughput: 0.07540027212853362
(INFO) 2023-04-10 17:20:55,695 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:20:55,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,791 [api1.py:131] throughput: 0.19178297220740817
(INFO) 2023-04-10 17:20:55,791 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,792 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 76, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 17:20:55,793 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:55,882 [api1.py:131] throughput: 2.5567481919985844
(INFO) 2023-04-10 17:20:55,883 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:55,883 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 230, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:55,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:56,007 [api1.py:131] throughput: 0.044119608170216165
(INFO) 2023-04-10 17:20:56,008 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:56,008 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:20:56,010 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:56,151 [api1.py:131] throughput: 0.03994167212174048
(INFO) 2023-04-10 17:20:56,152 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:56,152 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 246, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 17:20:56,154 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:56,247 [api1.py:131] throughput: 0.7252117562632024
(INFO) 2023-04-10 17:20:56,248 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:56,248 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 117, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 100}
(INFO) 2023-04-10 17:20:56,250 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:56,353 [api1.py:131] throughput: 2.530139835363599
(INFO) 2023-04-10 17:20:56,354 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 15}
(INFO) 2023-04-10 17:20:56,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 114, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:20:56,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:20:56,621 [api1.py:131] throughput: 0.10851194141934684
(INFO) 2023-04-10 17:21:01,227 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,227 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:01,231 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,231 [api2.py:131] throughput: 1.4235157309534268
(INFO) 2023-04-10 17:21:01,232 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,232 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:21:01,236 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,236 [api2.py:131] throughput: 1.2478738825777123
(INFO) 2023-04-10 17:21:01,237 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,237 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 223, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:21:01,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,241 [api2.py:131] throughput: 0.28948388442499956
(INFO) 2023-04-10 17:21:01,241 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,241 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 11, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:21:01,245 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,245 [api2.py:131] throughput: 1.3919093349662506
(INFO) 2023-04-10 17:21:01,246 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,246 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 11, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:01,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,249 [api2.py:131] throughput: 3.0470437987060848
(INFO) 2023-04-10 17:21:01,250 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,250 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 288}
(INFO) 2023-04-10 17:21:01,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,254 [api2.py:131] throughput: 47.920070255929026
(INFO) 2023-04-10 17:21:01,254 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,254 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 10, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 100}
(INFO) 2023-04-10 17:21:01,258 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,258 [api2.py:131] throughput: 3.624020211459973
(INFO) 2023-04-10 17:21:01,259 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,259 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:21:01,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,263 [api2.py:131] throughput: 0.3678738928825388
(INFO) 2023-04-10 17:21:01,263 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,263 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 50}
(INFO) 2023-04-10 17:21:01,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,267 [api2.py:131] throughput: 0.7437028004891669
(INFO) 2023-04-10 17:21:01,267 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,267 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 241}
(INFO) 2023-04-10 17:21:01,271 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,272 [api2.py:131] throughput: 93.14165675230922
(INFO) 2023-04-10 17:21:01,272 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,272 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 57}
(INFO) 2023-04-10 17:21:01,276 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,276 [api2.py:131] throughput: 0.47160759618024023
(INFO) 2023-04-10 17:21:01,277 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,277 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:21:01,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,281 [api2.py:131] throughput: 1.166547705249098
(INFO) 2023-04-10 17:21:01,282 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,282 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:01,285 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,285 [api2.py:131] throughput: 3.821832581314297
(INFO) 2023-04-10 17:21:01,286 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,286 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 156}
(INFO) 2023-04-10 17:21:01,289 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,289 [api2.py:131] throughput: 1.5886994052868082
(INFO) 2023-04-10 17:21:01,290 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,290 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:01,294 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,294 [api2.py:131] throughput: 2.1356327883427038
(INFO) 2023-04-10 17:21:01,295 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,295 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 381, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 102}
(INFO) 2023-04-10 17:21:01,298 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,298 [api2.py:131] throughput: 0.7126891941939603
(INFO) 2023-04-10 17:21:01,299 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,299 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 427, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 17:21:01,302 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,303 [api2.py:131] throughput: 0.3973063453994258
(INFO) 2023-04-10 17:21:01,303 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,303 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 253, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:21:01,306 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,307 [api2.py:131] throughput: 0.6010654077356593
(INFO) 2023-04-10 17:21:01,307 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,307 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 6, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 96}
(INFO) 2023-04-10 17:21:01,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,311 [api2.py:131] throughput: 20.99395513599149
(INFO) 2023-04-10 17:21:01,312 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 47}
(INFO) 2023-04-10 17:21:01,312 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 190}
(INFO) 2023-04-10 17:21:01,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:01,316 [api2.py:131] throughput: 73.81846544461246
(INFO) 2023-04-10 17:21:06,080 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,080 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:06,082 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,082 [api2.py:131] throughput: 2.3715690956118367
(INFO) 2023-04-10 17:21:06,082 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,083 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:21:06,084 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,084 [api2.py:131] throughput: 1.5214420564251032
(INFO) 2023-04-10 17:21:06,085 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,085 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:21:06,086 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,086 [api2.py:131] throughput: 2.1543278200389944
(INFO) 2023-04-10 17:21:06,087 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,087 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 17:21:06,088 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,088 [api2.py:131] throughput: 1.196759651200237
(INFO) 2023-04-10 17:21:06,089 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,089 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 281, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:06,090 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,090 [api2.py:131] throughput: 0.38688426169585877
(INFO) 2023-04-10 17:21:06,091 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,091 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:21:06,093 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,093 [api2.py:131] throughput: 0.7541289653627052
(INFO) 2023-04-10 17:21:06,094 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,094 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 7, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:06,095 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,095 [api2.py:131] throughput: 5.366275333047919
(INFO) 2023-04-10 17:21:06,095 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,095 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 257, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:06,096 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,097 [api2.py:131] throughput: 0.6442586293471423
(INFO) 2023-04-10 17:21:06,097 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,097 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:21:06,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,099 [api2.py:131] throughput: 2.4710163181992777
(INFO) 2023-04-10 17:21:06,099 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,099 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:06,100 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,101 [api2.py:131] throughput: 1.2874942676073409
(INFO) 2023-04-10 17:21:06,101 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,101 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:21:06,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,103 [api2.py:131] throughput: 0.7244165293110529
(INFO) 2023-04-10 17:21:06,103 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,103 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:21:06,105 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,105 [api2.py:131] throughput: 1.4914647197713171
(INFO) 2023-04-10 17:21:06,106 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,106 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:06,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,107 [api2.py:131] throughput: 0.6275765768289627
(INFO) 2023-04-10 17:21:06,107 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,107 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:06,109 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,109 [api2.py:131] throughput: 0.5871983313419431
(INFO) 2023-04-10 17:21:06,109 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,109 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:06,110 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,111 [api2.py:131] throughput: 2.4834935868492605
(INFO) 2023-04-10 17:21:06,111 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,111 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 396, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:21:06,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,112 [api2.py:131] throughput: 0.4406429663629074
(INFO) 2023-04-10 17:21:06,113 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,113 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 97, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:21:06,114 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,114 [api2.py:131] throughput: 0.6309997262356557
(INFO) 2023-04-10 17:21:06,115 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,115 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:06,116 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,117 [api2.py:131] throughput: 2.688977217641781
(INFO) 2023-04-10 17:21:06,117 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,117 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:06,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,118 [api2.py:131] throughput: 0.3369003705276661
(INFO) 2023-04-10 17:21:06,119 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-10 17:21:06,119 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 7, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:06,120 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:06,121 [api2.py:131] throughput: 2.79346305687951
(INFO) 2023-04-10 17:21:11,035 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 190, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:21:11,037 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,085 [api1.py:131] throughput: 3.333070828517399
(INFO) 2023-04-10 17:21:11,086 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,086 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:11,087 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,149 [api1.py:131] throughput: 0.24989703066350635
(INFO) 2023-04-10 17:21:11,151 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,151 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:21:11,152 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,210 [api1.py:131] throughput: 1.2858594181383143
(INFO) 2023-04-10 17:21:11,211 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:11,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,276 [api1.py:131] throughput: 0.7767751718669715
(INFO) 2023-04-10 17:21:11,277 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 9, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:21:11,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,321 [api1.py:131] throughput: 8.952747156152492
(INFO) 2023-04-10 17:21:11,322 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:11,323 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,366 [api1.py:131] throughput: 1.7911568014503985
(INFO) 2023-04-10 17:21:11,367 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,367 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 9, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:21:11,368 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,410 [api1.py:131] throughput: 10.139601626197146
(INFO) 2023-04-10 17:21:11,411 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,411 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:21:11,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,459 [api1.py:131] throughput: 5.610477790650959
(INFO) 2023-04-10 17:21:11,460 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,460 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:21:11,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,502 [api1.py:131] throughput: 7.388890557908873
(INFO) 2023-04-10 17:21:11,502 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,502 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 9, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:21:11,504 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,545 [api1.py:131] throughput: 7.247136970716366
(INFO) 2023-04-10 17:21:11,546 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,565 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 157, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:11,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,633 [api1.py:131] throughput: 0.22629988502696427
(INFO) 2023-04-10 17:21:11,634 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,634 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:21:11,636 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,687 [api1.py:131] throughput: 0.5722652483821412
(INFO) 2023-04-10 17:21:11,688 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,688 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 8, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:21:11,690 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,753 [api1.py:131] throughput: 0.5212693673754828
(INFO) 2023-04-10 17:21:11,754 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,754 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 17:21:11,756 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,793 [api1.py:131] throughput: 18.419135125479645
(INFO) 2023-04-10 17:21:11,794 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,794 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 8, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:21:11,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,842 [api1.py:131] throughput: 7.991950312077949
(INFO) 2023-04-10 17:21:11,843 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,843 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:11,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,886 [api1.py:131] throughput: 3.6589781909609598
(INFO) 2023-04-10 17:21:11,887 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,887 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 7, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:11,888 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,934 [api1.py:131] throughput: 2.464861674329104
(INFO) 2023-04-10 17:21:11,935 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,935 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 200, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:21:11,936 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:11,976 [api1.py:131] throughput: 2.1854965013625813
(INFO) 2023-04-10 17:21:11,977 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:11,977 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:21:11,979 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:12,022 [api1.py:131] throughput: 3.69250274520637
(INFO) 2023-04-10 17:21:12,023 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:12,023 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:12,025 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:12,092 [api1.py:131] throughput: 0.2952659477315167
(INFO) 2023-04-10 17:21:16,967 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,967 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 127}
(INFO) 2023-04-10 17:21:16,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,971 [api2.py:131] throughput: 3.911680052241404
(INFO) 2023-04-10 17:21:16,972 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,972 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:21:16,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,975 [api2.py:131] throughput: 0.7610354449675488
(INFO) 2023-04-10 17:21:16,976 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,976 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 111}
(INFO) 2023-04-10 17:21:16,979 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,980 [api2.py:131] throughput: 0.451989243968668
(INFO) 2023-04-10 17:21:16,981 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,981 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 176, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 98}
(INFO) 2023-04-10 17:21:16,984 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,985 [api2.py:131] throughput: 1.0517548497466926
(INFO) 2023-04-10 17:21:16,985 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,986 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:16,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,990 [api2.py:131] throughput: 0.9473458068532828
(INFO) 2023-04-10 17:21:16,990 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,990 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 216}
(INFO) 2023-04-10 17:21:16,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,994 [api2.py:131] throughput: 2.648245583872638
(INFO) 2023-04-10 17:21:16,995 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:16,995 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:21:16,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:16,999 [api2.py:131] throughput: 1.6984355052733926
(INFO) 2023-04-10 17:21:17,000 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,000 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 443}
(INFO) 2023-04-10 17:21:17,004 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,004 [api2.py:131] throughput: 125.53138615458968
(INFO) 2023-04-10 17:21:17,005 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,005 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 100}
(INFO) 2023-04-10 17:21:17,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,009 [api2.py:131] throughput: 2.962490494584183
(INFO) 2023-04-10 17:21:17,009 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,009 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 17:21:17,013 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,014 [api2.py:131] throughput: 0.4858299184365413
(INFO) 2023-04-10 17:21:17,014 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,014 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 88}
(INFO) 2023-04-10 17:21:17,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,018 [api2.py:131] throughput: 22.230702990769103
(INFO) 2023-04-10 17:21:17,019 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,019 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 165, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:21:17,023 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,023 [api2.py:131] throughput: 0.493493049576567
(INFO) 2023-04-10 17:21:17,024 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,024 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:17,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,028 [api2.py:131] throughput: 1.2047724662357722
(INFO) 2023-04-10 17:21:17,028 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,028 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 101, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 128}
(INFO) 2023-04-10 17:21:17,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,032 [api2.py:131] throughput: 0.7886937832487396
(INFO) 2023-04-10 17:21:17,033 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,033 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:21:17,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,037 [api2.py:131] throughput: 2.7506280267000824
(INFO) 2023-04-10 17:21:17,037 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,037 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:21:17,041 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,041 [api2.py:131] throughput: 2.5107059461534145
(INFO) 2023-04-10 17:21:17,042 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,042 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:17,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,046 [api2.py:131] throughput: 1.744283538511019
(INFO) 2023-04-10 17:21:17,047 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,047 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 112, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:17,050 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,051 [api2.py:131] throughput: 1.1446229375504369
(INFO) 2023-04-10 17:21:17,051 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,051 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:21:17,055 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,055 [api2.py:131] throughput: 4.498317072570555
(INFO) 2023-04-10 17:21:17,056 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-10 17:21:17,056 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 156}
(INFO) 2023-04-10 17:21:17,060 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:17,060 [api2.py:131] throughput: 63.7103291102062
(INFO) 2023-04-10 17:21:21,702 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,702 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 11, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:21:21,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,704 [api2.py:131] throughput: 2.557926550557823
(INFO) 2023-04-10 17:21:21,704 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,704 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:21:21,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,706 [api2.py:131] throughput: 0.39368466879235653
(INFO) 2023-04-10 17:21:21,706 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,706 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 13, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:21:21,707 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,708 [api2.py:131] throughput: 3.9721347717030895
(INFO) 2023-04-10 17:21:21,708 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,709 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 7, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:21:21,710 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,710 [api2.py:131] throughput: 25.647510464815593
(INFO) 2023-04-10 17:21:21,710 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,711 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 11, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:21:21,712 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,712 [api2.py:131] throughput: 2.2419864234810185
(INFO) 2023-04-10 17:21:21,713 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,713 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 12, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:21:21,714 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,714 [api2.py:131] throughput: 0.5135096222336791
(INFO) 2023-04-10 17:21:21,714 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,715 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:21,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,716 [api2.py:131] throughput: 1.1262966509254184
(INFO) 2023-04-10 17:21:21,716 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,716 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 17:21:21,717 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,718 [api2.py:131] throughput: 13.826804127528899
(INFO) 2023-04-10 17:21:21,718 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,718 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:21:21,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,720 [api2.py:131] throughput: 20.060872464503245
(INFO) 2023-04-10 17:21:21,721 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,721 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 221, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:21:21,722 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,722 [api2.py:131] throughput: 1.2316262285399628
(INFO) 2023-04-10 17:21:21,723 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,723 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:21,724 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,724 [api2.py:131] throughput: 1.1449430369011242
(INFO) 2023-04-10 17:21:21,725 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,725 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 14, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 74}
(INFO) 2023-04-10 17:21:21,726 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,726 [api2.py:131] throughput: 123.16212812227315
(INFO) 2023-04-10 17:21:21,727 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,727 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 14, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:21,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,729 [api2.py:131] throughput: 7.196844294054856
(INFO) 2023-04-10 17:21:21,729 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,729 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:21:21,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,731 [api2.py:131] throughput: 33.106230702632786
(INFO) 2023-04-10 17:21:21,731 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,731 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 17:21:21,732 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,733 [api2.py:131] throughput: 0.5214515020555239
(INFO) 2023-04-10 17:21:21,733 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,733 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:21,734 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,735 [api2.py:131] throughput: 2.689418568867687
(INFO) 2023-04-10 17:21:21,735 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,735 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:21:21,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,737 [api2.py:131] throughput: 0.7214803480296883
(INFO) 2023-04-10 17:21:21,737 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,738 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 244, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:21,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,739 [api2.py:131] throughput: 0.3961311333704135
(INFO) 2023-04-10 17:21:21,740 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,740 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 17:21:21,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,742 [api2.py:131] throughput: 24.61629482983278
(INFO) 2023-04-10 17:21:21,742 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-10 17:21:21,742 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 398, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:21:21,743 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,743 [api2.py:131] throughput: 0.5286484225400822
(INFO) 2023-04-10 17:21:21,749 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:21,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 115, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:21:21,751 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,788 [api1.py:131] throughput: 0.6905851232946151
(INFO) 2023-04-10 17:21:21,788 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:21,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:21:21,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,841 [api1.py:131] throughput: 0.2861537924428033
(INFO) 2023-04-10 17:21:21,841 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:21,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 156, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:21,843 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:21,893 [api1.py:131] throughput: 0.13196923677072772
(INFO) 2023-04-10 17:21:21,894 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:21,894 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:21:21,895 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,048 [api1.py:131] throughput: 0.220509061758407
(INFO) 2023-04-10 17:21:22,049 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,103 [api1.py:131] throughput: 0.22611788842499955
(INFO) 2023-04-10 17:21:22,104 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,104 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 345, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:22,105 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,145 [api1.py:131] throughput: 0.2182532286041695
(INFO) 2023-04-10 17:21:22,146 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,146 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,148 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,201 [api1.py:131] throughput: 0.05590755727784883
(INFO) 2023-04-10 17:21:22,201 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,202 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 454, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,203 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,254 [api1.py:131] throughput: 0.038992931081044825
(INFO) 2023-04-10 17:21:22,255 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:22,256 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,306 [api1.py:131] throughput: 0.18958036263248992
(INFO) 2023-04-10 17:21:22,307 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,308 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,358 [api1.py:131] throughput: 0.2060029630924461
(INFO) 2023-04-10 17:21:22,359 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,359 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,411 [api1.py:131] throughput: 0.19036506427163172
(INFO) 2023-04-10 17:21:22,412 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,412 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:22,413 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,463 [api1.py:131] throughput: 0.3272074509105705
(INFO) 2023-04-10 17:21:22,464 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,517 [api1.py:131] throughput: 0.16277974454160718
(INFO) 2023-04-10 17:21:22,518 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,518 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:21:22,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,572 [api1.py:131] throughput: 0.36479842803759044
(INFO) 2023-04-10 17:21:22,573 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,573 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 316, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,629 [api1.py:131] throughput: 0.0768594312632725
(INFO) 2023-04-10 17:21:22,631 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,631 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,632 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,686 [api1.py:131] throughput: 0.11443656742856441
(INFO) 2023-04-10 17:21:22,687 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,687 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:21:22,688 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,731 [api1.py:131] throughput: 0.4982492424628663
(INFO) 2023-04-10 17:21:22,731 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,731 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:22,733 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,785 [api1.py:131] throughput: 0.3438475646833935
(INFO) 2023-04-10 17:21:22,786 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:21:22,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,839 [api1.py:131] throughput: 0.18906069543186738
(INFO) 2023-04-10 17:21:22,840 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-10 17:21:22,840 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 198, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:22,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:22,895 [api1.py:131] throughput: 0.03684608728915945
(INFO) 2023-04-10 17:21:27,652 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,653 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:27,654 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,701 [api1.py:131] throughput: 0.33507087291311927
(INFO) 2023-04-10 17:21:27,702 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 147, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:27,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,750 [api1.py:131] throughput: 0.12099244084647683
(INFO) 2023-04-10 17:21:27,750 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:27,752 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,797 [api1.py:131] throughput: 0.09634649141415488
(INFO) 2023-04-10 17:21:27,798 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,798 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:27,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,843 [api1.py:131] throughput: 0.39704865902272973
(INFO) 2023-04-10 17:21:27,844 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,844 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 235, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:21:27,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,884 [api1.py:131] throughput: 0.4226254427730417
(INFO) 2023-04-10 17:21:27,885 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,885 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 446, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:21:27,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,933 [api1.py:131] throughput: 0.3601549849460602
(INFO) 2023-04-10 17:21:27,934 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,934 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:27,935 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:27,983 [api1.py:131] throughput: 0.4976849771731783
(INFO) 2023-04-10 17:21:27,984 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:27,984 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 84, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:27,985 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,035 [api1.py:131] throughput: 0.09686015519838179
(INFO) 2023-04-10 17:21:28,036 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,036 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 206, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,037 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,084 [api1.py:131] throughput: 0.09250678469195826
(INFO) 2023-04-10 17:21:28,085 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,085 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 156, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,086 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,132 [api1.py:131] throughput: 0.2192015562078524
(INFO) 2023-04-10 17:21:28,133 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,133 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 439, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:21:28,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,164 [api1.py:131] throughput: 2.975468174784921
(INFO) 2023-04-10 17:21:28,165 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,165 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,166 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,212 [api1.py:131] throughput: 0.1989819459637114
(INFO) 2023-04-10 17:21:28,213 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,213 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 489, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:28,214 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,255 [api1.py:131] throughput: 0.10965543971415262
(INFO) 2023-04-10 17:21:28,256 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,256 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 371, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,257 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,299 [api1.py:131] throughput: 0.06758955466667319
(INFO) 2023-04-10 17:21:28,300 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,300 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 459, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:28,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,467 [api1.py:131] throughput: 0.13742491617727187
(INFO) 2023-04-10 17:21:28,468 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,468 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 494, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,518 [api1.py:131] throughput: 0.03346508818182061
(INFO) 2023-04-10 17:21:28,519 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,519 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 218, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:28,520 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,565 [api1.py:131] throughput: 0.20447513949791435
(INFO) 2023-04-10 17:21:28,566 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 149, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,617 [api1.py:131] throughput: 0.09541107018757872
(INFO) 2023-04-10 17:21:28,618 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,618 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:21:28,619 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,663 [api1.py:131] throughput: 0.5460636305744201
(INFO) 2023-04-10 17:21:28,664 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:21:28,664 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 247, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:28,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:28,703 [api1.py:131] throughput: 0.1391106889397033
(INFO) 2023-04-10 17:21:33,482 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,482 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:33,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,485 [api2.py:131] throughput: 1.153534414512079
(INFO) 2023-04-10 17:21:33,485 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,485 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 254, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:33,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,487 [api2.py:131] throughput: 0.6245266216681803
(INFO) 2023-04-10 17:21:33,488 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,488 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 171, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:33,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,490 [api2.py:131] throughput: 0.6228302059099131
(INFO) 2023-04-10 17:21:33,491 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,491 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 151, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,493 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,493 [api2.py:131] throughput: 0.6404731712892163
(INFO) 2023-04-10 17:21:33,494 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,494 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 17:21:33,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,496 [api2.py:131] throughput: 1.0089906663915524
(INFO) 2023-04-10 17:21:33,496 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,496 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 488, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:33,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,498 [api2.py:131] throughput: 0.3965565303324408
(INFO) 2023-04-10 17:21:33,499 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,499 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:21:33,501 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,501 [api2.py:131] throughput: 1.0612508007779589
(INFO) 2023-04-10 17:21:33,502 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,502 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 90, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:33,503 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,504 [api2.py:131] throughput: 2.124627664610794
(INFO) 2023-04-10 17:21:33,504 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,504 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 118, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,506 [api2.py:131] throughput: 1.0926022816148098
(INFO) 2023-04-10 17:21:33,507 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,507 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 507, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,509 [api2.py:131] throughput: 0.422997600496328
(INFO) 2023-04-10 17:21:33,510 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,510 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 329, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:33,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,512 [api2.py:131] throughput: 0.6161891692373329
(INFO) 2023-04-10 17:21:33,513 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,513 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 170, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,514 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,514 [api2.py:131] throughput: 1.1679751935656377
(INFO) 2023-04-10 17:21:33,515 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,515 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 97, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:33,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,517 [api2.py:131] throughput: 1.9639774455641439
(INFO) 2023-04-10 17:21:33,518 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,518 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 457, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,520 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,520 [api2.py:131] throughput: 0.3578960721391971
(INFO) 2023-04-10 17:21:33,521 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,521 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:33,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,523 [api2.py:131] throughput: 1.5911580993983672
(INFO) 2023-04-10 17:21:33,524 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,524 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,526 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,526 [api2.py:131] throughput: 7.034311267950109
(INFO) 2023-04-10 17:21:33,527 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,527 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 474, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,529 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,529 [api2.py:131] throughput: 0.33550291150675066
(INFO) 2023-04-10 17:21:33,530 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,530 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:21:33,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,532 [api2.py:131] throughput: 0.7933667245825029
(INFO) 2023-04-10 17:21:33,532 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,532 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,534 [api2.py:131] throughput: 4.465785226535233
(INFO) 2023-04-10 17:21:33,535 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:21:33,535 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 139, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:33,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:33,538 [api2.py:131] throughput: 1.2701838207284661
(INFO) 2023-04-10 17:21:38,385 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,386 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:38,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,387 [api2.py:131] throughput: 4.567847251109965
(INFO) 2023-04-10 17:21:38,388 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,388 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,389 [api2.py:131] throughput: 8.2302799857851
(INFO) 2023-04-10 17:21:38,390 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,390 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:21:38,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,392 [api2.py:131] throughput: 0.5275831770525866
(INFO) 2023-04-10 17:21:38,392 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,393 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 17:21:38,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,394 [api2.py:131] throughput: 8.51168374995642
(INFO) 2023-04-10 17:21:38,395 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,395 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:21:38,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,397 [api2.py:131] throughput: 2.759619965231818
(INFO) 2023-04-10 17:21:38,397 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,398 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,399 [api2.py:131] throughput: 0.9656143643371453
(INFO) 2023-04-10 17:21:38,400 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,400 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 130, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:38,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,401 [api2.py:131] throughput: 0.5972882707308457
(INFO) 2023-04-10 17:21:38,402 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,402 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 7, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,403 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,403 [api2.py:131] throughput: 3.5741048611485993
(INFO) 2023-04-10 17:21:38,404 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,404 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 251, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,406 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,406 [api2.py:131] throughput: 0.5092526828832799
(INFO) 2023-04-10 17:21:38,406 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,406 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,408 [api2.py:131] throughput: 1.7481429966426705
(INFO) 2023-04-10 17:21:38,409 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,409 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 9, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:38,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,410 [api2.py:131] throughput: 1.1577186021676702
(INFO) 2023-04-10 17:21:38,411 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,411 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 88, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,412 [api2.py:131] throughput: 1.365050472533734
(INFO) 2023-04-10 17:21:38,413 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,413 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:21:38,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,414 [api2.py:131] throughput: 0.6507928034986088
(INFO) 2023-04-10 17:21:38,415 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,415 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 453, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,417 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,417 [api2.py:131] throughput: 0.3789085857440859
(INFO) 2023-04-10 17:21:38,418 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,418 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,419 [api2.py:131] throughput: 2.2125700828918675
(INFO) 2023-04-10 17:21:38,420 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,420 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:21:38,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,422 [api2.py:131] throughput: 1.26645792810282
(INFO) 2023-04-10 17:21:38,422 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,422 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 230, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:38,423 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,424 [api2.py:131] throughput: 0.4353176335998593
(INFO) 2023-04-10 17:21:38,424 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,424 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,426 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,426 [api2.py:131] throughput: 4.010680088134856
(INFO) 2023-04-10 17:21:38,427 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,427 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:38,428 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,429 [api2.py:131] throughput: 8.615004940188433
(INFO) 2023-04-10 17:21:38,429 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-10 17:21:38,429 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 9, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 17:21:38,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,431 [api2.py:131] throughput: 89.89911244198782
(INFO) 2023-04-10 17:21:38,436 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,437 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 466, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:21:38,438 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,439 [api2.py:131] throughput: 0.3072952197571985
(INFO) 2023-04-10 17:21:38,439 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,439 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 167, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,442 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,442 [api2.py:131] throughput: 1.1607194344493366
(INFO) 2023-04-10 17:21:38,442 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,443 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 222, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:38,444 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,444 [api2.py:131] throughput: 0.32124452143482973
(INFO) 2023-04-10 17:21:38,445 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,445 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 174, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:21:38,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,447 [api2.py:131] throughput: 0.4323616600647971
(INFO) 2023-04-10 17:21:38,447 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,448 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 476, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,450 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,450 [api2.py:131] throughput: 0.48261350410908577
(INFO) 2023-04-10 17:21:38,451 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,451 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 341, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,452 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,452 [api2.py:131] throughput: 0.5114136263519593
(INFO) 2023-04-10 17:21:38,453 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,453 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,455 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,455 [api2.py:131] throughput: 2.0222689519682095
(INFO) 2023-04-10 17:21:38,456 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,456 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:21:38,458 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,458 [api2.py:131] throughput: 0.635595870195256
(INFO) 2023-04-10 17:21:38,459 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,459 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 363, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,460 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,461 [api2.py:131] throughput: 0.4949890995806557
(INFO) 2023-04-10 17:21:38,461 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,461 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 155, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:21:38,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,463 [api2.py:131] throughput: 1.3050820969379007
(INFO) 2023-04-10 17:21:38,464 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,464 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 176, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,466 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,466 [api2.py:131] throughput: 0.7908067755525028
(INFO) 2023-04-10 17:21:38,467 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,467 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:21:38,469 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,470 [api2.py:131] throughput: 84.44095862163553
(INFO) 2023-04-10 17:21:38,470 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,471 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:21:38,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,473 [api2.py:131] throughput: 0.39447526080569095
(INFO) 2023-04-10 17:21:38,473 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,474 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,476 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,476 [api2.py:131] throughput: 1.5416907301010836
(INFO) 2023-04-10 17:21:38,477 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,477 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 164, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:21:38,479 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,480 [api2.py:131] throughput: 0.6853850275931583
(INFO) 2023-04-10 17:21:38,480 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,480 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,482 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,483 [api2.py:131] throughput: 1.962308203635196
(INFO) 2023-04-10 17:21:38,483 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,484 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 17:21:38,486 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,486 [api2.py:131] throughput: 0.9079268582510663
(INFO) 2023-04-10 17:21:38,487 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,487 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 98, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:21:38,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,490 [api2.py:131] throughput: 99.25731831532813
(INFO) 2023-04-10 17:21:38,490 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,490 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 60, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:38,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,493 [api2.py:131] throughput: 1.9464393659024917
(INFO) 2023-04-10 17:21:38,493 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 17}
(INFO) 2023-04-10 17:21:38,494 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 119, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:38,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:38,496 [api2.py:131] throughput: 0.7045262524574201
(INFO) 2023-04-10 17:21:43,426 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:43,428 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,475 [api1.py:131] throughput: 0.13765150100394413
(INFO) 2023-04-10 17:21:43,476 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:43,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,522 [api1.py:131] throughput: 0.21135867061848537
(INFO) 2023-04-10 17:21:43,523 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,523 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:43,524 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,572 [api1.py:131] throughput: 0.11660127368739016
(INFO) 2023-04-10 17:21:43,573 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,573 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 209, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:43,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,619 [api1.py:131] throughput: 0.12614685861463318
(INFO) 2023-04-10 17:21:43,620 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,620 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 236, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:43,621 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,666 [api1.py:131] throughput: 0.08084414094027598
(INFO) 2023-04-10 17:21:43,667 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,667 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 394, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:43,668 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,713 [api1.py:131] throughput: 0.05443302636381155
(INFO) 2023-04-10 17:21:43,714 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,714 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:43,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,759 [api1.py:131] throughput: 0.30051872705767907
(INFO) 2023-04-10 17:21:43,760 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,760 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:43,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,806 [api1.py:131] throughput: 0.7008715298224537
(INFO) 2023-04-10 17:21:43,807 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,807 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 254, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:43,808 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,852 [api1.py:131] throughput: 0.09634503739630312
(INFO) 2023-04-10 17:21:43,853 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,853 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 190, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:43,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,898 [api1.py:131] throughput: 0.1085350007421363
(INFO) 2023-04-10 17:21:43,898 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,898 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:43,899 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,943 [api1.py:131] throughput: 0.45694373453665843
(INFO) 2023-04-10 17:21:43,943 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,944 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 112, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:21:43,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:43,987 [api1.py:131] throughput: 0.19118167576801562
(INFO) 2023-04-10 17:21:43,988 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:43,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 195, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:21:43,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,032 [api1.py:131] throughput: 0.17910390550594868
(INFO) 2023-04-10 17:21:44,033 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,033 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 186, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:44,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,077 [api1.py:131] throughput: 0.13349985229149142
(INFO) 2023-04-10 17:21:44,078 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,078 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 209, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:44,079 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,123 [api1.py:131] throughput: 0.06486062151892526
(INFO) 2023-04-10 17:21:44,124 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,124 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 270, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:44,125 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,168 [api1.py:131] throughput: 0.0940460479304645
(INFO) 2023-04-10 17:21:44,169 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,169 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 231, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:21:44,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,215 [api1.py:131] throughput: 0.056685855182365616
(INFO) 2023-04-10 17:21:44,215 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,215 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 86, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:21:44,216 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,255 [api1.py:131] throughput: 0.44952944092450675
(INFO) 2023-04-10 17:21:44,256 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,256 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:21:44,258 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,302 [api1.py:131] throughput: 0.3440773097342763
(INFO) 2023-04-10 17:21:44,302 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-10 17:21:44,303 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 355, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:21:44,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:21:44,342 [api1.py:131] throughput: 0.24662205908916168
