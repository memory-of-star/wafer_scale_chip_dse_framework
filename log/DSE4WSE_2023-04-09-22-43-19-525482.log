(DEBUG) 2023-04-09 22:43:19,525 [logger.py:40] logger init.
(INFO) 2023-04-09 22:43:19,525 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-09-22-43-19-525482.log
(INFO) 2023-04-09 22:43:23,779 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 22:43:23,779 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 12, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-09 22:43:23,786 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:23,786 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 130, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-09 22:43:23,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:23,794 [api2.py:131] throughput: 2.0097128434091656
(INFO) 2023-04-09 22:43:23,797 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:23,798 [api2.py:131] throughput: 1.0788708816995465
(INFO) 2023-04-09 22:43:23,894 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 22:43:23,895 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-09 22:43:23,900 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:23,901 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-09 22:43:23,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:23,907 [api2.py:131] throughput: 15.273755979631828
(INFO) 2023-04-09 22:43:23,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:23,912 [api2.py:131] throughput: 5.129435771913607
(INFO) 2023-04-09 22:43:24,005 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 22:43:24,006 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 230, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-09 22:43:24,012 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:24,013 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-09 22:43:24,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,017 [api2.py:131] throughput: 0.5775838326429783
(INFO) 2023-04-09 22:43:24,023 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,024 [api2.py:131] throughput: 0.3885474071258361
(INFO) 2023-04-09 22:43:24,117 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 14}
(INFO) 2023-04-09 22:43:24,117 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 78, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:24,125 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:24,126 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 327, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:24,128 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,129 [api2.py:131] throughput: 0.9376703452320894
(INFO) 2023-04-09 22:43:24,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,137 [api2.py:131] throughput: 0.4741729884421055
(INFO) 2023-04-09 22:43:24,179 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:24,180 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 131}
(INFO) 2023-04-09 22:43:24,201 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,202 [api2.py:131] throughput: 1.9711423806526205
(INFO) 2023-04-09 22:43:24,307 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:24,307 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 88}
(INFO) 2023-04-09 22:43:24,330 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,331 [api2.py:131] throughput: 14.422337047100855
(INFO) 2023-04-09 22:43:24,345 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:24,346 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 506, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:24,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,355 [api2.py:131] throughput: 0.3590345944181616
(INFO) 2023-04-09 22:43:24,431 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:24,432 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 181}
(INFO) 2023-04-09 22:43:24,443 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:24,444 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 16, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-09 22:43:24,452 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,453 [api2.py:131] throughput: 0.3880194368738025
(INFO) 2023-04-09 22:43:24,455 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:24,455 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:24,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,465 [api2.py:131] throughput: 10.141783759645243
(INFO) 2023-04-09 22:43:24,546 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:24,546 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-09 22:43:24,552 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,554 [api2.py:131] throughput: 0.3911829408755742
(INFO) 2023-04-09 22:43:24,559 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:24,560 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 166, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-09 22:43:24,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,583 [api2.py:131] throughput: 0.6701005047854939
(INFO) 2023-04-09 22:43:24,658 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:24,659 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 327, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:24,668 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,669 [api2.py:131] throughput: 0.47855447183870947
(INFO) 2023-04-09 22:43:24,761 [api1.py:131] throughput: 0.6621254576490089
(INFO) 2023-04-09 22:43:24,806 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 22:43:24,806 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 62, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-09 22:43:24,817 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,861 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:24,862 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 14, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 56}
(INFO) 2023-04-09 22:43:24,873 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,884 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 55}
(INFO) 2023-04-09 22:43:24,884 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:24,892 [api1.py:131] throughput: 4.658924084884877
(INFO) 2023-04-09 22:43:24,896 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:24,897 [api2.py:131] throughput: 1.6223512881685387
(INFO) 2023-04-09 22:43:25,000 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 22:43:25,001 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-09 22:43:25,001 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 55}
(INFO) 2023-04-09 22:43:25,002 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-09 22:43:25,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,017 [api2.py:131] throughput: 7.459731653947825
(INFO) 2023-04-09 22:43:25,102 [api1.py:131] throughput: 3.152902284387606
(INFO) 2023-04-09 22:43:25,114 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 55}
(INFO) 2023-04-09 22:43:25,114 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-09 22:43:25,118 [api1.py:131] throughput: 0.2848971401895035
(INFO) 2023-04-09 22:43:25,128 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,129 [api2.py:131] throughput: 0.3877784046819443
(INFO) 2023-04-09 22:43:25,207 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:25,207 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-09 22:43:25,217 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 22:43:25,218 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-09 22:43:25,219 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,227 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,227 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 55}
(INFO) 2023-04-09 22:43:25,227 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-09 22:43:25,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,241 [api2.py:131] throughput: 0.9707312218349813
(INFO) 2023-04-09 22:43:25,322 [api1.py:131] throughput: 0.5767399639685824
(INFO) 2023-04-09 22:43:25,390 [api1.py:131] throughput: 2.506976688396467
(INFO) 2023-04-09 22:43:25,425 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 22:43:25,425 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 150, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-09 22:43:25,432 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,497 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:25,498 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:25,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,537 [api1.py:131] throughput: 0.8271669398748567
(INFO) 2023-04-09 22:43:25,553 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:25,554 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 114, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:25,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,636 [api1.py:131] throughput: 0.10574988422292868
(INFO) 2023-04-09 22:43:25,736 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:25,737 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-09 22:43:25,744 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,748 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:25,749 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 506, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:25,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,762 [api2.py:131] throughput: 0.35640411710175446
(INFO) 2023-04-09 22:43:25,821 [api1.py:131] throughput: 0.12929786578738017
(INFO) 2023-04-09 22:43:25,826 [api1.py:131] throughput: 0.16121406425941548
(INFO) 2023-04-09 22:43:25,862 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:25,863 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-09 22:43:25,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,876 [api2.py:131] throughput: 5.123531788119025
(INFO) 2023-04-09 22:43:25,931 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:25,932 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-09 22:43:25,940 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,975 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:25,975 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-09 22:43:25,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:25,990 [api2.py:131] throughput: 0.38882639336105895
(INFO) 2023-04-09 22:43:26,015 [api1.py:131] throughput: 0.3836266990319121
(INFO) 2023-04-09 22:43:26,038 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 22:43:26,038 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:26,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,048 [api2.py:131] throughput: 1.3311659427630491
(INFO) 2023-04-09 22:43:26,093 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:26,093 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 327, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:26,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,104 [api2.py:131] throughput: 0.47417675194805303
(INFO) 2023-04-09 22:43:26,122 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:26,123 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 191, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:26,133 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,152 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 22:43:26,153 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-09 22:43:26,162 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,164 [api2.py:131] throughput: 11.464727121320742
(INFO) 2023-04-09 22:43:26,215 [api1.py:131] throughput: 0.20558916728575255
(INFO) 2023-04-09 22:43:26,266 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 22:43:26,266 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-09 22:43:26,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,274 [api2.py:131] throughput: 0.381117927043168
(INFO) 2023-04-09 22:43:26,329 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:26,330 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-09 22:43:26,341 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,342 [api2.py:131] throughput: 1.6740719381021025
(INFO) 2023-04-09 22:43:26,380 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 22:43:26,381 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 120, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-09 22:43:26,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,389 [api2.py:131] throughput: 0.8067525991137553
(INFO) 2023-04-09 22:43:26,436 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 28}
(INFO) 2023-04-09 22:43:26,437 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 506, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-09 22:43:26,449 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:26,450 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 102, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-09 22:43:26,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,461 [api2.py:131] throughput: 9.525147208019739
(INFO) 2023-04-09 22:43:26,461 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,569 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:26,570 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-09 22:43:26,581 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,583 [api2.py:131] throughput: 0.3864073973800647
(INFO) 2023-04-09 22:43:26,607 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 17}
(INFO) 2023-04-09 22:43:26,608 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 145, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-09 22:43:26,618 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,699 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:26,700 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 202, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:26,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,710 [api2.py:131] throughput: 0.6284795839658163
(INFO) 2023-04-09 22:43:26,789 [api1.py:131] throughput: 0.22450375681142498
(INFO) 2023-04-09 22:43:26,894 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 17}
(INFO) 2023-04-09 22:43:26,895 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-09 22:43:26,903 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,913 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:26,914 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 57, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-09 22:43:26,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:26,922 [api2.py:131] throughput: 1.59378074588708
(INFO) 2023-04-09 22:43:27,019 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:27,019 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-09 22:43:27,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,030 [api2.py:131] throughput: 11.921191215144136
(INFO) 2023-04-09 22:43:27,073 [api1.py:131] throughput: 0.061370068053887075
(INFO) 2023-04-09 22:43:27,131 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:27,131 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-09 22:43:27,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,138 [api2.py:131] throughput: 0.38727152479416305
(INFO) 2023-04-09 22:43:27,170 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 17}
(INFO) 2023-04-09 22:43:27,170 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-09 22:43:27,179 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,240 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:27,241 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-09 22:43:27,252 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,253 [api2.py:131] throughput: 1.4511553207941839
(INFO) 2023-04-09 22:43:27,339 [api1.py:131] throughput: 0.2378327002135078
(INFO) 2023-04-09 22:43:27,443 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 17}
(INFO) 2023-04-09 22:43:27,444 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 327, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:27,454 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,664 [api1.py:131] throughput: 0.11895551684903137
(INFO) 2023-04-09 22:43:27,859 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:27,859 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 11, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-09 22:43:27,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,869 [api2.py:131] throughput: 2.1368613822097835
(INFO) 2023-04-09 22:43:27,973 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:27,974 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 9, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-09 22:43:27,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:27,985 [api2.py:131] throughput: 13.559251026557959
(INFO) 2023-04-09 22:43:28,085 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:28,086 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 221, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-09 22:43:28,092 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:28,093 [api2.py:131] throughput: 0.60249992731342
(INFO) 2023-04-09 22:43:28,159 [api1.py:131] throughput: 0.18164410346451046
(INFO) 2023-04-09 22:43:28,193 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 22:43:28,194 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-09 22:43:28,203 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:28,205 [api2.py:131] throughput: 0.9803117021046993
(INFO) 2023-04-09 22:43:28,248 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 28}
(INFO) 2023-04-09 22:43:28,249 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 255, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-09 22:43:28,271 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:29,784 [api1.py:131] throughput: 0.04385909891491752
(INFO) 2023-04-09 22:43:29,885 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 28}
(INFO) 2023-04-09 22:43:29,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-09 22:43:29,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:31,469 [api1.py:131] throughput: 0.38117270330911085
(INFO) 2023-04-09 22:43:31,567 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 28}
(INFO) 2023-04-09 22:43:31,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 327, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-09 22:43:31,587 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:32,463 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:32,463 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-09 22:43:32,469 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:32,471 [api2.py:131] throughput: 0.8558389999382278
(INFO) 2023-04-09 22:43:32,579 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:32,580 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 495, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:32,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:32,588 [api2.py:131] throughput: 0.4195248052599416
(INFO) 2023-04-09 22:43:32,692 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:32,692 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-09 22:43:32,698 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:32,700 [api2.py:131] throughput: 0.5325282210987647
(INFO) 2023-04-09 22:43:32,802 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 22:43:32,803 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-09 22:43:32,809 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:32,810 [api2.py:131] throughput: 0.8975919145609687
(INFO) 2023-04-09 22:43:33,229 [api1.py:131] throughput: 0.2353342077530984
(INFO) 2023-04-09 22:43:33,540 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 22:43:33,541 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 91}
(INFO) 2023-04-09 22:43:33,551 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:33,552 [api2.py:131] throughput: 0.860600808166437
(INFO) 2023-04-09 22:43:33,638 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 22:43:33,638 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 495, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-09 22:43:33,650 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:33,651 [api2.py:131] throughput: 0.44435013618093583
(INFO) 2023-04-09 22:43:33,745 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 22:43:33,746 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-09 22:43:33,758 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:33,759 [api2.py:131] throughput: 0.528453929825291
(INFO) 2023-04-09 22:43:33,843 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 22:43:33,843 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 166, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-09 22:43:33,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:33,855 [api2.py:131] throughput: 0.7616833811233857
(INFO) 2023-04-09 22:43:37,784 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:37,785 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 139, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 93}
(INFO) 2023-04-09 22:43:37,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:37,862 [api1.py:131] throughput: 12.392682195661186
(INFO) 2023-04-09 22:43:37,966 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:37,967 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 74, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:37,974 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:38,093 [api1.py:131] throughput: 0.15950715655374345
(INFO) 2023-04-09 22:43:38,196 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:38,196 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 253, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-09 22:43:38,202 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:38,314 [api1.py:131] throughput: 0.21021350599437166
(INFO) 2023-04-09 22:43:38,417 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:38,417 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:38,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:38,577 [api1.py:131] throughput: 0.23383240726507104
(INFO) 2023-04-09 22:43:38,782 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:38,783 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-09 22:43:38,788 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:38,789 [api2.py:131] throughput: 0.8588466128290052
(INFO) 2023-04-09 22:43:38,903 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:38,904 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 495, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:38,908 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:38,910 [api2.py:131] throughput: 0.4520672501548573
(INFO) 2023-04-09 22:43:38,999 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:38,999 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 244, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-09 22:43:39,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,007 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:39,007 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:39,007 [api2.py:131] throughput: 1.1712645794926078
(INFO) 2023-04-09 22:43:39,013 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,014 [api2.py:131] throughput: 0.5318312345270296
(INFO) 2023-04-09 22:43:39,097 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:39,097 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 239, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-09 22:43:39,098 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 22:43:39,099 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-09 22:43:39,105 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,107 [api2.py:131] throughput: 0.7332648215735846
(INFO) 2023-04-09 22:43:39,107 [api2.py:131] throughput: 0.3053612891006323
(INFO) 2023-04-09 22:43:39,211 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:39,211 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 251, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-09 22:43:39,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,218 [api2.py:131] throughput: 0.6279886073735896
(INFO) 2023-04-09 22:43:39,327 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:39,328 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-09 22:43:39,337 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,338 [api2.py:131] throughput: 1.842942251412326
(INFO) 2023-04-09 22:43:39,566 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:39,566 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 12, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 247}
(INFO) 2023-04-09 22:43:39,575 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,698 [api1.py:131] throughput: 24.78316584111312
(INFO) 2023-04-09 22:43:39,787 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:39,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 19, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-09 22:43:39,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:39,980 [api1.py:131] throughput: 9.716566034733502
(INFO) 2023-04-09 22:43:40,092 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:40,093 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-09 22:43:40,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:40,280 [api1.py:131] throughput: 6.443360310412056
(INFO) 2023-04-09 22:43:40,389 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 22:43:40,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-09 22:43:40,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:40,624 [api1.py:131] throughput: 0.6397018557834765
(INFO) 2023-04-09 22:43:43,318 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:43,319 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:43,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:43,380 [api1.py:131] throughput: 0.46724275710031965
(INFO) 2023-04-09 22:43:43,485 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:43,486 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-09 22:43:43,493 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:43,540 [api1.py:131] throughput: 5.488036276604694
(INFO) 2023-04-09 22:43:43,644 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:43,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-09 22:43:43,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:43,694 [api1.py:131] throughput: 6.315519727864761
(INFO) 2023-04-09 22:43:43,805 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 22:43:43,806 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-09 22:43:43,812 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:43,864 [api1.py:131] throughput: 0.9495862733132091
(INFO) 2023-04-09 22:43:44,304 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 47}
(INFO) 2023-04-09 22:43:44,305 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 139, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 188}
(INFO) 2023-04-09 22:43:44,318 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:44,320 [api2.py:131] throughput: 193.47979353022012
(INFO) 2023-04-09 22:43:44,428 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 47}
(INFO) 2023-04-09 22:43:44,428 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 114, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-09 22:43:44,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:44,440 [api2.py:131] throughput: 1.4338704827146271
(INFO) 2023-04-09 22:43:45,041 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 47}
(INFO) 2023-04-09 22:43:45,041 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 253, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-09 22:43:45,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:45,055 [api2.py:131] throughput: 0.648477193631986
(INFO) 2023-04-09 22:43:45,153 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 47}
(INFO) 2023-04-09 22:43:45,154 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-09 22:43:45,166 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:45,167 [api2.py:131] throughput: 0.6831637347674605
(INFO) 2023-04-09 22:43:45,691 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 22:43:45,691 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:45,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:45,902 [api1.py:131] throughput: 0.3076610823856556
(INFO) 2023-04-09 22:43:45,988 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 22:43:45,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 97, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-09 22:43:45,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:46,284 [api1.py:131] throughput: 1.1296703675206456
(INFO) 2023-04-09 22:43:46,376 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 22:43:46,377 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 428, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-09 22:43:46,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:46,679 [api1.py:131] throughput: 0.07850023342270616
(INFO) 2023-04-09 22:43:46,768 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 49}
(INFO) 2023-04-09 22:43:46,768 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 187, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-09 22:43:46,780 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:47,033 [api1.py:131] throughput: 2.1452800260714078
(INFO) 2023-04-09 22:43:50,107 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 44}
(INFO) 2023-04-09 22:43:50,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-09 22:43:50,128 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:51,054 [api1.py:131] throughput: 0.17871675858516134
(INFO) 2023-04-09 22:43:51,154 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 44}
(INFO) 2023-04-09 22:43:51,155 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 479, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-09 22:43:51,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:52,015 [api1.py:131] throughput: 0.0688525207123117
(INFO) 2023-04-09 22:43:52,116 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 44}
(INFO) 2023-04-09 22:43:52,129 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 428, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 63}
(INFO) 2023-04-09 22:43:52,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:52,983 [api1.py:131] throughput: 0.47107041433071867
(INFO) 2023-04-09 22:43:53,080 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 44}
(INFO) 2023-04-09 22:43:53,081 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 187, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 66}
(INFO) 2023-04-09 22:43:53,099 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 22:43:53,891 [api1.py:131] throughput: 0.7839619329506569
