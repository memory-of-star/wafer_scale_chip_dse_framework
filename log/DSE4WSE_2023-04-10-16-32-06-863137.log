(DEBUG) 2023-04-10 16:32:06,863 [logger.py:40] logger init.
(INFO) 2023-04-10 16:32:06,863 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-16-32-06-863137.log
(INFO) 2023-04-10 16:32:10,800 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:10,802 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 102, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 16:32:10,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:10,842 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:10,843 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 16:32:10,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:10,855 [api2.py:131] throughput: 0.7983622831695485
(INFO) 2023-04-10 16:32:10,962 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:10,963 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 16:32:10,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:10,977 [api2.py:131] throughput: 0.3981002364053976
(INFO) 2023-04-10 16:32:11,083 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,084 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 80}
(INFO) 2023-04-10 16:32:11,096 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,097 [api2.py:131] throughput: 1.877380336718091
(INFO) 2023-04-10 16:32:11,132 [api1.py:131] throughput: 0.2602099894265488
(INFO) 2023-04-10 16:32:11,212 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,213 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 223, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 16:32:11,224 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,225 [api2.py:131] throughput: 0.643804922021578
(INFO) 2023-04-10 16:32:11,226 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:11,227 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 478, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 16:32:11,238 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,326 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,326 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 235, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 16:32:11,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,339 [api2.py:131] throughput: 1.03432454758537
(INFO) 2023-04-10 16:32:11,440 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,441 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 16:32:11,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,452 [api2.py:131] throughput: 1.2378378098667473
(INFO) 2023-04-10 16:32:11,532 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,533 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 16:32:11,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,546 [api2.py:131] throughput: 4.514690155142134
(INFO) 2023-04-10 16:32:11,572 [api1.py:131] throughput: 0.2390739668770029
(INFO) 2023-04-10 16:32:11,629 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,630 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 170, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 16:32:11,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,641 [api2.py:131] throughput: 0.7435856294127974
(INFO) 2023-04-10 16:32:11,653 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:11,654 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 64, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 16:32:11,665 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,734 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,735 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 16:32:11,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,748 [api2.py:131] throughput: 0.3780043534786978
(INFO) 2023-04-10 16:32:11,846 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 28}
(INFO) 2023-04-10 16:32:11,847 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 16:32:11,857 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:11,858 [api2.py:131] throughput: 1.0912330949296334
(INFO) 2023-04-10 16:32:11,894 [api1.py:131] throughput: 0.46629477450268697
(INFO) 2023-04-10 16:32:11,979 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:11,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 223, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 16:32:11,990 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:12,268 [api1.py:131] throughput: 0.5323520112514514
(INFO) 2023-04-10 16:32:12,368 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:12,369 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 235, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 16:32:12,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:12,651 [api1.py:131] throughput: 0.26765570177170495
(INFO) 2023-04-10 16:32:12,758 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:12,759 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 16:32:12,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:13,107 [api1.py:131] throughput: 0.2195176183283736
(INFO) 2023-04-10 16:32:13,200 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:13,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 16:32:13,214 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:13,493 [api1.py:131] throughput: 1.1435708108647167
(INFO) 2023-04-10 16:32:13,602 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:13,603 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 16:32:13,615 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:13,936 [api1.py:131] throughput: 0.20806396307743794
(INFO) 2023-04-10 16:32:14,033 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:14,034 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 16:32:14,047 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:14,351 [api1.py:131] throughput: 1.5865700360383777
(INFO) 2023-04-10 16:32:14,442 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-10 16:32:14,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 16:32:14,454 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 16:32:14,791 [api1.py:131] throughput: 0.13681912448188804
