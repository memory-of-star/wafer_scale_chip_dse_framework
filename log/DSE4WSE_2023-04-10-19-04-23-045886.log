(DEBUG) 2023-04-10 19:04:23,046 [logger.py:40] logger init.
(INFO) 2023-04-10 19:04:23,046 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-19-04-23-045886.log
(INFO) 2023-04-10 19:04:26,972 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,972 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 170, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:26,975 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,975 [api2.py:131] throughput: 0.5162377982199209
(INFO) 2023-04-10 19:04:26,976 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,976 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:26,978 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,979 [api2.py:131] throughput: 1.137364947648311
(INFO) 2023-04-10 19:04:26,979 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,979 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 7, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 104}
(INFO) 2023-04-10 19:04:26,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,982 [api2.py:131] throughput: 0.7069201545757716
(INFO) 2023-04-10 19:04:26,982 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,983 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:26,985 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,985 [api2.py:131] throughput: 2.3471922983370495
(INFO) 2023-04-10 19:04:26,985 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,985 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 206, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 19:04:26,988 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,988 [api2.py:131] throughput: 1.2935481881924722
(INFO) 2023-04-10 19:04:26,989 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,989 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 128, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 90}
(INFO) 2023-04-10 19:04:26,991 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,991 [api2.py:131] throughput: 29.165861948129
(INFO) 2023-04-10 19:04:26,992 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,992 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 10, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:26,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,994 [api2.py:131] throughput: 2.1457548486781177
(INFO) 2023-04-10 19:04:26,995 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,995 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:26,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:26,997 [api2.py:131] throughput: 6.682106716644121
(INFO) 2023-04-10 19:04:26,998 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:26,998 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 7, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:27,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,000 [api2.py:131] throughput: 1.9491194296308136
(INFO) 2023-04-10 19:04:27,001 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,001 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 316, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,003 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,003 [api2.py:131] throughput: 0.3980255581946091
(INFO) 2023-04-10 19:04:27,004 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,004 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 67}
(INFO) 2023-04-10 19:04:27,006 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,006 [api2.py:131] throughput: 1.8890552822614881
(INFO) 2023-04-10 19:04:27,007 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,007 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 284, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 55}
(INFO) 2023-04-10 19:04:27,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,010 [api2.py:131] throughput: 0.7738145482577323
(INFO) 2023-04-10 19:04:27,010 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,010 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 51, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 117}
(INFO) 2023-04-10 19:04:27,013 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,013 [api2.py:131] throughput: 1.6676762022381804
(INFO) 2023-04-10 19:04:27,013 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,014 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 130, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:27,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,016 [api2.py:131] throughput: 0.6403109655485573
(INFO) 2023-04-10 19:04:27,017 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,017 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 124, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:27,019 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,019 [api2.py:131] throughput: 0.31656423226649244
(INFO) 2023-04-10 19:04:27,020 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,020 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 51, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:27,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,023 [api2.py:131] throughput: 0.6561216227184016
(INFO) 2023-04-10 19:04:27,023 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,023 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:27,026 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,026 [api2.py:131] throughput: 0.6153118735304712
(INFO) 2023-04-10 19:04:27,026 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,026 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,029 [api2.py:131] throughput: 1.5984397454881543
(INFO) 2023-04-10 19:04:27,030 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,030 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 19:04:27,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,032 [api2.py:131] throughput: 1.8106640013583502
(INFO) 2023-04-10 19:04:27,033 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:27,033 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:27,035 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,035 [api2.py:131] throughput: 0.41550313968798064
(INFO) 2023-04-10 19:04:27,041 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,041 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:27,043 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,043 [api2.py:131] throughput: 1.1618581401071348
(INFO) 2023-04-10 19:04:27,044 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,044 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 156, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:27,046 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,047 [api2.py:131] throughput: 0.6282785946577082
(INFO) 2023-04-10 19:04:27,047 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,047 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 216, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,049 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,049 [api2.py:131] throughput: 0.7015805761427909
(INFO) 2023-04-10 19:04:27,050 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,050 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 182, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:27,052 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,053 [api2.py:131] throughput: 0.6152221950067611
(INFO) 2023-04-10 19:04:27,053 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,053 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 476, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:27,055 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,055 [api2.py:131] throughput: 0.3825453933407716
(INFO) 2023-04-10 19:04:27,056 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,056 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 294, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,059 [api2.py:131] throughput: 0.9975798959937355
(INFO) 2023-04-10 19:04:27,059 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,059 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 386, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 184}
(INFO) 2023-04-10 19:04:27,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,062 [api2.py:131] throughput: 304.79098938701225
(INFO) 2023-04-10 19:04:27,062 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,063 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 252, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:27,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,065 [api2.py:131] throughput: 0.5178728457812665
(INFO) 2023-04-10 19:04:27,065 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,065 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,067 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,068 [api2.py:131] throughput: 1.5978796272064528
(INFO) 2023-04-10 19:04:27,068 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,068 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 304, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 86}
(INFO) 2023-04-10 19:04:27,070 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,071 [api2.py:131] throughput: 140.56734761331535
(INFO) 2023-04-10 19:04:27,071 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,071 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 452, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:27,073 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,074 [api2.py:131] throughput: 0.3464743015821147
(INFO) 2023-04-10 19:04:27,074 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,074 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,077 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,077 [api2.py:131] throughput: 7.136700434983721
(INFO) 2023-04-10 19:04:27,077 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,078 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 325, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:27,080 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,080 [api2.py:131] throughput: 0.6056724607629934
(INFO) 2023-04-10 19:04:27,081 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,081 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 494, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,083 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,083 [api2.py:131] throughput: 0.3775182582420073
(INFO) 2023-04-10 19:04:27,084 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,084 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 492, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 19:04:27,086 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,086 [api2.py:131] throughput: 27.09411258779112
(INFO) 2023-04-10 19:04:27,087 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,087 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 310, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 19:04:27,089 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,089 [api2.py:131] throughput: 87.30493981127606
(INFO) 2023-04-10 19:04:27,090 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,090 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 340, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:27,092 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,092 [api2.py:131] throughput: 0.7300449399312015
(INFO) 2023-04-10 19:04:27,092 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,093 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 510, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:27,095 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,095 [api2.py:131] throughput: 1.9776097785969322
(INFO) 2023-04-10 19:04:27,096 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,096 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 19:04:27,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,098 [api2.py:131] throughput: 218.21809650716514
(INFO) 2023-04-10 19:04:27,099 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 35}
(INFO) 2023-04-10 19:04:27,099 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 160, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:27,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,101 [api2.py:131] throughput: 10.120467595595851
(INFO) 2023-04-10 19:04:27,107 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:27,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:27,111 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:27,669 [api1.py:131] throughput: 0.06724683561637851
(INFO) 2023-04-10 19:04:27,670 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:27,671 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:27,675 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:28,060 [api1.py:131] throughput: 0.509486486236148
(INFO) 2023-04-10 19:04:28,061 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:28,061 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 179}
(INFO) 2023-04-10 19:04:28,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:28,370 [api1.py:131] throughput: 8.177913012079973
(INFO) 2023-04-10 19:04:28,371 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:28,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 64}
(INFO) 2023-04-10 19:04:28,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:28,842 [api1.py:131] throughput: 1.2811126547074434
(INFO) 2023-04-10 19:04:28,843 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:28,843 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 55}
(INFO) 2023-04-10 19:04:28,848 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:29,091 [api1.py:131] throughput: 0.9881141996459052
(INFO) 2023-04-10 19:04:29,092 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:29,092 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:29,097 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:29,472 [api1.py:131] throughput: 1.028999430553696
(INFO) 2023-04-10 19:04:29,473 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:29,473 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 226, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 19:04:29,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:29,840 [api1.py:131] throughput: 1.8280344545369418
(INFO) 2023-04-10 19:04:29,841 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:29,841 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 104, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 531}
(INFO) 2023-04-10 19:04:29,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:30,210 [api1.py:131] throughput: 16.631531802769466
(INFO) 2023-04-10 19:04:30,211 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:30,211 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 19:04:30,216 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:30,616 [api1.py:131] throughput: 2.233505428543209
(INFO) 2023-04-10 19:04:30,617 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:30,617 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 347, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 96}
(INFO) 2023-04-10 19:04:30,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:30,941 [api1.py:131] throughput: 0.6404009645785685
(INFO) 2023-04-10 19:04:30,942 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:30,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 246, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 59}
(INFO) 2023-04-10 19:04:30,947 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:31,234 [api1.py:131] throughput: 1.619017571313132
(INFO) 2023-04-10 19:04:31,236 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:31,236 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 90, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:31,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:31,696 [api1.py:131] throughput: 0.5804999132320138
(INFO) 2023-04-10 19:04:31,697 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:31,697 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 53}
(INFO) 2023-04-10 19:04:31,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:31,995 [api1.py:131] throughput: 2.60395367161126
(INFO) 2023-04-10 19:04:31,997 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:31,997 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 188, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 493}
(INFO) 2023-04-10 19:04:32,001 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:32,245 [api1.py:131] throughput: 12.525443830443107
(INFO) 2023-04-10 19:04:32,246 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:32,247 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 61}
(INFO) 2023-04-10 19:04:32,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:32,706 [api1.py:131] throughput: 1.6775380098377954
(INFO) 2023-04-10 19:04:32,707 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:32,707 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 338, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 116}
(INFO) 2023-04-10 19:04:32,712 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:32,977 [api1.py:131] throughput: 1.2634617461440178
(INFO) 2023-04-10 19:04:32,979 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:32,979 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:32,984 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:33,354 [api1.py:131] throughput: 0.7997310822312742
(INFO) 2023-04-10 19:04:33,356 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:33,356 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 107}
(INFO) 2023-04-10 19:04:33,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:33,537 [api1.py:131] throughput: 15.81425959589387
(INFO) 2023-04-10 19:04:33,538 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:33,538 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:33,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:34,025 [api1.py:131] throughput: 0.9515032226558918
(INFO) 2023-04-10 19:04:34,026 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 11}
(INFO) 2023-04-10 19:04:34,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 19:04:34,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:34,219 [api1.py:131] throughput: 3.3432055646794616
(INFO) 2023-04-10 19:04:34,225 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:34,225 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 43}
(INFO) 2023-04-10 19:04:34,230 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:34,497 [api1.py:131] throughput: 0.6368681210711208
(INFO) 2023-04-10 19:04:34,498 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:34,498 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 362, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:34,503 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:34,914 [api1.py:131] throughput: 0.06862094073359941
(INFO) 2023-04-10 19:04:34,916 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:34,916 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 410, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:34,920 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:35,401 [api1.py:131] throughput: 0.21544589196740765
(INFO) 2023-04-10 19:04:35,402 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:35,402 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:35,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:35,755 [api1.py:131] throughput: 0.2970944617435317
(INFO) 2023-04-10 19:04:35,756 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:35,756 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 147, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:35,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:36,171 [api1.py:131] throughput: 0.06543459360354632
(INFO) 2023-04-10 19:04:36,172 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:36,172 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 146, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:36,177 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:36,673 [api1.py:131] throughput: 0.15531958774223692
(INFO) 2023-04-10 19:04:36,674 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:36,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 118, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:36,679 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:37,095 [api1.py:131] throughput: 0.144140771661176
(INFO) 2023-04-10 19:04:37,096 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:37,096 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 19:04:37,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:37,584 [api1.py:131] throughput: 0.6497680368627716
(INFO) 2023-04-10 19:04:37,585 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:37,585 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:37,590 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:38,176 [api1.py:131] throughput: 0.04247280520822454
(INFO) 2023-04-10 19:04:38,178 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:38,178 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 150, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:38,183 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:38,678 [api1.py:131] throughput: 0.15904775359396883
(INFO) 2023-04-10 19:04:38,680 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:38,680 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 214}
(INFO) 2023-04-10 19:04:38,686 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:39,023 [api1.py:131] throughput: 1.2748388237499118
(INFO) 2023-04-10 19:04:39,025 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:39,025 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 453, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 19:04:39,030 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:39,465 [api1.py:131] throughput: 0.33304879714717034
(INFO) 2023-04-10 19:04:39,467 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:39,467 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 476, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 19:04:39,593 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:40,009 [api1.py:131] throughput: 0.08794280352570606
(INFO) 2023-04-10 19:04:40,010 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:40,010 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 186, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 67}
(INFO) 2023-04-10 19:04:40,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:40,400 [api1.py:131] throughput: 0.6825656415131235
(INFO) 2023-04-10 19:04:40,401 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:40,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 315, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:40,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:40,812 [api1.py:131] throughput: 0.25340245204181305
(INFO) 2023-04-10 19:04:40,813 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:40,813 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 254, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 19:04:40,818 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:41,406 [api1.py:131] throughput: 0.25796012370968946
(INFO) 2023-04-10 19:04:41,407 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:41,407 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 311, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:41,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:41,902 [api1.py:131] throughput: 0.03412701927115882
(INFO) 2023-04-10 19:04:41,903 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:41,903 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 402, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 19:04:41,908 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:42,317 [api1.py:131] throughput: 0.1928200653363459
(INFO) 2023-04-10 19:04:42,318 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:42,318 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 439, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 68}
(INFO) 2023-04-10 19:04:42,323 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:42,917 [api1.py:131] throughput: 0.4538412468799048
(INFO) 2023-04-10 19:04:42,918 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:42,918 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 19:04:42,923 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,413 [api1.py:131] throughput: 0.49458670481133243
(INFO) 2023-04-10 19:04:43,420 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,420 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 174, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,422 [api2.py:131] throughput: 0.5869016032636811
(INFO) 2023-04-10 19:04:43,422 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,422 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:43,423 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,424 [api2.py:131] throughput: 0.8811533263020452
(INFO) 2023-04-10 19:04:43,424 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,424 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 207, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,425 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,426 [api2.py:131] throughput: 1.1443754295527613
(INFO) 2023-04-10 19:04:43,426 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,426 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 157, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,427 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,427 [api2.py:131] throughput: 1.3409907904614562
(INFO) 2023-04-10 19:04:43,428 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,428 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 133, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,430 [api2.py:131] throughput: 0.3663230781522994
(INFO) 2023-04-10 19:04:43,430 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,430 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,431 [api2.py:131] throughput: 1.738773108340977
(INFO) 2023-04-10 19:04:43,432 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,432 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 239, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:43,433 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,433 [api2.py:131] throughput: 0.519674436243868
(INFO) 2023-04-10 19:04:43,434 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,434 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 88, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,435 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,435 [api2.py:131] throughput: 2.4698111362427384
(INFO) 2023-04-10 19:04:43,436 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,436 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 341, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 19:04:43,437 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,437 [api2.py:131] throughput: 0.6171187033509604
(INFO) 2023-04-10 19:04:43,438 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,438 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 135, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:43,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,439 [api2.py:131] throughput: 0.830178919098015
(INFO) 2023-04-10 19:04:43,440 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,440 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,442 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,442 [api2.py:131] throughput: 11.481406367569605
(INFO) 2023-04-10 19:04:43,443 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,443 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 410, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,443 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,444 [api2.py:131] throughput: 0.6597424230465179
(INFO) 2023-04-10 19:04:43,444 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,445 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 19:04:43,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,446 [api2.py:131] throughput: 0.6822168422429448
(INFO) 2023-04-10 19:04:43,446 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,446 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 140, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:43,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,448 [api2.py:131] throughput: 0.7116264597337383
(INFO) 2023-04-10 19:04:43,448 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,449 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,449 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,450 [api2.py:131] throughput: 1.1781512473247482
(INFO) 2023-04-10 19:04:43,450 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,451 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:43,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,452 [api2.py:131] throughput: 1.0208496814469874
(INFO) 2023-04-10 19:04:43,452 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,452 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:43,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,454 [api2.py:131] throughput: 0.4782524802681443
(INFO) 2023-04-10 19:04:43,454 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,454 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 117, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 19:04:43,455 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,456 [api2.py:131] throughput: 0.6376469362992956
(INFO) 2023-04-10 19:04:43,456 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,456 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 152, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,458 [api2.py:131] throughput: 0.8083519945522853
(INFO) 2023-04-10 19:04:43,458 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:43,458 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 160, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,460 [api2.py:131] throughput: 0.9059996251915792
(INFO) 2023-04-10 19:04:43,466 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,466 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 26, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:43,473 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,473 [api2.py:131] throughput: 3.3039147228421073
(INFO) 2023-04-10 19:04:43,474 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,474 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 473, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 19:04:43,480 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,481 [api2.py:131] throughput: 0.35600316733994386
(INFO) 2023-04-10 19:04:43,481 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,481 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 471, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 116}
(INFO) 2023-04-10 19:04:43,488 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,488 [api2.py:131] throughput: 0.5965747537119284
(INFO) 2023-04-10 19:04:43,489 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,489 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 354, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:43,494 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,495 [api2.py:131] throughput: 0.4946526732227126
(INFO) 2023-04-10 19:04:43,495 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,495 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,502 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,502 [api2.py:131] throughput: 3.8026890864940563
(INFO) 2023-04-10 19:04:43,503 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,503 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 88, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,510 [api2.py:131] throughput: 1.556637578348932
(INFO) 2023-04-10 19:04:43,510 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,510 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 503, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 68}
(INFO) 2023-04-10 19:04:43,516 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,517 [api2.py:131] throughput: 0.22427843600643224
(INFO) 2023-04-10 19:04:43,518 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,518 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:43,524 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,524 [api2.py:131] throughput: 0.9108258740629014
(INFO) 2023-04-10 19:04:43,525 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,525 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,531 [api2.py:131] throughput: 1.9020709895012744
(INFO) 2023-04-10 19:04:43,531 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,532 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 205, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,537 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,538 [api2.py:131] throughput: 0.5735163011550725
(INFO) 2023-04-10 19:04:43,538 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,538 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 165, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:43,681 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,682 [api2.py:131] throughput: 17.20386775978666
(INFO) 2023-04-10 19:04:43,683 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,683 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 432, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,688 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,688 [api2.py:131] throughput: 0.44587998108293186
(INFO) 2023-04-10 19:04:43,689 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,689 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 387, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 19:04:43,695 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,695 [api2.py:131] throughput: 0.7565263981905435
(INFO) 2023-04-10 19:04:43,696 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,696 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 472, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 19:04:43,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,702 [api2.py:131] throughput: 0.4670526355535553
(INFO) 2023-04-10 19:04:43,703 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,703 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 425, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,709 [api2.py:131] throughput: 0.43983148866364946
(INFO) 2023-04-10 19:04:43,710 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,710 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 387, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 19:04:43,716 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,716 [api2.py:131] throughput: 0.2537178320862868
(INFO) 2023-04-10 19:04:43,717 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,717 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,723 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,723 [api2.py:131] throughput: 8.072819362283953
(INFO) 2023-04-10 19:04:43,724 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,724 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 474, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,730 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,730 [api2.py:131] throughput: 0.3695764846021026
(INFO) 2023-04-10 19:04:43,731 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,731 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 510, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 19:04:43,736 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,737 [api2.py:131] throughput: 0.3900828097996656
(INFO) 2023-04-10 19:04:43,737 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:43,737 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 220, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,743 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,743 [api2.py:131] throughput: 0.7173236117536289
(INFO) 2023-04-10 19:04:43,749 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,749 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 109, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:43,750 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,750 [api2.py:131] throughput: 2.022059113527806
(INFO) 2023-04-10 19:04:43,751 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,751 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 195, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:43,752 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,752 [api2.py:131] throughput: 0.5508025748724887
(INFO) 2023-04-10 19:04:43,753 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,753 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,755 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,755 [api2.py:131] throughput: 2.1467787556148283
(INFO) 2023-04-10 19:04:43,756 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,756 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,757 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,757 [api2.py:131] throughput: 1.5433998709727585
(INFO) 2023-04-10 19:04:43,758 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,758 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 467, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,759 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,759 [api2.py:131] throughput: 0.3132680966161127
(INFO) 2023-04-10 19:04:43,760 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,760 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 465, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,762 [api2.py:131] throughput: 0.3869995111793694
(INFO) 2023-04-10 19:04:43,762 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,762 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 187, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,764 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,764 [api2.py:131] throughput: 0.9973419194227899
(INFO) 2023-04-10 19:04:43,764 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,765 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:43,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,767 [api2.py:131] throughput: 0.3662169800801714
(INFO) 2023-04-10 19:04:43,767 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,767 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,769 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,769 [api2.py:131] throughput: 0.7079938225420664
(INFO) 2023-04-10 19:04:43,769 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,769 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 370, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,771 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,771 [api2.py:131] throughput: 5.2040867985057515
(INFO) 2023-04-10 19:04:43,772 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,772 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 67, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,773 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,773 [api2.py:131] throughput: 17.42291470662748
(INFO) 2023-04-10 19:04:43,774 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,774 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,775 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,775 [api2.py:131] throughput: 2.562994719492735
(INFO) 2023-04-10 19:04:43,776 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,776 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 174, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,778 [api2.py:131] throughput: 1.0516444448081919
(INFO) 2023-04-10 19:04:43,779 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,779 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 269, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,780 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,780 [api2.py:131] throughput: 0.7728608530603146
(INFO) 2023-04-10 19:04:43,781 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,781 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 239, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:43,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,783 [api2.py:131] throughput: 0.47050836558123943
(INFO) 2023-04-10 19:04:43,783 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,783 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 408, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 44}
(INFO) 2023-04-10 19:04:43,784 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,785 [api2.py:131] throughput: 91.72638910710505
(INFO) 2023-04-10 19:04:43,785 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,786 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,787 [api2.py:131] throughput: 0.5847499897208749
(INFO) 2023-04-10 19:04:43,788 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,788 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 215, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,790 [api2.py:131] throughput: 0.7523981396576789
(INFO) 2023-04-10 19:04:43,791 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,791 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,792 [api2.py:131] throughput: 1.538930964492603
(INFO) 2023-04-10 19:04:43,793 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,793 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,794 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,794 [api2.py:131] throughput: 5.005406586587565
(INFO) 2023-04-10 19:04:43,800 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,800 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:43,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,802 [api2.py:131] throughput: 0.37684985349585914
(INFO) 2023-04-10 19:04:43,803 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,803 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 137, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,805 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,806 [api2.py:131] throughput: 1.1188833393506534
(INFO) 2023-04-10 19:04:43,806 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,806 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:43,808 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,809 [api2.py:131] throughput: 2.2023298426119124
(INFO) 2023-04-10 19:04:43,809 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,809 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 19:04:43,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,812 [api2.py:131] throughput: 26.144854848535843
(INFO) 2023-04-10 19:04:43,812 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,812 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 164, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:43,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,815 [api2.py:131] throughput: 1.4094324870251418
(INFO) 2023-04-10 19:04:43,816 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,816 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 189, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,818 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,818 [api2.py:131] throughput: 0.7845756806960317
(INFO) 2023-04-10 19:04:43,819 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,819 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,822 [api2.py:131] throughput: 1.2055263662791116
(INFO) 2023-04-10 19:04:43,822 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,822 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,824 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,824 [api2.py:131] throughput: 2.663218290169935
(INFO) 2023-04-10 19:04:43,825 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,825 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 78, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 59}
(INFO) 2023-04-10 19:04:43,827 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,828 [api2.py:131] throughput: 1.9037894295129443
(INFO) 2023-04-10 19:04:43,828 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,828 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 53, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,831 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,831 [api2.py:131] throughput: 0.9090179729887187
(INFO) 2023-04-10 19:04:43,832 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,832 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,834 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,834 [api2.py:131] throughput: 1.1933595322426678
(INFO) 2023-04-10 19:04:43,835 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,835 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,837 [api2.py:131] throughput: 5.439785216256985
(INFO) 2023-04-10 19:04:43,838 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,838 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 109, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 94}
(INFO) 2023-04-10 19:04:43,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,840 [api2.py:131] throughput: 1.4676036972262745
(INFO) 2023-04-10 19:04:43,841 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,841 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 19:04:43,843 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,844 [api2.py:131] throughput: 1.4240441685415708
(INFO) 2023-04-10 19:04:43,844 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,844 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,847 [api2.py:131] throughput: 1.4282120622769967
(INFO) 2023-04-10 19:04:43,848 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,848 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,850 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,850 [api2.py:131] throughput: 2.928554096559128
(INFO) 2023-04-10 19:04:43,851 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,851 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 326, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 19:04:43,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,853 [api2.py:131] throughput: 1.0371975373538593
(INFO) 2023-04-10 19:04:43,854 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,854 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,856 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,856 [api2.py:131] throughput: 1.1752037044031207
(INFO) 2023-04-10 19:04:43,857 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,857 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 19:04:43,859 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,859 [api2.py:131] throughput: 0.9337385623897748
(INFO) 2023-04-10 19:04:43,860 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-10 19:04:43,860 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 157, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,863 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,863 [api2.py:131] throughput: 0.3116982877075343
(INFO) 2023-04-10 19:04:43,869 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,869 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 406, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,871 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,871 [api2.py:131] throughput: 0.7722903154897093
(INFO) 2023-04-10 19:04:43,872 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,872 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,873 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,873 [api2.py:131] throughput: 0.4972058268660309
(INFO) 2023-04-10 19:04:43,874 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,874 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 19:04:43,875 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,876 [api2.py:131] throughput: 0.6643233247256872
(INFO) 2023-04-10 19:04:43,876 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,876 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 274, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 50}
(INFO) 2023-04-10 19:04:43,878 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,878 [api2.py:131] throughput: 52.223555991488446
(INFO) 2023-04-10 19:04:43,879 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,879 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 19:04:43,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,881 [api2.py:131] throughput: 1.8194966847135654
(INFO) 2023-04-10 19:04:43,881 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,881 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 218, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 41}
(INFO) 2023-04-10 19:04:43,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,883 [api2.py:131] throughput: 1.025781150083912
(INFO) 2023-04-10 19:04:43,884 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,884 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:43,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,885 [api2.py:131] throughput: 0.39959796199039266
(INFO) 2023-04-10 19:04:43,886 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,886 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 76, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,887 [api2.py:131] throughput: 0.4790467035454998
(INFO) 2023-04-10 19:04:43,888 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,888 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 78, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:43,890 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,890 [api2.py:131] throughput: 1.0969833872241779
(INFO) 2023-04-10 19:04:43,891 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,891 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 69, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 19:04:43,893 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,893 [api2.py:131] throughput: 1.3133269540225871
(INFO) 2023-04-10 19:04:43,893 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,894 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 292, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,895 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,895 [api2.py:131] throughput: 0.5444611841110967
(INFO) 2023-04-10 19:04:43,896 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,896 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:43,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,897 [api2.py:131] throughput: 2.0464276169207594
(INFO) 2023-04-10 19:04:43,898 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,898 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 96, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:43,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,900 [api2.py:131] throughput: 0.7407501575112901
(INFO) 2023-04-10 19:04:43,901 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,901 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 108, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:43,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,903 [api2.py:131] throughput: 1.4785224712414369
(INFO) 2023-04-10 19:04:43,903 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,904 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 118, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:43,905 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,905 [api2.py:131] throughput: 0.9069140386538688
(INFO) 2023-04-10 19:04:43,906 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,906 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,907 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,908 [api2.py:131] throughput: 1.7907676667682753
(INFO) 2023-04-10 19:04:43,908 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,908 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,910 [api2.py:131] throughput: 0.8150780985463267
(INFO) 2023-04-10 19:04:43,911 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,911 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:43,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,913 [api2.py:131] throughput: 2.5507039126573683
(INFO) 2023-04-10 19:04:43,914 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,914 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:43,915 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,916 [api2.py:131] throughput: 0.3821000078735645
(INFO) 2023-04-10 19:04:43,916 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:43,916 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 134, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:43,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,918 [api2.py:131] throughput: 0.8254689224290285
(INFO) 2023-04-10 19:04:43,924 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:43,924 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 251, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 83}
(INFO) 2023-04-10 19:04:43,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,930 [api2.py:131] throughput: 0.9802673900007041
(INFO) 2023-04-10 19:04:43,931 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:43,931 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 94, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 53}
(INFO) 2023-04-10 19:04:43,937 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,937 [api2.py:131] throughput: 2.1171297212275744
(INFO) 2023-04-10 19:04:43,938 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:43,938 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 107, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 210}
(INFO) 2023-04-10 19:04:43,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,944 [api2.py:131] throughput: 1.5779045672510925
(INFO) 2023-04-10 19:04:43,944 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:43,944 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:43,950 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,951 [api2.py:131] throughput: 1.4492268974218692
(INFO) 2023-04-10 19:04:43,951 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:43,951 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 93, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 19:04:43,957 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:43,958 [api2.py:131] throughput: 1.1743301120168732
(INFO) 2023-04-10 19:04:43,958 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:43,958 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 58, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 92}
(INFO) 2023-04-10 19:04:44,087 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,087 [api2.py:131] throughput: 86.31488368274509
(INFO) 2023-04-10 19:04:44,088 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,088 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 113}
(INFO) 2023-04-10 19:04:44,094 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,095 [api2.py:131] throughput: 1.296233459193095
(INFO) 2023-04-10 19:04:44,096 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,096 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,102 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,102 [api2.py:131] throughput: 1.0519875443205795
(INFO) 2023-04-10 19:04:44,103 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,103 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 147, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 19:04:44,109 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,109 [api2.py:131] throughput: 1.6455867473836885
(INFO) 2023-04-10 19:04:44,110 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,110 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 113, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 75}
(INFO) 2023-04-10 19:04:44,116 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,116 [api2.py:131] throughput: 0.44116692998298285
(INFO) 2023-04-10 19:04:44,117 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,117 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 251}
(INFO) 2023-04-10 19:04:44,123 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,123 [api2.py:131] throughput: 115.20048481985094
(INFO) 2023-04-10 19:04:44,124 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,124 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 212, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 154}
(INFO) 2023-04-10 19:04:44,129 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,130 [api2.py:131] throughput: 0.8787159292592387
(INFO) 2023-04-10 19:04:44,130 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,130 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 60, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 37}
(INFO) 2023-04-10 19:04:44,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,137 [api2.py:131] throughput: 1.9722323651241445
(INFO) 2023-04-10 19:04:44,137 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,137 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 473, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 73}
(INFO) 2023-04-10 19:04:44,143 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,143 [api2.py:131] throughput: 0.3951437585195379
(INFO) 2023-04-10 19:04:44,144 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,144 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 628}
(INFO) 2023-04-10 19:04:44,150 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,150 [api2.py:131] throughput: 265.570173522069
(INFO) 2023-04-10 19:04:44,151 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,151 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,157 [api2.py:131] throughput: 0.8505509085660272
(INFO) 2023-04-10 19:04:44,158 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,158 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 19:04:44,163 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,164 [api2.py:131] throughput: 1.8869021801805244
(INFO) 2023-04-10 19:04:44,164 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,164 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 19:04:44,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,170 [api2.py:131] throughput: 0.4280621000858895
(INFO) 2023-04-10 19:04:44,171 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,171 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:44,176 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,177 [api2.py:131] throughput: 1.3700072988409215
(INFO) 2023-04-10 19:04:44,177 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 22}
(INFO) 2023-04-10 19:04:44,177 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 387, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:44,183 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,183 [api2.py:131] throughput: 0.4530833944228146
(INFO) 2023-04-10 19:04:44,189 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,189 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 191, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,190 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,191 [api2.py:131] throughput: 1.1360617803223902
(INFO) 2023-04-10 19:04:44,191 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,192 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 344, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,193 [api2.py:131] throughput: 0.6810207308518884
(INFO) 2023-04-10 19:04:44,194 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,194 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 269, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:44,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,195 [api2.py:131] throughput: 0.4193692110397351
(INFO) 2023-04-10 19:04:44,196 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,196 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 173, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,198 [api2.py:131] throughput: 0.7098843281351664
(INFO) 2023-04-10 19:04:44,198 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,198 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 476, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 19:04:44,199 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,200 [api2.py:131] throughput: 112.00893818525697
(INFO) 2023-04-10 19:04:44,200 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,200 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 468, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,201 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,202 [api2.py:131] throughput: 0.4177022661772287
(INFO) 2023-04-10 19:04:44,202 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,203 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 511, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,204 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,204 [api2.py:131] throughput: 0.31226046486994496
(INFO) 2023-04-10 19:04:44,205 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,205 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 479, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,206 [api2.py:131] throughput: 0.36487153232559116
(INFO) 2023-04-10 19:04:44,207 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,207 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,208 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,209 [api2.py:131] throughput: 1.7950349023948822
(INFO) 2023-04-10 19:04:44,209 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,209 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,211 [api2.py:131] throughput: 0.9253919332322418
(INFO) 2023-04-10 19:04:44,211 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,212 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 174, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,213 [api2.py:131] throughput: 1.0473179215000414
(INFO) 2023-04-10 19:04:44,214 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,214 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,215 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,215 [api2.py:131] throughput: 1.568360817139779
(INFO) 2023-04-10 19:04:44,216 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,216 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,217 [api2.py:131] throughput: 1.5718869108479576
(INFO) 2023-04-10 19:04:44,218 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,218 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 241, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:44,220 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,220 [api2.py:131] throughput: 0.4459357567079421
(INFO) 2023-04-10 19:04:44,221 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,221 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 414, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 19:04:44,222 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,222 [api2.py:131] throughput: 0.3406201637164229
(INFO) 2023-04-10 19:04:44,223 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,223 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 259, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:44,224 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,225 [api2.py:131] throughput: 1.2257498349087679
(INFO) 2023-04-10 19:04:44,225 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,225 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 346, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:44,227 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,227 [api2.py:131] throughput: 60.761229598710344
(INFO) 2023-04-10 19:04:44,228 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,228 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 200, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,229 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,229 [api2.py:131] throughput: 0.7976002982544342
(INFO) 2023-04-10 19:04:44,230 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,230 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 108, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,232 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,232 [api2.py:131] throughput: 1.551833765146977
(INFO) 2023-04-10 19:04:44,233 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-10 19:04:44,233 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 291, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,234 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,234 [api2.py:131] throughput: 0.6744336284383797
(INFO) 2023-04-10 19:04:44,240 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,240 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 55}
(INFO) 2023-04-10 19:04:44,243 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,244 [api2.py:131] throughput: 174.44297231166195
(INFO) 2023-04-10 19:04:44,244 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,244 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 154, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 19:04:44,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,248 [api2.py:131] throughput: 31.487121079993326
(INFO) 2023-04-10 19:04:44,248 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,248 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,251 [api2.py:131] throughput: 3.9607292581636564
(INFO) 2023-04-10 19:04:44,252 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,252 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 154, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:44,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,255 [api2.py:131] throughput: 0.331738057740616
(INFO) 2023-04-10 19:04:44,256 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,256 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:44,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,259 [api2.py:131] throughput: 1.0055534538329147
(INFO) 2023-04-10 19:04:44,260 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,260 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 19:04:44,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,263 [api2.py:131] throughput: 0.9956113793501243
(INFO) 2023-04-10 19:04:44,263 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,263 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:44,266 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,266 [api2.py:131] throughput: 3.262583243809601
(INFO) 2023-04-10 19:04:44,267 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,267 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 238, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:44,269 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,269 [api2.py:131] throughput: 0.4122986499713546
(INFO) 2023-04-10 19:04:44,270 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,270 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 39, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,273 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,273 [api2.py:131] throughput: 0.9390845987537558
(INFO) 2023-04-10 19:04:44,274 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,274 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 389, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 19:04:44,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,277 [api2.py:131] throughput: 21.981317859649398
(INFO) 2023-04-10 19:04:44,278 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,278 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 23, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 19:04:44,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,281 [api2.py:131] throughput: 2.474967904725872
(INFO) 2023-04-10 19:04:44,282 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,282 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 142}
(INFO) 2023-04-10 19:04:44,285 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,285 [api2.py:131] throughput: 1.4226495692282513
(INFO) 2023-04-10 19:04:44,286 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,286 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 399, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 19:04:44,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,408 [api2.py:131] throughput: 0.8305356599077155
(INFO) 2023-04-10 19:04:44,409 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,409 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:44,412 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,412 [api2.py:131] throughput: 1.2075963709619315
(INFO) 2023-04-10 19:04:44,413 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,413 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 187, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 19:04:44,416 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,416 [api2.py:131] throughput: 1.1803148599277775
(INFO) 2023-04-10 19:04:44,417 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,417 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 159, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,420 [api2.py:131] throughput: 0.9909922272877889
(INFO) 2023-04-10 19:04:44,420 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,420 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 246, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 19:04:44,423 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,424 [api2.py:131] throughput: 1.1976337637795762
(INFO) 2023-04-10 19:04:44,424 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,424 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 126, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,427 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,427 [api2.py:131] throughput: 1.0730820939003496
(INFO) 2023-04-10 19:04:44,428 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,428 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,431 [api2.py:131] throughput: 5.952494654186531
(INFO) 2023-04-10 19:04:44,432 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 25}
(INFO) 2023-04-10 19:04:44,432 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:44,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,435 [api2.py:131] throughput: 0.4235269776047195
(INFO) 2023-04-10 19:04:44,440 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,441 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 404, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,443 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,443 [api2.py:131] throughput: 0.6445075417900752
(INFO) 2023-04-10 19:04:44,444 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,444 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:44,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,446 [api2.py:131] throughput: 1.2189227982191306
(INFO) 2023-04-10 19:04:44,446 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,446 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 109, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,448 [api2.py:131] throughput: 0.37517611066775636
(INFO) 2023-04-10 19:04:44,449 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,449 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,451 [api2.py:131] throughput: 0.5357621087585629
(INFO) 2023-04-10 19:04:44,452 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,452 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 86, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,454 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,454 [api2.py:131] throughput: 1.726003595699062
(INFO) 2023-04-10 19:04:44,455 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,455 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 202, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,457 [api2.py:131] throughput: 0.5206425976531583
(INFO) 2023-04-10 19:04:44,457 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,457 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 467, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:44,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,460 [api2.py:131] throughput: 17.442927160959595
(INFO) 2023-04-10 19:04:44,460 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,460 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,462 [api2.py:131] throughput: 0.3830800807468575
(INFO) 2023-04-10 19:04:44,463 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,463 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 346, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,465 [api2.py:131] throughput: 0.4681516927736014
(INFO) 2023-04-10 19:04:44,466 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,466 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 325, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,468 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,468 [api2.py:131] throughput: 0.703151601982982
(INFO) 2023-04-10 19:04:44,469 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,469 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 19:04:44,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,471 [api2.py:131] throughput: 0.7815016223101972
(INFO) 2023-04-10 19:04:44,471 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,471 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 510, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 88}
(INFO) 2023-04-10 19:04:44,473 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,473 [api2.py:131] throughput: 97.31379391251333
(INFO) 2023-04-10 19:04:44,474 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,474 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 401, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:44,476 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,476 [api2.py:131] throughput: 0.5199397579328199
(INFO) 2023-04-10 19:04:44,477 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,477 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:44,479 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,479 [api2.py:131] throughput: 0.9282494704938278
(INFO) 2023-04-10 19:04:44,480 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,480 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 326, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:44,482 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,482 [api2.py:131] throughput: 0.5175844253333878
(INFO) 2023-04-10 19:04:44,483 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,483 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:44,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,485 [api2.py:131] throughput: 25.595687409247926
(INFO) 2023-04-10 19:04:44,486 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,486 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 366, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,488 [api2.py:131] throughput: 0.5304296652742014
(INFO) 2023-04-10 19:04:44,488 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,488 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,490 [api2.py:131] throughput: 0.9903481713032455
(INFO) 2023-04-10 19:04:44,491 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,491 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 19:04:44,494 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,494 [api2.py:131] throughput: 246.32522181093574
(INFO) 2023-04-10 19:04:44,495 [api2.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,495 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 368, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 19:04:44,496 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,497 [api2.py:131] throughput: 0.75942615786219
(INFO) 2023-04-10 19:04:44,502 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,502 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 130, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:44,504 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,505 [api2.py:131] throughput: 0.5290793055842533
(INFO) 2023-04-10 19:04:44,506 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,506 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 205, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:44,508 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,509 [api2.py:131] throughput: 0.8037454096827299
(INFO) 2023-04-10 19:04:44,509 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,509 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 148, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:44,511 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,512 [api2.py:131] throughput: 0.6577417993483388
(INFO) 2023-04-10 19:04:44,512 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,513 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 105, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:44,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,515 [api2.py:131] throughput: 8.88217776084389
(INFO) 2023-04-10 19:04:44,515 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,516 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,518 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,518 [api2.py:131] throughput: 2.176688081604723
(INFO) 2023-04-10 19:04:44,519 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,519 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:44,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,522 [api2.py:131] throughput: 0.7581761498750597
(INFO) 2023-04-10 19:04:44,522 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,522 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 83, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 19:04:44,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,525 [api2.py:131] throughput: 1.4763690093324546
(INFO) 2023-04-10 19:04:44,526 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,526 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 47, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 55}
(INFO) 2023-04-10 19:04:44,528 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,528 [api2.py:131] throughput: 0.4832735649268321
(INFO) 2023-04-10 19:04:44,529 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,529 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 103, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 19:04:44,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,531 [api2.py:131] throughput: 1.0899586840547812
(INFO) 2023-04-10 19:04:44,532 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,532 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 19:04:44,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,535 [api2.py:131] throughput: 1.8111383858980437
(INFO) 2023-04-10 19:04:44,535 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,535 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 310, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,538 [api2.py:131] throughput: 0.5495048920479088
(INFO) 2023-04-10 19:04:44,538 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,539 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 139}
(INFO) 2023-04-10 19:04:44,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,541 [api2.py:131] throughput: 231.51845175301844
(INFO) 2023-04-10 19:04:44,542 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,542 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,544 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,544 [api2.py:131] throughput: 1.6952476293459886
(INFO) 2023-04-10 19:04:44,545 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,545 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 111, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,547 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,547 [api2.py:131] throughput: 1.1175642854577652
(INFO) 2023-04-10 19:04:44,548 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,548 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 19:04:44,550 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,551 [api2.py:131] throughput: 0.6852847286931897
(INFO) 2023-04-10 19:04:44,551 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,551 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 22}
(INFO) 2023-04-10 19:04:44,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,554 [api2.py:131] throughput: 0.8495387011031471
(INFO) 2023-04-10 19:04:44,555 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,555 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 94, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,557 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,557 [api2.py:131] throughput: 1.7447322738652373
(INFO) 2023-04-10 19:04:44,558 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,558 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 355, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,560 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,561 [api2.py:131] throughput: 0.4924827569515912
(INFO) 2023-04-10 19:04:44,561 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,561 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 125, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,563 [api2.py:131] throughput: 1.2163172233106083
(INFO) 2023-04-10 19:04:44,564 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-10 19:04:44,564 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 159, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 19:04:44,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,567 [api2.py:131] throughput: 0.8922732666717328
(INFO) 2023-04-10 19:04:44,573 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:44,573 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 123, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:44,574 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,643 [api1.py:131] throughput: 0.29955792484677635
(INFO) 2023-04-10 19:04:44,643 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:44,643 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 232, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,712 [api1.py:131] throughput: 0.08716988249453614
(INFO) 2023-04-10 19:04:44,713 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:44,713 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 109, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:44,715 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,787 [api1.py:131] throughput: 0.18841419119273722
(INFO) 2023-04-10 19:04:44,788 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:44,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 222, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:44,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,873 [api1.py:131] throughput: 0.19946371938861265
(INFO) 2023-04-10 19:04:44,874 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:44,874 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 261, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:44,876 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:44,950 [api1.py:131] throughput: 0.2625359770901853
(INFO) 2023-04-10 19:04:44,951 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:44,951 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 230, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:44,952 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,010 [api1.py:131] throughput: 0.8027733293955945
(INFO) 2023-04-10 19:04:45,011 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,011 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:45,012 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,081 [api1.py:131] throughput: 0.22818941422433037
(INFO) 2023-04-10 19:04:45,082 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,082 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 76, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:45,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,155 [api1.py:131] throughput: 0.15281469960555488
(INFO) 2023-04-10 19:04:45,156 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,156 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:45,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,228 [api1.py:131] throughput: 0.2979826483052703
(INFO) 2023-04-10 19:04:45,229 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,229 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 494, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:45,231 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,304 [api1.py:131] throughput: 0.06826259998965667
(INFO) 2023-04-10 19:04:45,305 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 132, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:45,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,378 [api1.py:131] throughput: 0.24378612266344488
(INFO) 2023-04-10 19:04:45,379 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,379 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 493, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:45,380 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,455 [api1.py:131] throughput: 0.0524926046738772
(INFO) 2023-04-10 19:04:45,456 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 314, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:45,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,526 [api1.py:131] throughput: 0.1216389247080735
(INFO) 2023-04-10 19:04:45,527 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,527 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:45,528 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,591 [api1.py:131] throughput: 0.8088228707017581
(INFO) 2023-04-10 19:04:45,592 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,593 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 185, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:45,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,651 [api1.py:131] throughput: 0.18575202414135372
(INFO) 2023-04-10 19:04:45,659 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 217, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:45,661 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:45,765 [api1.py:131] throughput: 0.12085594670629339
(INFO) 2023-04-10 19:04:45,766 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:45,767 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 472, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:45,768 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,005 [api1.py:131] throughput: 0.2501930161713106
(INFO) 2023-04-10 19:04:46,007 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:46,007 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 292, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:46,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,097 [api1.py:131] throughput: 0.1808514073956324
(INFO) 2023-04-10 19:04:46,099 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:46,099 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,183 [api1.py:131] throughput: 0.44221493914457055
(INFO) 2023-04-10 19:04:46,184 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-10 19:04:46,185 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 188, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:46,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,259 [api1.py:131] throughput: 0.1777051554997831
(INFO) 2023-04-10 19:04:46,271 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,271 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 361, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,274 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,274 [api2.py:131] throughput: 0.4721544635539975
(INFO) 2023-04-10 19:04:46,275 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,275 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,279 [api2.py:131] throughput: 1.257547052943914
(INFO) 2023-04-10 19:04:46,280 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,280 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,283 [api2.py:131] throughput: 2.901370455935267
(INFO) 2023-04-10 19:04:46,284 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,284 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 240, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-10 19:04:46,287 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,287 [api2.py:131] throughput: 0.6914283523172408
(INFO) 2023-04-10 19:04:46,288 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,288 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,292 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,292 [api2.py:131] throughput: 3.8323319807551934
(INFO) 2023-04-10 19:04:46,293 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,293 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 8, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:46,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,296 [api2.py:131] throughput: 2.1069925885642333
(INFO) 2023-04-10 19:04:46,297 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,297 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 242, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:46,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,300 [api2.py:131] throughput: 0.2609629052726839
(INFO) 2023-04-10 19:04:46,301 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,301 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 52}
(INFO) 2023-04-10 19:04:46,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,305 [api2.py:131] throughput: 22.032302777997604
(INFO) 2023-04-10 19:04:46,305 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,306 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,308 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,309 [api2.py:131] throughput: 3.1150003859610083
(INFO) 2023-04-10 19:04:46,309 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,310 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:46,313 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,313 [api2.py:131] throughput: 0.3817140235375641
(INFO) 2023-04-10 19:04:46,314 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,314 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 80, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 68}
(INFO) 2023-04-10 19:04:46,317 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,317 [api2.py:131] throughput: 23.137373347744848
(INFO) 2023-04-10 19:04:46,318 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,318 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 72}
(INFO) 2023-04-10 19:04:46,321 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,321 [api2.py:131] throughput: 43.0229736975928
(INFO) 2023-04-10 19:04:46,322 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,322 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:46,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,326 [api2.py:131] throughput: 3.940119265203692
(INFO) 2023-04-10 19:04:46,327 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,327 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 72, 'data_parallel_size': 7, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:46,330 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,330 [api2.py:131] throughput: 1.5022141379684228
(INFO) 2023-04-10 19:04:46,331 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,331 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 3, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:46,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,334 [api2.py:131] throughput: 2.738496162875511
(INFO) 2023-04-10 19:04:46,335 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,335 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 297, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 94}
(INFO) 2023-04-10 19:04:46,338 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,338 [api2.py:131] throughput: 1.1067311810052765
(INFO) 2023-04-10 19:04:46,339 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,339 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:46,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,343 [api2.py:131] throughput: 0.7902323863960043
(INFO) 2023-04-10 19:04:46,343 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,344 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 8, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 19:04:46,347 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,347 [api2.py:131] throughput: 2.7648552061696474
(INFO) 2023-04-10 19:04:46,348 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,348 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:46,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,351 [api2.py:131] throughput: 2.7217281092982106
(INFO) 2023-04-10 19:04:46,352 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-10 19:04:46,352 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 106, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,356 [api2.py:131] throughput: 1.4356565448034575
(INFO) 2023-04-10 19:04:46,363 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,363 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 19:04:46,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,373 [api2.py:131] throughput: 1.673909349815334
(INFO) 2023-04-10 19:04:46,374 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,374 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 173, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 75}
(INFO) 2023-04-10 19:04:46,385 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,385 [api2.py:131] throughput: 37.80525197709095
(INFO) 2023-04-10 19:04:46,386 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,387 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 136, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 19:04:46,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,397 [api2.py:131] throughput: 1.1797179583382094
(INFO) 2023-04-10 19:04:46,398 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,398 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:46,407 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,408 [api2.py:131] throughput: 2.171493110956267
(INFO) 2023-04-10 19:04:46,409 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,409 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 209, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 105}
(INFO) 2023-04-10 19:04:46,418 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,419 [api2.py:131] throughput: 0.35935211084608804
(INFO) 2023-04-10 19:04:46,419 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,420 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 270, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 19:04:46,429 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,430 [api2.py:131] throughput: 0.629268916394408
(INFO) 2023-04-10 19:04:46,431 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,431 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 391, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 19:04:46,439 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,439 [api2.py:131] throughput: 0.8187802392751636
(INFO) 2023-04-10 19:04:46,440 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,440 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 251, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 173}
(INFO) 2023-04-10 19:04:46,446 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,447 [api2.py:131] throughput: 50.05910637521506
(INFO) 2023-04-10 19:04:46,447 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,447 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 382, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:46,457 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,457 [api2.py:131] throughput: 0.4845307403548862
(INFO) 2023-04-10 19:04:46,458 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,458 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 12, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 19:04:46,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,464 [api2.py:131] throughput: 2.321231354789331
(INFO) 2023-04-10 19:04:46,465 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,465 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 51}
(INFO) 2023-04-10 19:04:46,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,472 [api2.py:131] throughput: 2.112520777147493
(INFO) 2023-04-10 19:04:46,472 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,473 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 391, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 84}
(INFO) 2023-04-10 19:04:46,478 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,479 [api2.py:131] throughput: 0.6294484751476088
(INFO) 2023-04-10 19:04:46,479 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,479 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 136, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:46,638 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,638 [api2.py:131] throughput: 0.785880142889274
(INFO) 2023-04-10 19:04:46,639 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,639 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 315, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 19:04:46,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,646 [api2.py:131] throughput: 0.4541636635838894
(INFO) 2023-04-10 19:04:46,646 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,646 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 129, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 132}
(INFO) 2023-04-10 19:04:46,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,653 [api2.py:131] throughput: 1.153139755978868
(INFO) 2023-04-10 19:04:46,654 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,654 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 19:04:46,660 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,660 [api2.py:131] throughput: 0.9842948609315606
(INFO) 2023-04-10 19:04:46,661 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,661 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 92, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 27}
(INFO) 2023-04-10 19:04:46,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,668 [api2.py:131] throughput: 1.831458577880832
(INFO) 2023-04-10 19:04:46,668 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,668 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 153, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:46,675 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,675 [api2.py:131] throughput: 1.3611978286194413
(INFO) 2023-04-10 19:04:46,676 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,676 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:46,682 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,683 [api2.py:131] throughput: 2.3662276830208384
(INFO) 2023-04-10 19:04:46,683 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 55, 'reticle_array_w': 16}
(INFO) 2023-04-10 19:04:46,683 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 454, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 257}
(INFO) 2023-04-10 19:04:46,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,690 [api2.py:131] throughput: 30.884140895354115
(INFO) 2023-04-10 19:04:46,695 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 245, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:46,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,727 [api1.py:131] throughput: 0.057661059762415146
(INFO) 2023-04-10 19:04:46,728 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,728 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 490, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,729 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,762 [api1.py:131] throughput: 0.03359332394595039
(INFO) 2023-04-10 19:04:46,763 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,763 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 283, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 19:04:46,764 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,790 [api1.py:131] throughput: 0.40594292122969244
(INFO) 2023-04-10 19:04:46,791 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 179, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:46,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,820 [api1.py:131] throughput: 0.07413376064744653
(INFO) 2023-04-10 19:04:46,821 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,821 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 482, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,822 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,852 [api1.py:131] throughput: 0.050799609918817455
(INFO) 2023-04-10 19:04:46,852 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,852 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 157, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,875 [api1.py:131] throughput: 0.29843686695571775
(INFO) 2023-04-10 19:04:46,876 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,876 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 307, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:46,877 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,902 [api1.py:131] throughput: 0.2572541422691543
(INFO) 2023-04-10 19:04:46,902 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:46,903 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,933 [api1.py:131] throughput: 0.664024931129459
(INFO) 2023-04-10 19:04:46,934 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,934 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:46,935 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,965 [api1.py:131] throughput: 0.2734462147308635
(INFO) 2023-04-10 19:04:46,966 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,966 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 239, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:46,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:46,998 [api1.py:131] throughput: 0.09044515449534501
(INFO) 2023-04-10 19:04:46,999 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:46,999 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 114, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:47,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,031 [api1.py:131] throughput: 0.084672032028517
(INFO) 2023-04-10 19:04:47,032 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 493, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,032 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,062 [api1.py:131] throughput: 0.04169649967825646
(INFO) 2023-04-10 19:04:47,063 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,063 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 190, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:47,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,092 [api1.py:131] throughput: 0.08957084027915789
(INFO) 2023-04-10 19:04:47,093 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,093 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 112, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,094 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,120 [api1.py:131] throughput: 0.2379972756965925
(INFO) 2023-04-10 19:04:47,121 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,121 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 442, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,121 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,150 [api1.py:131] throughput: 0.028892549172684787
(INFO) 2023-04-10 19:04:47,150 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,150 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 209, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,181 [api1.py:131] throughput: 0.045896681842914885
(INFO) 2023-04-10 19:04:47,181 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:47,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,209 [api1.py:131] throughput: 0.26352723675846895
(INFO) 2023-04-10 19:04:47,210 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,210 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 226, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,211 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,245 [api1.py:131] throughput: 0.04244431303413486
(INFO) 2023-04-10 19:04:47,246 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,246 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 154, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:47,247 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,281 [api1.py:131] throughput: 0.06639024361102146
(INFO) 2023-04-10 19:04:47,282 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-10 19:04:47,282 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,312 [api1.py:131] throughput: 0.366186586092025
(INFO) 2023-04-10 19:04:47,318 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,318 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 183, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,321 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,321 [api2.py:131] throughput: 1.0064295439710984
(INFO) 2023-04-10 19:04:47,322 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,322 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 19:04:47,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,325 [api2.py:131] throughput: 0.7465238600205603
(INFO) 2023-04-10 19:04:47,325 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,326 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 390, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,328 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,328 [api2.py:131] throughput: 0.4094481859292462
(INFO) 2023-04-10 19:04:47,329 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,329 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 507, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:47,332 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,332 [api2.py:131] throughput: 2.200796348827447
(INFO) 2023-04-10 19:04:47,333 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,333 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 354, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 47}
(INFO) 2023-04-10 19:04:47,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,335 [api2.py:131] throughput: 0.3539390534969795
(INFO) 2023-04-10 19:04:47,336 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,336 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 396, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:47,339 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,339 [api2.py:131] throughput: 0.3709598180313668
(INFO) 2023-04-10 19:04:47,340 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,340 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 463, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 19:04:47,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,343 [api2.py:131] throughput: 0.23129674875781162
(INFO) 2023-04-10 19:04:47,344 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,344 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 487, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,346 [api2.py:131] throughput: 0.32790144191216675
(INFO) 2023-04-10 19:04:47,347 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,347 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 207, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,351 [api2.py:131] throughput: 0.9171344186365377
(INFO) 2023-04-10 19:04:47,351 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,351 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:47,354 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,354 [api2.py:131] throughput: 3.799450380800284
(INFO) 2023-04-10 19:04:47,355 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,355 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 280, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:47,358 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,358 [api2.py:131] throughput: 0.6949876626870329
(INFO) 2023-04-10 19:04:47,359 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,359 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 177, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 19:04:47,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,362 [api2.py:131] throughput: 1.6575093800785803
(INFO) 2023-04-10 19:04:47,363 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,363 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 374, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:47,366 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,366 [api2.py:131] throughput: 14.603022101265067
(INFO) 2023-04-10 19:04:47,367 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,367 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 342, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,370 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,370 [api2.py:131] throughput: 0.5230553831899768
(INFO) 2023-04-10 19:04:47,371 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,371 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 263, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 19:04:47,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,374 [api2.py:131] throughput: 0.9060623214607819
(INFO) 2023-04-10 19:04:47,374 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,374 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:47,377 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,377 [api2.py:131] throughput: 90.12930586040018
(INFO) 2023-04-10 19:04:47,378 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,378 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 368, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,381 [api2.py:131] throughput: 0.4649104665115863
(INFO) 2023-04-10 19:04:47,382 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,382 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 180, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 19:04:47,385 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,385 [api2.py:131] throughput: 1.1222567028732382
(INFO) 2023-04-10 19:04:47,386 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,386 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 503, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 19:04:47,389 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,389 [api2.py:131] throughput: 69.12534527034678
(INFO) 2023-04-10 19:04:47,390 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-10 19:04:47,390 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 118, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 19:04:47,393 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,393 [api2.py:131] throughput: 0.6999855083960156
(INFO) 2023-04-10 19:04:47,399 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,399 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 114}
(INFO) 2023-04-10 19:04:47,524 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,525 [api2.py:131] throughput: 127.7328728930514
(INFO) 2023-04-10 19:04:47,526 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,526 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 204, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:47,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,532 [api2.py:131] throughput: 0.7858491218915254
(INFO) 2023-04-10 19:04:47,532 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,532 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 73}
(INFO) 2023-04-10 19:04:47,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,538 [api2.py:131] throughput: 0.4765381188401256
(INFO) 2023-04-10 19:04:47,538 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,538 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 214, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 45}
(INFO) 2023-04-10 19:04:47,544 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,544 [api2.py:131] throughput: 0.5016330854723289
(INFO) 2023-04-10 19:04:47,545 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,545 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 368, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 19:04:47,550 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,550 [api2.py:131] throughput: 0.5511256248944784
(INFO) 2023-04-10 19:04:47,551 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,551 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 223, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:47,556 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,556 [api2.py:131] throughput: 1.2111722048174582
(INFO) 2023-04-10 19:04:47,557 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,557 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 124, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 19:04:47,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,563 [api2.py:131] throughput: 34.539152180448035
(INFO) 2023-04-10 19:04:47,563 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,563 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 133, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 19:04:47,569 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,569 [api2.py:131] throughput: 1.4721252023292115
(INFO) 2023-04-10 19:04:47,569 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,570 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 61, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 19:04:47,575 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,575 [api2.py:131] throughput: 2.131250707446285
(INFO) 2023-04-10 19:04:47,576 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,576 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 152, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 19:04:47,581 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,581 [api2.py:131] throughput: 0.9352458922933068
(INFO) 2023-04-10 19:04:47,582 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,582 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 228, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:47,587 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,588 [api2.py:131] throughput: 0.8370636423191157
(INFO) 2023-04-10 19:04:47,588 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,588 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 190, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:47,593 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,594 [api2.py:131] throughput: 0.9199984516664177
(INFO) 2023-04-10 19:04:47,594 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,594 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 19:04:47,600 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,600 [api2.py:131] throughput: 2.7056454129445364
(INFO) 2023-04-10 19:04:47,601 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,601 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 252, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 19:04:47,606 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,606 [api2.py:131] throughput: 0.6938809058597378
(INFO) 2023-04-10 19:04:47,607 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,607 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 246, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:47,612 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,612 [api2.py:131] throughput: 1.0196236039073705
(INFO) 2023-04-10 19:04:47,613 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,613 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 102, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 176}
(INFO) 2023-04-10 19:04:47,618 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,618 [api2.py:131] throughput: 228.21957461015006
(INFO) 2023-04-10 19:04:47,619 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,619 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 250, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 19:04:47,624 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,624 [api2.py:131] throughput: 0.8278115585568588
(INFO) 2023-04-10 19:04:47,625 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,625 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 19:04:47,632 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,632 [api2.py:131] throughput: 1.4168346755913148
(INFO) 2023-04-10 19:04:47,633 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,633 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 158, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 19:04:47,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,639 [api2.py:131] throughput: 0.5194241631400166
(INFO) 2023-04-10 19:04:47,640 [api2.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 17}
(INFO) 2023-04-10 19:04:47,640 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 172, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 19:04:47,645 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 19:04:47,645 [api2.py:131] throughput: 1.3256469996266218
