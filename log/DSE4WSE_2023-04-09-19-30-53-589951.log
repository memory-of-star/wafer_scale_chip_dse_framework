(DEBUG) 2023-04-09 19:30:53,590 [logger.py:40] logger init.
(INFO) 2023-04-09 19:30:53,590 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-09-19-30-53-589951.log
(INFO) 2023-04-09 19:30:55,947 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:55,947 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:30:55,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,948 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,953 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,954 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:55,955 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,960 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,963 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,963 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:30:55,964 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,971 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:30:55,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,972 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:55,972 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,973 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,979 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,984 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:30:55,985 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,986 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:55,986 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:55,993 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:55,994 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,001 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:30:56,003 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:30:56,004 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,025 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,055 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:30:56,057 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:56,057 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,061 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:30:56,061 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,090 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:56,090 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,097 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,144 [api1.py:131] throughput: 0.008681802511060117
(INFO) 2023-04-09 19:30:56,157 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:56,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,160 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:30:56,161 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,163 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,165 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,187 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:30:56,190 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 19:30:56,197 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:30:56,247 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 19:30:56,251 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 48}
(INFO) 2023-04-09 19:30:56,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,257 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:30:56,257 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,260 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 19:30:56,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,265 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:30:56,265 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,267 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:30:56,267 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,276 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:56,276 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:56,277 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,277 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,283 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:30:56,284 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,285 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,289 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,289 [api1.py:131] throughput: 0.011575338091036336
(INFO) 2023-04-09 19:30:56,291 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:56,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,292 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:30:56,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,293 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,298 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,301 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:56,302 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,302 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,335 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:30:56,336 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,335 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:30:56,336 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,337 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:56,337 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,338 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:30:56,338 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,348 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 45}
(INFO) 2023-04-09 19:30:56,348 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,348 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,349 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,351 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,354 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:30:56,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,363 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:56,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,369 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,377 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:30:56,394 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:56,395 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,401 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:30:56,402 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,405 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:30:56,416 [api1.py:131] throughput: 0.008681807437366683
(INFO) 2023-04-09 19:30:56,453 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:30:56,477 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 19:30:56,487 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:56,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,492 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:30:56,501 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,509 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 19:30:56,514 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:56,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,515 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:30:56,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,516 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:56,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,520 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 19:30:56,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,544 [api1.py:131] throughput: 0.005788071882782347
(INFO) 2023-04-09 19:30:56,551 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:30:56,552 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,559 [api1.py:131] throughput: 0.006945588863462108
(INFO) 2023-04-09 19:30:56,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,566 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 19:30:56,581 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:56,582 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,582 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:30:56,583 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,592 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,601 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 19:30:56,601 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:30:56,601 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,614 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 48}
(INFO) 2023-04-09 19:30:56,615 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,630 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:30:56,630 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:56,631 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,639 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:30:56,640 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,669 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:56,670 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,676 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,697 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:56,697 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,701 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:56,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,710 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,747 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:56,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,798 [api1.py:131] throughput: 0.011575341702605418
(INFO) 2023-04-09 19:30:56,803 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 19:30:56,803 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:30:56,853 [api1.py:131] throughput: 0.00578807258268928
(INFO) 2023-04-09 19:30:56,858 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 19:30:56,870 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 19:30:56,879 [api1.py:131] throughput: 0.008681807780066478
(INFO) 2023-04-09 19:30:56,898 [api1.py:131] throughput: 0.0049612539575765
(INFO) 2023-04-09 19:30:56,902 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:56,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,902 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:56,903 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,907 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:30:56,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,907 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,932 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 19:30:56,952 [api1.py:131] throughput: 0.006945591108016238
(INFO) 2023-04-09 19:30:56,954 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:56,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,960 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:56,961 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,961 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,973 [api1.py:131] throughput: 0.006945589629895063
(INFO) 2023-04-09 19:30:56,976 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:56,976 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,979 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:30:56,979 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:30:56,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,982 [api1.py:131] throughput: 0.00496125387607221
(INFO) 2023-04-09 19:30:56,982 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,985 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:56,986 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:56,988 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:56,991 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,033 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 19:30:57,040 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:57,041 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,043 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:30:57,044 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,060 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:57,060 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,060 [api1.py:131] throughput: 0.008681807616686339
(INFO) 2023-04-09 19:30:57,065 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,075 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:57,075 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,076 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:30:57,076 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:30:57,077 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,079 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,081 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:30:57,081 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:30:57,082 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,099 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,117 [api1.py:131] throughput: 0.0069455879054211534
(INFO) 2023-04-09 19:30:57,121 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:30:57,129 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:30:57,131 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:57,132 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,135 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:30:57,136 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,138 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,143 [api1.py:131] throughput: 0.006945588615498541
(INFO) 2023-04-09 19:30:57,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,181 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:30:57,182 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,193 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 47, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:57,194 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,200 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,206 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:30:57,206 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,207 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:30:57,209 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 59}
(INFO) 2023-04-09 19:30:57,209 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:30:57,209 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 34, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:30:57,209 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,209 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,209 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,220 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,220 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:30:57,222 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,225 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,231 [api1.py:131] throughput: 0.01157533564366845
(INFO) 2023-04-09 19:30:57,234 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 52}
(INFO) 2023-04-09 19:30:57,234 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,252 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,253 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:57,253 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,259 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 19:30:57,261 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,306 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:57,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,312 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,321 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:57,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,328 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:30:57,328 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,329 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,336 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 19:30:57,346 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:57,346 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,350 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,355 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:57,356 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,361 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,393 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 19:30:57,420 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 19:30:57,441 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:30:57,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,445 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:30:57,446 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,474 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 19:30:57,484 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 19:30:57,499 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:57,499 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,512 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 19:30:57,513 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:30:57,513 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,522 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,537 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 19:30:57,551 [api1.py:131] throughput: 0.004961254785887699
(INFO) 2023-04-09 19:30:57,552 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:30:57,553 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,570 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:30:57,578 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:57,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,593 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:57,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,600 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,609 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:57,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,615 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,625 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:30:57,626 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,635 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,664 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:30:57,664 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,672 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:30:57,676 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:57,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,677 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,684 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,690 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:30:57,705 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:30:57,718 [api1.py:131] throughput: 0.0028941370049427513
(INFO) 2023-04-09 19:30:57,753 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:30:57,761 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 19:30:57,778 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:30:57,779 [api1.py:131] throughput: 0.008681807929541928
(INFO) 2023-04-09 19:30:57,782 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:57,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,788 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,878 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 39}
(INFO) 2023-04-09 19:30:57,879 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,902 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,922 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:57,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,923 [api1.py:131] throughput: 0.011575342930467403
(INFO) 2023-04-09 19:30:57,931 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:57,931 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,933 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:57,958 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:30:57,958 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:57,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,014 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:58,014 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,019 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 19:30:58,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,058 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:30:58,059 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,062 [api1.py:131] throughput: 0.005788072378024285
(INFO) 2023-04-09 19:30:58,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,115 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:30:58,122 [api1.py:131] throughput: 0.008681807228075036
(INFO) 2023-04-09 19:30:58,136 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:58,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,144 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,181 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:30:58,181 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,198 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,251 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:58,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,303 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:58,304 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,323 [api1.py:131] throughput: 0.005788073179046551
(INFO) 2023-04-09 19:30:58,331 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 19:30:58,360 [api1.py:131] throughput: 0.004341130158546594
(INFO) 2023-04-09 19:30:58,382 [api1.py:131] throughput: 0.006945589917307465
(INFO) 2023-04-09 19:30:58,405 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:30:58,419 [api1.py:131] throughput: 0.00868180560220974
(INFO) 2023-04-09 19:30:58,454 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:58,455 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,461 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:58,462 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,465 [api1.py:131] throughput: 0.0038588046610442826
(INFO) 2023-04-09 19:30:58,467 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,485 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:30:58,486 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,492 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,495 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:58,496 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,499 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:30:58,499 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,502 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,517 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:58,517 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,524 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,526 [api1.py:131] throughput: 0.004341130189065275
(INFO) 2023-04-09 19:30:58,527 [api1.py:131] throughput: 0.003157230271084519
(INFO) 2023-04-09 19:30:58,542 [api1.py:131] throughput: 0.004961253684297419
(INFO) 2023-04-09 19:30:58,550 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:30:58,575 [api1.py:131] throughput: 0.00868180734098237
(INFO) 2023-04-09 19:30:58,585 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:30:58,586 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,592 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,615 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:30:58,633 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:30:58,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,638 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,638 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:30:58,639 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,639 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:30:58,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,673 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 19:30:58,675 [api1.py:131] throughput: 0.003157230346767118
(INFO) 2023-04-09 19:30:58,684 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:30:58,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,685 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:30:58,686 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,693 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,702 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,708 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 19:30:58,718 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 45}
(INFO) 2023-04-09 19:30:58,719 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,735 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,745 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 19:30:58,845 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:30:58,845 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,846 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:30:58,846 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,851 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,852 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,855 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 19:30:58,872 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:30:58,872 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,874 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:30:58,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,877 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:58,878 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:58,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:58,899 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 19:30:58,907 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 19:30:59,049 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:59,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,115 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 19:30:59,125 [api1.py:131] throughput: 0.011575328217402394
(INFO) 2023-04-09 19:30:59,154 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 19:30:59,195 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 19:30:59,203 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 19:30:59,215 [api1.py:131] throughput: 0.00578807258268928
(INFO) 2023-04-09 19:30:59,251 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:30:59,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,274 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,275 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,282 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,371 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 19:30:59,400 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:30:59,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,414 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 50}
(INFO) 2023-04-09 19:30:59,414 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,438 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:59,439 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,446 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,449 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,465 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,465 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,469 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,488 [api1.py:131] throughput: 0.0038588046702727505
(INFO) 2023-04-09 19:30:59,499 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,499 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,517 [api1.py:131] throughput: 0.006945587560526473
(INFO) 2023-04-09 19:30:59,521 [api1.py:131] throughput: 0.0038588045105873085
(INFO) 2023-04-09 19:30:59,527 [api1.py:131] throughput: 0.005788070518376598
(INFO) 2023-04-09 19:30:59,546 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:30:59,551 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:30:59,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,565 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:30:59,567 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,571 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,579 [api1.py:131] throughput: 0.004341130343125931
(INFO) 2023-04-09 19:30:59,632 [api1.py:131] throughput: 0.002671518194281985
(INFO) 2023-04-09 19:30:59,646 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:30:59,647 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,667 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:59,668 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,669 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:30:59,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,697 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,697 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,703 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,707 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:30:59,708 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,717 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,720 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,720 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:30:59,720 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,720 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,722 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 19:30:59,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,740 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:30:59,740 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,741 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:30:59,741 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,749 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,759 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,785 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:30:59,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,805 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:30:59,805 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,831 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:30:59,879 [api1.py:131] throughput: 0.00694558872227711
(INFO) 2023-04-09 19:30:59,886 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:30:59,887 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,900 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:30:59,906 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:30:59,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,912 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,945 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 19:30:59,962 [api1.py:131] throughput: 0.006945588209740017
(INFO) 2023-04-09 19:30:59,964 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:30:59,965 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,973 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:30:59,973 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:30:59,980 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:30:59,986 [api1.py:131] throughput: 0.006945588020386053
(INFO) 2023-04-09 19:31:00,015 [api1.py:131] throughput: 0.006945588480245695
(INFO) 2023-04-09 19:31:00,031 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:00,031 [api1.py:131] throughput: 0.003157230416994385
(INFO) 2023-04-09 19:31:00,062 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 19:31:00,121 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:00,122 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,153 [api1.py:131] throughput: 0.011575340601635167
(INFO) 2023-04-09 19:31:00,153 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:00,153 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,161 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,201 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:31:00,216 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 19:31:00,241 [api1.py:131] throughput: 0.004341129560739578
(INFO) 2023-04-09 19:31:00,296 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,297 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,304 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,304 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:00,304 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,309 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,317 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:00,317 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,322 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,374 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:00,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,381 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:00,381 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,393 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,406 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,408 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,418 [api1.py:131] throughput: 0.004341129851340191
(INFO) 2023-04-09 19:31:00,425 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 19:31:00,452 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 19:31:00,459 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:00,547 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:31:00,594 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 48}
(INFO) 2023-04-09 19:31:00,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,597 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 19:31:00,604 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:00,605 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,612 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,617 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 19:31:00,619 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,627 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:00,627 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,633 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,645 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,706 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:00,709 [api1.py:131] throughput: 0.004341130258181113
(INFO) 2023-04-09 19:31:00,715 [api1.py:131] throughput: 0.006945589917307465
(INFO) 2023-04-09 19:31:00,720 [api1.py:131] throughput: 0.002671518170933834
(INFO) 2023-04-09 19:31:00,761 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:00,779 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:00,779 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,782 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:00,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,783 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 46}
(INFO) 2023-04-09 19:31:00,783 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,792 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,813 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,835 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,835 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,839 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,878 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,878 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,888 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,916 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:00,954 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 41, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:00,955 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:00,963 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:00,998 [api1.py:131] throughput: 0.00868180451829988
(INFO) 2023-04-09 19:31:01,003 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:01,022 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:01,022 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,039 [api1.py:131] throughput: 0.008681806981731769
(INFO) 2023-04-09 19:31:01,065 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:01,065 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,066 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:31:01,066 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,071 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,101 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:01,102 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,110 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,120 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:01,120 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,133 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,146 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:01,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,156 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,183 [api1.py:131] throughput: 0.00496125450707324
(INFO) 2023-04-09 19:31:01,215 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 19:31:01,254 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:01,366 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:01,367 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,387 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:31:01,399 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:01,400 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,495 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:01,509 [api1.py:131] throughput: 0.008681807905519088
(INFO) 2023-04-09 19:31:01,528 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 19:31:01,535 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:01,536 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,553 [api1.py:131] throughput: 0.008681806023730315
(INFO) 2023-04-09 19:31:01,568 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:01,569 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,586 [api1.py:131] throughput: 0.011575340578199882
(INFO) 2023-04-09 19:31:01,598 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:01,610 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 19:31:01,667 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:01,667 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,674 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,679 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:01,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,681 [api1.py:131] throughput: 0.003858804716308195
(INFO) 2023-04-09 19:31:01,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,716 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:01,716 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,720 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,775 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:01,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,795 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:01,796 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,801 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,835 [api1.py:131] throughput: 0.0019294469719382222
(INFO) 2023-04-09 19:31:01,881 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:01,904 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:01,905 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,913 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:01,914 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,918 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:01,918 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,926 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:01,938 [api1.py:131] throughput: 0.003858804426942559
(INFO) 2023-04-09 19:31:01,962 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 19:31:01,974 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:01,981 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:01,981 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:01,987 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:01,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,006 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:02,014 [api1.py:131] throughput: 0.00694558872227711
(INFO) 2023-04-09 19:31:02,070 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:02,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,081 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,090 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:02,090 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,098 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,122 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 19:31:02,191 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:02,191 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,198 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:02,199 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,227 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:02,227 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,232 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,273 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:02,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,288 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,331 [api1.py:131] throughput: 0.011575334393090756
(INFO) 2023-04-09 19:31:02,336 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:02,337 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,341 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,381 [api1.py:131] throughput: 0.006945591647205251
(INFO) 2023-04-09 19:31:02,382 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:31:02,388 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 19:31:02,410 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:02,453 [api1.py:131] throughput: 0.01157533490773175
(INFO) 2023-04-09 19:31:02,507 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:02,508 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,520 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:02,521 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,584 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:02,585 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,593 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,593 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:02,594 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,601 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:02,626 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:02,626 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,629 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:02,629 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,661 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:02,668 [api1.py:131] throughput: 0.0069455901226020515
(INFO) 2023-04-09 19:31:02,721 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:31:02,737 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:02,737 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,744 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 19:31:02,846 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:02,847 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,871 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,892 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:02,903 [api1.py:131] throughput: 0.006945591591062511
(INFO) 2023-04-09 19:31:02,904 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 19:31:02,937 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:02,937 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:02,948 [api1.py:131] throughput: 0.004341129560739578
(INFO) 2023-04-09 19:31:02,971 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:02,973 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:02,977 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,031 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 19:31:03,034 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:03,035 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,043 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,062 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:03,063 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,069 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,133 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:03,134 [api1.py:131] throughput: 0.005788072638597768
(INFO) 2023-04-09 19:31:03,135 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,151 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:03,151 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,152 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:03,160 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,176 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:03,182 [api1.py:131] throughput: 0.005788073342200921
(INFO) 2023-04-09 19:31:03,201 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:03,202 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,228 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:31:03,231 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:03,312 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:31:03,312 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,332 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,347 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:03,348 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,395 [api1.py:131] throughput: 0.003858804803343962
(INFO) 2023-04-09 19:31:03,399 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:03,400 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,404 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,426 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:03,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,446 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:03,446 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,451 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,455 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 19:31:03,469 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:03,488 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:03,488 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,514 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:03,515 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,519 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:31:03,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,540 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:31:03,541 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:03,542 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,547 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,572 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:31:03,604 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:03,622 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:03,623 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:03,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,627 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:03,628 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,631 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,639 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 19:31:03,703 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:31:03,703 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,719 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,742 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:31:03,743 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,748 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,782 [api1.py:131] throughput: 0.011575337340580706
(INFO) 2023-04-09 19:31:03,794 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:31:03,816 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:03,817 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,825 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,838 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:03,839 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:03,869 [api1.py:131] throughput: 0.0038588044573588313
(INFO) 2023-04-09 19:31:03,922 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:03,931 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 19:31:03,995 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:03,995 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:03,998 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:03,999 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,004 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,013 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:04,014 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,018 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:04,019 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,029 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,058 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:04,058 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,062 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,062 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:04,063 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,070 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,143 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:04,144 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:04,146 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 19:31:04,147 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:04,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,152 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,157 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:31:04,268 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 19:31:04,276 [api1.py:131] throughput: 0.0034729403133700676
(INFO) 2023-04-09 19:31:04,278 [api1.py:131] throughput: 0.00868180240128922
(INFO) 2023-04-09 19:31:04,334 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:04,334 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,339 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,370 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 19:31:04,372 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:04,373 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,380 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,461 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:04,461 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,467 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,487 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:04,505 [api1.py:131] throughput: 0.005788071248872792
(INFO) 2023-04-09 19:31:04,510 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:04,518 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:04,519 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,520 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:04,520 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,526 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,527 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,558 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:04,559 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,562 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:04,563 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,563 [api1.py:131] throughput: 0.008681806193698297
(INFO) 2023-04-09 19:31:04,564 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,694 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:04,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,701 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:04,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,708 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,711 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,741 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 19:31:04,755 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:04,755 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:04,787 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:04,813 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:04,832 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 19:31:04,860 [api1.py:131] throughput: 0.0069455907138505305
(INFO) 2023-04-09 19:31:04,971 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:04,971 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:04,979 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,030 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 62}
(INFO) 2023-04-09 19:31:05,031 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,074 [api1.py:131] throughput: 0.011575334393090756
(INFO) 2023-04-09 19:31:05,084 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 19:31:05,093 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:31:05,097 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:05,098 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,105 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,130 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:05,131 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,134 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:05,134 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,141 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,190 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:05,190 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,207 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:05,208 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,215 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,242 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:05,243 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,249 [api1.py:131] throughput: 0.003858804426942559
(INFO) 2023-04-09 19:31:05,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,255 [api1.py:131] throughput: 0.0038588046827970988
(INFO) 2023-04-09 19:31:05,266 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:05,266 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,272 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,272 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:05,277 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:05,278 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,278 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:05,281 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,321 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:05,321 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,339 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:05,361 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:31:05,366 [api1.py:131] throughput: 0.008681806502731015
(INFO) 2023-04-09 19:31:05,392 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:05,393 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,403 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:05,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,426 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:05,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,434 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,485 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 19:31:05,492 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:31:05,511 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 19:31:05,531 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:05,534 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 19:31:05,556 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:05,579 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:05,579 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,590 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 19:31:05,597 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:05,597 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,603 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,621 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:05,622 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,626 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,729 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:05,729 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,737 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:05,742 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:05,743 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,743 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,762 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:05,786 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 42}
(INFO) 2023-04-09 19:31:05,786 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,804 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 19:31:05,821 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:05,821 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,822 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:05,822 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,825 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:05,825 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,826 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,828 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,829 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,842 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 19:31:05,856 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:05,857 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,864 [api1.py:131] throughput: 0.00868179706642693
(INFO) 2023-04-09 19:31:05,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,888 [api1.py:131] throughput: 0.0019294469735207634
(INFO) 2023-04-09 19:31:05,912 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:05,930 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 19:31:05,930 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,931 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:05,937 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,966 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:05,966 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,970 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:05,976 [api1.py:131] throughput: 0.004961254888373031
(INFO) 2023-04-09 19:31:05,994 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:05,994 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:05,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,106 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:06,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,114 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,121 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 39}
(INFO) 2023-04-09 19:31:06,121 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,139 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:06,140 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,140 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:06,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,156 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:06,174 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 19:31:06,207 [api1.py:131] throughput: 0.00694558872227711
(INFO) 2023-04-09 19:31:06,214 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:06,215 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,227 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,293 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:06,293 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,337 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:06,339 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,352 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,352 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:06,353 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,355 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 19:31:06,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,418 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 19:31:06,486 [api1.py:131] throughput: 0.011575340761776281
(INFO) 2023-04-09 19:31:06,548 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:06,548 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,554 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,558 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:06,558 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,559 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 19:31:06,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,570 [api1.py:131] throughput: 0.00868180451829988
(INFO) 2023-04-09 19:31:06,583 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:31:06,583 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,588 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,599 [api1.py:131] throughput: 0.004961254314941498
(INFO) 2023-04-09 19:31:06,621 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:06,622 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,629 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,634 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:06,634 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,638 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,657 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 19:31:06,673 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:06,686 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:06,686 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,696 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,736 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:06,737 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,739 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:06,746 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,747 [api1.py:131] throughput: 0.00496125378551189
(INFO) 2023-04-09 19:31:06,755 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:06,777 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:06,778 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,785 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,787 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:06,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,871 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:06,871 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,881 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:06,881 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,886 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,922 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:06,922 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,929 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:06,929 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,939 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:06,962 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:06,962 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:06,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,007 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 19:31:07,007 [api1.py:131] throughput: 0.0019294469702180682
(INFO) 2023-04-09 19:31:07,025 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 19:31:07,041 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:07,046 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:07,046 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,049 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:07,054 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:07,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,109 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 19:31:07,122 [api1.py:131] throughput: 0.006945588615498541
(INFO) 2023-04-09 19:31:07,157 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:07,157 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,162 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,178 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:07,179 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,181 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:07,181 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,182 [api1.py:131] throughput: 0.005788073012754607
(INFO) 2023-04-09 19:31:07,187 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,188 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,280 [api1.py:131] throughput: 0.006945589316354288
(INFO) 2023-04-09 19:31:07,368 [api1.py:131] throughput: 0.006945591028117781
(INFO) 2023-04-09 19:31:07,368 [api1.py:131] throughput: 0.006945590043768928
(INFO) 2023-04-09 19:31:07,393 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:07,394 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,398 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:07,398 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,405 [api1.py:131] throughput: 0.0049612541863442395
(INFO) 2023-04-09 19:31:07,405 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,409 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:07,410 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,423 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:07,427 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:07,428 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,435 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,439 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:07,439 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,449 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,463 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 19:31:07,466 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:07,466 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,472 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:07,482 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,495 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:07,496 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,554 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:07,554 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,568 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,608 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:07,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,617 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,621 [api1.py:131] throughput: 0.008681807228075036
(INFO) 2023-04-09 19:31:07,643 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:07,643 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,648 [api1.py:131] throughput: 0.005788070785631281
(INFO) 2023-04-09 19:31:07,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,665 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:31:07,674 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:07,674 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,679 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,691 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 19:31:07,709 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:07,709 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,714 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,796 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:07,797 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,803 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,819 [api1.py:131] throughput: 0.006945590160502592
(INFO) 2023-04-09 19:31:07,826 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:07,837 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:07,887 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:07,888 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,906 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:07,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,949 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:07,956 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 19:31:07,960 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 19:31:07,991 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:07,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,993 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:07,993 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:07,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:07,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,138 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:08,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,152 [api1.py:131] throughput: 0.011575336634269613
(INFO) 2023-04-09 19:31:08,192 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 19:31:08,210 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:08,225 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 19:31:08,246 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:08,246 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,253 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,300 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:08,310 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:08,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,354 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:31:08,363 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:08,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,365 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 19:31:08,366 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:08,366 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,370 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:31:08,370 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,374 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,443 [api1.py:131] throughput: 0.0038588045482519048
(INFO) 2023-04-09 19:31:08,463 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:08,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,468 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 19:31:08,470 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,485 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:08,485 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:08,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,492 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:08,492 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,493 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,497 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,584 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:08,611 [api1.py:131] throughput: 0.0069455861809480995
(INFO) 2023-04-09 19:31:08,618 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:08,619 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,627 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,642 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:31:08,657 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:08,658 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,661 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,669 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 19:31:08,768 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:08,775 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:08,776 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,788 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,830 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:08,831 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:08,844 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:08,845 [api1.py:131] throughput: 0.004341130085695552
(INFO) 2023-04-09 19:31:08,928 [api1.py:131] throughput: 0.003858804857521344
(INFO) 2023-04-09 19:31:08,943 [api1.py:131] throughput: 0.011575337751124067
(INFO) 2023-04-09 19:31:08,957 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:09,020 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:09,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,028 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,037 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:09,038 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,043 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,055 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:09,056 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,060 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,065 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 19:31:09,078 [api1.py:131] throughput: 0.00496125387607221
(INFO) 2023-04-09 19:31:09,095 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:31:09,095 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,105 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,115 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:09,116 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,124 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,126 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:09,137 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:09,150 [api1.py:131] throughput: 0.0034729402433456894
(INFO) 2023-04-09 19:31:09,216 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 19:31:09,223 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:09,250 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:09,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,256 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,260 [api1.py:131] throughput: 0.0069455905496148326
(INFO) 2023-04-09 19:31:09,271 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:09,272 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,294 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:09,294 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,299 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,319 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:09,320 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,321 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:09,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,322 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:09,322 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:09,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,324 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,326 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,339 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,342 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:09,342 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,345 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:09,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,385 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:09,386 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,399 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,411 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:09,416 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:09,416 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,431 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,461 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:09,462 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,466 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,478 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:09,515 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 48}
(INFO) 2023-04-09 19:31:09,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,530 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:09,531 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,531 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:09,538 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:09,538 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,540 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 19:31:09,542 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,557 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 19:31:09,605 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 41, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:09,606 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,669 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:09,669 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,675 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,676 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:31:09,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,681 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:09,682 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,682 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,683 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 19:31:09,688 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,688 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:09,733 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:09,734 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,768 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:09,772 [api1.py:131] throughput: 0.01736180469163078
(INFO) 2023-04-09 19:31:09,894 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:09,898 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,962 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:09,963 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:09,968 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:09,988 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 19:31:09,995 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:09,995 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,004 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:10,004 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,015 [api1.py:131] throughput: 0.004961254797965411
(INFO) 2023-04-09 19:31:10,020 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:10,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,026 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,045 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:10,057 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:31:10,123 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:10,123 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,141 [api1.py:131] throughput: 0.017361795159226368
(INFO) 2023-04-09 19:31:10,146 [api1.py:131] throughput: 0.004961254243727455
(INFO) 2023-04-09 19:31:10,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,175 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 19:31:10,176 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:10,178 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,190 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,206 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:31:10,206 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,215 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,291 [api1.py:131] throughput: 0.004341129851340191
(INFO) 2023-04-09 19:31:10,303 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:10,303 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,337 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 19:31:10,340 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 19:31:10,350 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:31:10,401 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:10,401 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,404 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:10,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,409 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:10,409 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,417 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,539 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:10,540 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:10,540 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,543 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,561 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:10,562 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,567 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,592 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:10,599 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:10,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,605 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,614 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:10,615 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,722 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:10,722 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,722 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:10,727 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,746 [api1.py:131] throughput: 0.004961254721404003
(INFO) 2023-04-09 19:31:10,748 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:31:10,749 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,749 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:31:10,750 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:10,751 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,754 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,756 [api1.py:131] throughput: 0.006945591228188721
(INFO) 2023-04-09 19:31:10,761 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,769 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:10,769 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,777 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 19:31:10,777 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:10,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,787 [api1.py:131] throughput: 0.008681806643613585
(INFO) 2023-04-09 19:31:10,794 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:10,794 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,813 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:10,832 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:10,832 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,836 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,920 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:10,920 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,925 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,931 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:10,932 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,941 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,959 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:10,959 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,969 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:10,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,969 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:10,978 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:10,986 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:31:10,986 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,991 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:10,992 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:10,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,010 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:11,018 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,035 [api1.py:131] throughput: 0.005788073246602655
(INFO) 2023-04-09 19:31:11,041 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:11,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,050 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,061 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 19:31:11,106 [api1.py:131] throughput: 0.008681804420119654
(INFO) 2023-04-09 19:31:11,131 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:11,150 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:11,150 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,155 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,208 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:11,208 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,212 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:11,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,213 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 19:31:11,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,284 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:11,285 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,289 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,327 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:11,327 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,349 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,377 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 19:31:11,382 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:11,382 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,387 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,390 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:31:11,391 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,397 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,398 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:31:11,406 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:11,406 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,411 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,420 [api1.py:131] throughput: 0.011575336130004294
(INFO) 2023-04-09 19:31:11,429 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 19:31:11,444 [api1.py:131] throughput: 0.004341129851340191
(INFO) 2023-04-09 19:31:11,449 [api1.py:131] throughput: 0.006945590013111603
(INFO) 2023-04-09 19:31:11,463 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:11,471 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:11,488 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 19:31:11,531 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:11,543 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:11,544 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,555 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,561 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:11,561 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,588 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,609 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:11,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,622 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,624 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:11,625 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,631 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:11,631 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,636 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,638 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,653 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:11,654 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,663 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,671 [api1.py:131] throughput: 0.0049612537981637
(INFO) 2023-04-09 19:31:11,706 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:11,706 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,707 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 19:31:11,712 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,741 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:11,782 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 19:31:11,862 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:11,862 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,870 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,883 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:11,884 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,900 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:11,900 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:11,988 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:11,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:11,993 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,003 [api1.py:131] throughput: 0.008681801808526419
(INFO) 2023-04-09 19:31:12,069 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:12,148 [api1.py:131] throughput: 0.011575327036756839
(INFO) 2023-04-09 19:31:12,189 [api1.py:131] throughput: 0.0069455905705175566
(INFO) 2023-04-09 19:31:12,201 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:12,201 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,212 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,317 [api1.py:131] throughput: 0.004341129621918651
(INFO) 2023-04-09 19:31:12,356 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:12,357 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,371 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:12,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,379 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 19:31:12,402 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:12,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,409 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,413 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 39}
(INFO) 2023-04-09 19:31:12,413 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,419 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,432 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 19:31:12,436 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:12,439 [api1.py:131] throughput: 0.011575338252899327
(INFO) 2023-04-09 19:31:12,443 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:12,444 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,449 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,453 [api1.py:131] throughput: 0.0069455906312028875
(INFO) 2023-04-09 19:31:12,476 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:12,501 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:12,501 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,531 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:12,532 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,536 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:12,539 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,545 [api1.py:131] throughput: 0.004341129975883322
(INFO) 2023-04-09 19:31:12,570 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:12,570 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,589 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 67}
(INFO) 2023-04-09 19:31:12,589 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,596 [api1.py:131] throughput: 0.0049612545382946485
(INFO) 2023-04-09 19:31:12,598 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:12,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,604 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,606 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,662 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:31:12,683 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 19:31:12,693 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:12,694 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,745 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 19:31:12,765 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:12,766 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,766 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:12,767 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,770 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,772 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,781 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:12,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,787 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:12,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,814 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:12,881 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:12,881 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,888 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:31:12,889 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,890 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,891 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:12,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,894 [api1.py:131] throughput: 0.017361797937901876
(INFO) 2023-04-09 19:31:12,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,919 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:12,922 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:12,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,924 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:12,927 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,951 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 43, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:12,951 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:12,951 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,951 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,953 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:12,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:12,958 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,963 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,966 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:12,998 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:12,999 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:13,178 [api1.py:131] throughput: 0.008681807984291658
(INFO) 2023-04-09 19:31:13,204 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:13,243 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:13,244 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,251 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:13,252 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,261 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,277 [api1.py:131] throughput: 0.011575337662575496
(INFO) 2023-04-09 19:31:13,309 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 62}
(INFO) 2023-04-09 19:31:13,309 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,313 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:13,314 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,320 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,351 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:31:13,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,368 [api1.py:131] throughput: 0.00578807160311635
(INFO) 2023-04-09 19:31:13,377 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,407 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 19:31:13,413 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:13,492 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:13,492 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,497 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,545 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:13,545 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,564 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,576 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:13,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,579 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:13,580 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,617 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:13,642 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:13,642 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,648 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,678 [api1.py:131] throughput: 0.01736179591178423
(INFO) 2023-04-09 19:31:13,684 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:13,685 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,689 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:13,716 [api1.py:131] throughput: 0.011575340936814248
(INFO) 2023-04-09 19:31:13,782 [api1.py:131] throughput: 0.011575342896194843
(INFO) 2023-04-09 19:31:13,902 [api1.py:131] throughput: 0.0057880728029738844
(INFO) 2023-04-09 19:31:13,950 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 19:31:13,983 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:13,984 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:13,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,001 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 42}
(INFO) 2023-04-09 19:31:14,002 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,007 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 19:31:14,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,067 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 19:31:14,068 [api1.py:131] throughput: 0.006945589137188145
(INFO) 2023-04-09 19:31:14,076 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 19:31:14,114 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:14,116 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:14,116 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,124 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,135 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:14,136 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,141 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,220 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:14,221 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,226 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,240 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:14,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,280 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:31:14,283 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:31:14,284 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,283 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:14,284 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,291 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:14,291 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,293 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,302 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,310 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:14,310 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,356 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:14,375 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:14,403 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:14,418 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 49}
(INFO) 2023-04-09 19:31:14,418 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,432 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,497 [api1.py:131] throughput: 0.011575324203208491
(INFO) 2023-04-09 19:31:14,502 [api1.py:131] throughput: 0.01736179967457452
(INFO) 2023-04-09 19:31:14,511 [api1.py:131] throughput: 0.01157533564366845
(INFO) 2023-04-09 19:31:14,663 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:14,664 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,703 [api1.py:131] throughput: 0.005788071398745061
(INFO) 2023-04-09 19:31:14,778 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:14,779 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,789 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,790 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:14,790 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,848 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:14,848 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,852 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:31:14,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:14,913 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:14,913 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:14,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,004 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 25, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:15,005 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,009 [api1.py:131] throughput: 0.0031572303788588506
(INFO) 2023-04-09 19:31:15,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,078 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:15,078 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,081 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 19:31:15,083 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,110 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:15,116 [api1.py:131] throughput: 0.011575331866671992
(INFO) 2023-04-09 19:31:15,125 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:15,126 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,130 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,151 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:31:15,169 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 29, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:15,169 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,177 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,192 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:15,203 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:15,203 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,208 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,219 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:15,272 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:15,273 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,361 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:15,361 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,367 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,412 [api1.py:131] throughput: 0.006945591855023954
(INFO) 2023-04-09 19:31:15,452 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:15,453 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,466 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:15,467 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,467 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 19:31:15,471 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,537 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 19:31:15,574 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:15,582 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:15,583 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,590 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,693 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:15,693 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,700 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,716 [api1.py:131] throughput: 0.011575337387769596
(INFO) 2023-04-09 19:31:15,828 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 19:31:15,851 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:31:15,860 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:15,861 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,863 [api1.py:131] throughput: 0.003858804767391388
(INFO) 2023-04-09 19:31:15,866 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:15,866 [api1.py:131] throughput: 0.01157533490773175
(INFO) 2023-04-09 19:31:15,911 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:31:15,914 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:15,915 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:15,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,029 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:16,029 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:16,030 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,034 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,083 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:16,083 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,090 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,107 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:16,108 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,116 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,122 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:16,122 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,126 [api1.py:131] throughput: 0.005788073267283097
(INFO) 2023-04-09 19:31:16,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,175 [api1.py:131] throughput: 0.003157230356421349
(INFO) 2023-04-09 19:31:16,186 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:31:16,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,194 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 19:31:16,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,210 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:16,250 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:16,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,320 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 19:31:16,333 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:16,333 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,335 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:31:16,335 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,340 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,342 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,351 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:16,352 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,357 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,364 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:16,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,364 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:16,364 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,368 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,369 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:16,371 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,371 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,372 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:16,372 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,375 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:16,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,376 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,377 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,380 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:16,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,459 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:16,504 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:31:16,508 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:16,511 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:16,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,532 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:16,567 [api1.py:131] throughput: 0.011575337751124067
(INFO) 2023-04-09 19:31:16,582 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:16,584 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:31:16,707 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 42}
(INFO) 2023-04-09 19:31:16,708 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,720 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:31:16,732 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:16,734 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,737 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,810 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 19:31:16,817 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:16,817 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,825 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,826 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 19:31:16,861 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 58}
(INFO) 2023-04-09 19:31:16,861 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,867 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:31:16,868 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,884 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,911 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:16,960 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:16,961 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,965 [api1.py:131] throughput: 0.0057880720490173935
(INFO) 2023-04-09 19:31:16,968 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,971 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:16,972 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:16,979 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:16,985 [api1.py:131] throughput: 0.002894136943433738
(INFO) 2023-04-09 19:31:16,995 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:16,996 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,001 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,081 [api1.py:131] throughput: 0.004961253893052271
(INFO) 2023-04-09 19:31:17,125 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:17,156 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:31:17,162 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:17,162 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,170 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,217 [api1.py:131] throughput: 0.006945587166361168
(INFO) 2023-04-09 19:31:17,227 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:17,227 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,230 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:17,231 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,233 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,239 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,338 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:17,341 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,357 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,357 [api1.py:131] throughput: 0.0031572304752570087
(INFO) 2023-04-09 19:31:17,378 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:17,378 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,383 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,398 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:31:17,398 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,413 [api1.py:131] throughput: 0.008681804107728034
(INFO) 2023-04-09 19:31:17,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,441 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:17,456 [api1.py:131] throughput: 0.002042941080990081
(INFO) 2023-04-09 19:31:17,548 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:17,596 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 19:31:17,630 [api1.py:131] throughput: 0.006945590492132339
(INFO) 2023-04-09 19:31:17,642 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:17,643 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,651 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,659 [api1.py:131] throughput: 0.01736179591178423
(INFO) 2023-04-09 19:31:17,677 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:17,677 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,682 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,702 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:17,702 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,709 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,714 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:17,714 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,718 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,750 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 42, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:17,752 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,835 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:17,849 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:17,850 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,856 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,862 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:17,863 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,866 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,866 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:17,867 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,872 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,880 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:17,881 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,892 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,892 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:17,925 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:17,926 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:17,931 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:17,953 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:17,998 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 44}
(INFO) 2023-04-09 19:31:17,999 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,090 [api1.py:131] throughput: 0.0057880712199201954
(INFO) 2023-04-09 19:31:18,098 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:18,155 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:31:18,174 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 19:31:18,210 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:31:18,214 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:18,214 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,219 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,220 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 19:31:18,240 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:18,241 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,259 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:18,259 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,270 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,289 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:18,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,298 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:18,298 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,300 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,306 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,317 [api1.py:131] throughput: 0.008681807437366683
(INFO) 2023-04-09 19:31:18,338 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:18,339 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,354 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:18,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,355 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:18,359 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,407 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:31:18,411 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:18,411 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,416 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:18,421 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,460 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:18,476 [api1.py:131] throughput: 0.017361800700790336
(INFO) 2023-04-09 19:31:18,477 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:18,478 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,588 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:18,609 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:18,610 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,615 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,670 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:18,671 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,677 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,731 [api1.py:131] throughput: 0.0031572303252307564
(INFO) 2023-04-09 19:31:18,760 [api1.py:131] throughput: 0.006945589285000212
(INFO) 2023-04-09 19:31:18,769 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:18,881 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 19:31:18,897 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:18,898 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,905 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,919 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:31:18,930 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:18,930 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,942 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,968 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:18,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,969 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:18,969 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,972 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:18,971 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:18,972 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:18,978 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,978 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:18,980 [api1.py:131] throughput: 0.0049612537981637
(INFO) 2023-04-09 19:31:18,980 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,014 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 19:31:19,022 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 256, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:19,022 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,023 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:19,024 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,028 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,030 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,082 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:19,083 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,087 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,107 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:19,110 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,204 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:19,204 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,204 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 45, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:19,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,205 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:19,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,211 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,211 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,229 [api1.py:131] throughput: 0.011575334393090756
(INFO) 2023-04-09 19:31:19,231 [api1.py:131] throughput: 0.008681799437476026
(INFO) 2023-04-09 19:31:19,240 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 19:31:19,245 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:19,371 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:19,371 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,387 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:19,388 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,395 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,449 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 19:31:19,467 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:31:19,470 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:19,470 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,520 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 19:31:19,532 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:19,563 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 19:31:19,594 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:19,595 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,599 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,638 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:19,639 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,678 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:19,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,685 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,687 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:19,688 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,732 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:19,733 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,738 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,753 [api1.py:131] throughput: 0.00868180702056967
(INFO) 2023-04-09 19:31:19,770 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 19:31:19,792 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:19,796 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:31:19,796 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,805 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:19,831 [api1.py:131] throughput: 0.005788071882782347
(INFO) 2023-04-09 19:31:19,867 [api1.py:131] throughput: 0.006945585553867201
(INFO) 2023-04-09 19:31:19,903 [api1.py:131] throughput: 0.011575341263551801
(INFO) 2023-04-09 19:31:19,921 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:31:19,954 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:19,954 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:19,968 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,018 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:20,020 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,027 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,061 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:20,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,167 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:20,201 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:20,202 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:20,203 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,209 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,218 [api1.py:131] throughput: 0.008681805821076185
(INFO) 2023-04-09 19:31:20,242 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:20,245 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:20,246 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,249 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,278 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:20,279 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,288 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 44}
(INFO) 2023-04-09 19:31:20,289 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,303 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,314 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:20,353 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:20,354 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,356 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:20,357 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,361 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,364 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,378 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 56}
(INFO) 2023-04-09 19:31:20,379 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,411 [api1.py:131] throughput: 0.003858804841067599
(INFO) 2023-04-09 19:31:20,437 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:20,438 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,442 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,463 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:20,494 [api1.py:131] throughput: 0.0069455915303366905
(INFO) 2023-04-09 19:31:20,511 [api1.py:131] throughput: 0.006945590492132339
(INFO) 2023-04-09 19:31:20,515 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:20,569 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 19:31:20,577 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:20,578 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,592 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 19:31:20,663 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:20,663 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,672 [api1.py:131] throughput: 0.003157230440091353
(INFO) 2023-04-09 19:31:20,682 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 19:31:20,706 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:20,707 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,724 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:20,725 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,728 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,731 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,762 [api1.py:131] throughput: 0.008681808066815306
(INFO) 2023-04-09 19:31:20,809 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:20,818 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:20,819 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,825 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,825 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:20,826 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,832 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,909 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:20,925 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:20,925 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,929 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,932 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:20,933 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,944 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,966 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 19:31:20,974 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:31:20,975 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:20,980 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:20,991 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:20,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,000 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,010 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:21,023 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:21,024 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,030 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,039 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:21,039 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,047 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,078 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:21,079 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,083 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,086 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:21,136 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:21,137 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,186 [api1.py:131] throughput: 0.002315325664665477
(INFO) 2023-04-09 19:31:21,203 [api1.py:131] throughput: 0.006945589779849356
(INFO) 2023-04-09 19:31:21,260 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:21,268 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 19:31:21,285 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 59}
(INFO) 2023-04-09 19:31:21,286 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,301 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,317 [api1.py:131] throughput: 0.01736178955396985
(INFO) 2023-04-09 19:31:21,355 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 19:31:21,402 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:21,402 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:21,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,410 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,463 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 42}
(INFO) 2023-04-09 19:31:21,463 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:21,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,468 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,480 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,639 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:21,640 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,644 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,660 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:31:21,678 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:21,692 [api1.py:131] throughput: 0.0069455903689555724
(INFO) 2023-04-09 19:31:21,694 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:21,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,710 [api1.py:131] throughput: 0.004341129735099941
(INFO) 2023-04-09 19:31:21,742 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:31:21,771 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:21,775 [api1.py:131] throughput: 0.008681808470055879
(INFO) 2023-04-09 19:31:21,799 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:21,845 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:21,846 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,855 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,857 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:21,857 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,859 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:21,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,864 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,865 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,902 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 53, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:21,902 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 40}
(INFO) 2023-04-09 19:31:21,902 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,903 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,924 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,933 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:21,934 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,939 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:21,962 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:21,963 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:21,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,007 [api1.py:131] throughput: 0.003472940141352109
(INFO) 2023-04-09 19:31:22,075 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:22,155 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:22,170 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:22,170 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,190 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:22,191 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,196 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,270 [api1.py:131] throughput: 0.011575342165046914
(INFO) 2023-04-09 19:31:22,287 [api1.py:131] throughput: 0.008681805438285083
(INFO) 2023-04-09 19:31:22,296 [api1.py:131] throughput: 0.005788071699923808
(INFO) 2023-04-09 19:31:22,299 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:22,299 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,328 [api1.py:131] throughput: 0.011575319550396316
(INFO) 2023-04-09 19:31:22,450 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:22,470 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:22,470 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,475 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,492 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:22,493 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,499 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,504 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:22,504 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,505 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:22,505 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,509 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,510 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:22,510 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,554 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:22,558 [api1.py:131] throughput: 0.008681806193698297
(INFO) 2023-04-09 19:31:22,647 [api1.py:131] throughput: 0.0043411302740320595
(INFO) 2023-04-09 19:31:22,650 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:22,681 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:31:22,693 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:22,694 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,700 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,719 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:22,719 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,725 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,732 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:22,733 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,739 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,792 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 19:31:22,819 [api1.py:131] throughput: 0.017361798545737262
(INFO) 2023-04-09 19:31:22,822 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:22,822 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,825 [api1.py:131] throughput: 0.008681804179578105
(INFO) 2023-04-09 19:31:22,827 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,849 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:22,850 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,856 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,880 [api1.py:131] throughput: 0.006945590319684866
(INFO) 2023-04-09 19:31:22,882 [api1.py:131] throughput: 0.008681798359726278
(INFO) 2023-04-09 19:31:22,904 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:31:22,905 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,973 [api1.py:131] throughput: 0.008681801994491211
(INFO) 2023-04-09 19:31:22,978 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:22,979 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,979 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:22,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,985 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,985 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:22,985 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,988 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,989 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:22,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:22,989 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:22,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,005 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:31:23,051 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:23,052 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,052 [api1.py:131] throughput: 0.011575332482486465
(INFO) 2023-04-09 19:31:23,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,062 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:23,062 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,066 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,109 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:23,118 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:23,126 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:23,127 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,149 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,161 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 19:31:23,202 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:23,235 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:23,236 [api1.py:131] throughput: 0.011575334393090756
(INFO) 2023-04-09 19:31:23,242 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:23,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,270 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:23,271 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,315 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:23,315 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,324 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,328 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:23,328 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,340 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:23,341 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,345 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,360 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:23,360 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,366 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,382 [api1.py:131] throughput: 0.011575333837278531
(INFO) 2023-04-09 19:31:23,387 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:23,390 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:23,390 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,399 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:23,427 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 50}
(INFO) 2023-04-09 19:31:23,428 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,482 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:23,534 [api1.py:131] throughput: 0.0026715182019872064
(INFO) 2023-04-09 19:31:23,600 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 19:31:23,610 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 19:31:23,797 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:23,797 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,825 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:23,825 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,830 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,856 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:23,857 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,862 [api1.py:131] throughput: 0.005788072233261248
(INFO) 2023-04-09 19:31:23,868 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,912 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:31:23,918 [api1.py:131] throughput: 0.003858804743912156
(INFO) 2023-04-09 19:31:23,930 [api1.py:131] throughput: 0.008681807447786608
(INFO) 2023-04-09 19:31:23,936 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:23,937 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,942 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,943 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:23,957 [api1.py:131] throughput: 0.008681805213113862
(INFO) 2023-04-09 19:31:23,981 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:23,981 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,985 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:23,985 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:23,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:23,993 [api1.py:131] throughput: 0.0057880730735931215
(INFO) 2023-04-09 19:31:24,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,091 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:24,092 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,106 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:24,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,117 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:24,118 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,121 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:24,121 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,122 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,131 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:24,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,180 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:24,181 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,232 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:24,233 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,241 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,242 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:24,246 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:24,320 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 19:31:24,435 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:24,435 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,458 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:24,459 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,464 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,487 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:24,488 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,501 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,516 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:24,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,526 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,585 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:24,608 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:24,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,634 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:24,635 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,658 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 43}
(INFO) 2023-04-09 19:31:24,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,667 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,711 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:24,734 [api1.py:131] throughput: 0.00496125378551189
(INFO) 2023-04-09 19:31:24,780 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:24,797 [api1.py:131] throughput: 0.0069455901226020515
(INFO) 2023-04-09 19:31:24,876 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:24,876 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,881 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,899 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 19:31:24,911 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:24,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,918 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,949 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:24,949 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,953 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:24,953 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:24,955 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:24,957 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,022 [api1.py:131] throughput: 0.005788073287136319
(INFO) 2023-04-09 19:31:25,025 [api1.py:131] throughput: 0.0043411303502960494
(INFO) 2023-04-09 19:31:25,085 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:25,096 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:25,097 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,146 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:25,146 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,150 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,177 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:31:25,180 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:25,180 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,184 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,194 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:25,196 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,214 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,232 [api1.py:131] throughput: 0.017361800199615158
(INFO) 2023-04-09 19:31:25,258 [api1.py:131] throughput: 0.005788072908822148
(INFO) 2023-04-09 19:31:25,307 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:31:25,315 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:25,336 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 19:31:25,357 [api1.py:131] throughput: 0.0038588044797708216
(INFO) 2023-04-09 19:31:25,365 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 19:31:25,366 [api1.py:131] throughput: 0.003472940205878659
(INFO) 2023-04-09 19:31:25,369 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 19:31:25,375 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:25,376 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,384 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:25,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,456 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:25,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,463 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,470 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:25,470 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,503 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 19:31:25,547 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:25,602 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:25,603 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,603 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:25,603 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,606 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:25,607 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,609 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,613 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,614 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:25,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,614 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:25,615 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,615 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 19:31:25,617 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,621 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,672 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:25,677 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:25,677 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:25,686 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:25,690 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,690 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,694 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,695 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,708 [api1.py:131] throughput: 0.004341130157648986
(INFO) 2023-04-09 19:31:25,747 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:25,747 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,758 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:25,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,815 [api1.py:131] throughput: 0.011575335385612713
(INFO) 2023-04-09 19:31:25,832 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:25,834 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:31:25,859 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:25,860 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,863 [api1.py:131] throughput: 0.005788071033796366
(INFO) 2023-04-09 19:31:25,864 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,866 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:25,866 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,870 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,875 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:25,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,893 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,902 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:25,903 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,906 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:25,907 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,942 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 51}
(INFO) 2023-04-09 19:31:25,942 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:25,976 [api1.py:131] throughput: 0.011575336580315298
(INFO) 2023-04-09 19:31:25,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:25,985 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:25,989 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:26,009 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:26,011 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:26,012 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,016 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,088 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:26,088 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,099 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,108 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:26,138 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:26,138 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,148 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:31:26,154 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,158 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:26,171 [api1.py:131] throughput: 0.004961254159565406
(INFO) 2023-04-09 19:31:26,182 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 22, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:31:26,183 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,200 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,218 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:26,219 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,229 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,332 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 49, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:26,333 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,347 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,391 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:26,392 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,406 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:26,407 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,450 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:26,451 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,456 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,456 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:26,457 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,468 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,516 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:26,517 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,547 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 19:31:26,593 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:26,610 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:26,634 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 19:31:26,647 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:26,757 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 19:31:26,822 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:26,823 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,840 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:26,958 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:26,959 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:26,963 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,075 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:27,159 [api1.py:131] throughput: 0.005788072945338957
(INFO) 2023-04-09 19:31:27,185 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:27,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,191 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,251 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:27,251 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,269 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:27,270 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,274 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:27,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,275 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,276 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:27,276 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,278 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,280 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,287 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:27,287 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,295 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,313 [api1.py:131] throughput: 0.008681808503263928
(INFO) 2023-04-09 19:31:27,315 [api1.py:131] throughput: 0.003472940205878659
(INFO) 2023-04-09 19:31:27,347 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:31:27,362 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:27,363 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,363 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:27,366 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:27,368 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,385 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:27,385 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,386 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 19:31:27,393 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,434 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:27,439 [api1.py:131] throughput: 0.011575330224500389
(INFO) 2023-04-09 19:31:27,493 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:27,494 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,501 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,568 [api1.py:131] throughput: 0.0069455905705175566
(INFO) 2023-04-09 19:31:27,576 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:31:27,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,579 [api1.py:131] throughput: 0.008681808389407762
(INFO) 2023-04-09 19:31:27,598 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:27,598 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,603 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,604 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,606 [api1.py:131] throughput: 0.003472940236220298
(INFO) 2023-04-09 19:31:27,621 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 19:31:27,621 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 26, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:27,622 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,630 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,683 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:27,683 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,693 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,695 [api1.py:131] throughput: 0.01157533490773175
(INFO) 2023-04-09 19:31:27,705 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:27,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,710 [api1.py:131] throughput: 0.0038588046702727505
(INFO) 2023-04-09 19:31:27,712 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,732 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:27,752 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:31:27,755 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:27,755 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:27,756 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,756 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,760 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,760 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,767 [api1.py:131] throughput: 0.0028941369440771744
(INFO) 2023-04-09 19:31:27,768 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:27,769 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,777 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,787 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:27,787 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:27,816 [api1.py:131] throughput: 0.002480701123378736
(INFO) 2023-04-09 19:31:27,837 [api1.py:131] throughput: 0.011575308146460727
(INFO) 2023-04-09 19:31:27,961 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 24, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:27,962 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:27,967 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:27,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,063 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:28,064 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,068 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,070 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:28,070 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:28,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,070 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:28,071 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,077 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,100 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 69}
(INFO) 2023-04-09 19:31:28,101 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 19:31:28,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,125 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 19:31:28,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,160 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:31:28,163 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:28,166 [api1.py:131] throughput: 0.006945590160502592
(INFO) 2023-04-09 19:31:28,169 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:28,170 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,171 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:28,175 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,189 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:28,205 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:28,206 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,210 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:28,211 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,221 [api1.py:131] throughput: 0.008681796922727025
(INFO) 2023-04-09 19:31:28,251 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:28,350 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 19:31:28,463 [api1.py:131] throughput: 0.006945590910933379
(INFO) 2023-04-09 19:31:28,483 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:28,864 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 30, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:28,865 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,877 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,878 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:28,878 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,880 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:28,881 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:28,881 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,881 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,888 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:28,888 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,922 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:28,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,923 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:28,923 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,927 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,928 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,942 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:28,943 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,948 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,959 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 23, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:28,959 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:28,977 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:28,978 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:28,986 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,064 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:29,064 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,068 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,085 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:29,108 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:29,118 [api1.py:131] throughput: 0.011575329273769672
(INFO) 2023-04-09 19:31:29,138 [api1.py:131] throughput: 0.005788072870218663
(INFO) 2023-04-09 19:31:29,141 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:29,144 [api1.py:131] throughput: 0.008681800002011715
(INFO) 2023-04-09 19:31:29,189 [api1.py:131] throughput: 0.008681801413351264
(INFO) 2023-04-09 19:31:29,251 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:29,251 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,255 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:29,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,257 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:29,258 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,264 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,265 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,275 [api1.py:131] throughput: 0.011575334544126695
(INFO) 2023-04-09 19:31:29,332 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:29,333 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,337 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,355 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:29,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,360 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,371 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 19:31:29,406 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:29,441 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:29,442 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,444 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:31:29,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,487 [api1.py:131] throughput: 0.008681806550631088
(INFO) 2023-04-09 19:31:29,614 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:31:29,614 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,617 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:29,625 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,631 [api1.py:131] throughput: 0.006945586755772356
(INFO) 2023-04-09 19:31:29,642 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:29,643 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,649 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,651 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 19:31:29,677 [api1.py:131] throughput: 0.008681804706478656
(INFO) 2023-04-09 19:31:29,806 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 40}
(INFO) 2023-04-09 19:31:29,808 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,821 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,847 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:29,848 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,866 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:29,867 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,877 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:29,905 [api1.py:131] throughput: 0.008681803805201439
(INFO) 2023-04-09 19:31:29,920 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:29,921 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:29,925 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,005 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:30,084 [api1.py:131] throughput: 0.003472940143137429
(INFO) 2023-04-09 19:31:30,085 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:30,085 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,089 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,165 [api1.py:131] throughput: 0.005788072690899259
(INFO) 2023-04-09 19:31:30,167 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:30,167 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,171 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:30,171 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,176 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,185 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 45}
(INFO) 2023-04-09 19:31:30,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,203 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:30,239 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:30,240 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,259 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:30,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,333 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:30,371 [api1.py:131] throughput: 0.005788071557700507
(INFO) 2023-04-09 19:31:30,396 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:30,399 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 50}
(INFO) 2023-04-09 19:31:30,399 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,403 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 65}
(INFO) 2023-04-09 19:31:30,403 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,409 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,417 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,420 [api1.py:131] throughput: 0.008681806023730315
(INFO) 2023-04-09 19:31:30,476 [api1.py:131] throughput: 0.005788071248872792
(INFO) 2023-04-09 19:31:30,494 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:30,494 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,503 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:30,503 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,510 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,577 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:30,577 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,595 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:30,608 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:30,609 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,614 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,628 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:30,629 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,648 [api1.py:131] throughput: 0.008681804969928954
(INFO) 2023-04-09 19:31:30,649 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:30,650 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,657 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,683 [api1.py:131] throughput: 0.011575331650596755
(INFO) 2023-04-09 19:31:30,706 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:30,706 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,710 [api1.py:131] throughput: 0.008681807929541928
(INFO) 2023-04-09 19:31:30,712 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,732 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:30,735 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:31:30,736 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,741 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,745 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:30,745 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,749 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:30,750 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,750 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,753 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,770 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:30,771 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,777 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,798 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:30,799 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,805 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,831 [api1.py:131] throughput: 0.017361801179691095
(INFO) 2023-04-09 19:31:30,858 [api1.py:131] throughput: 0.005788071017252027
(INFO) 2023-04-09 19:31:30,862 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:30,886 [api1.py:131] throughput: 0.011575338091036336
(INFO) 2023-04-09 19:31:30,890 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:30,907 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:30,921 [api1.py:131] throughput: 0.005788073043934346
(INFO) 2023-04-09 19:31:30,981 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 60}
(INFO) 2023-04-09 19:31:30,981 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:30,992 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:30,996 [api1.py:131] throughput: 0.011575341062841587
(INFO) 2023-04-09 19:31:30,997 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:30,998 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,005 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,014 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:31,015 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,022 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,035 [api1.py:131] throughput: 0.017361788386208537
(INFO) 2023-04-09 19:31:31,089 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:31,090 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,094 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:31,095 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,096 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,104 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,196 [api1.py:131] throughput: 0.008681805365104435
(INFO) 2023-04-09 19:31:31,200 [api1.py:131] throughput: 0.011575337751124067
(INFO) 2023-04-09 19:31:31,201 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:31,210 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:31:31,212 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 19:31:31,218 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:31,218 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,233 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:31,234 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,235 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,238 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,260 [api1.py:131] throughput: 0.0031572304752570087
(INFO) 2023-04-09 19:31:31,318 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:31,318 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,322 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:31,322 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,324 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,378 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:31,486 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:31,487 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,491 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:31:31,499 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,509 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:31:31,516 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:31,542 [api1.py:131] throughput: 0.01157534342504685
(INFO) 2023-04-09 19:31:31,566 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:31,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,573 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,586 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 52}
(INFO) 2023-04-09 19:31:31,587 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,597 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,599 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:31,600 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,605 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,613 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:31,613 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:31,620 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:31,643 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:31:31,654 [api1.py:131] throughput: 0.005788072389159903
(INFO) 2023-04-09 19:31:31,762 [api1.py:131] throughput: 0.011575321945225646
(INFO) 2023-04-09 19:31:31,836 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:31,957 [api1.py:131] throughput: 0.017361806447601158
(INFO) 2023-04-09 19:31:32,182 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:32,183 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,187 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:32,187 [api1.py:131] throughput: 0.005788072522787328
(INFO) 2023-04-09 19:31:32,188 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,188 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:32,189 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,189 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:32,189 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,191 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:32,192 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,193 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,195 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,197 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,201 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:32,202 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,235 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:32,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,238 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:32,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,240 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:32,241 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,244 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:32,244 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,248 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,255 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,344 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:31:32,369 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:32,369 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,373 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:32,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,376 [api1.py:131] throughput: 0.011575325708530879
(INFO) 2023-04-09 19:31:32,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,380 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:32,380 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,380 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:31:32,380 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,384 [api1.py:131] throughput: 0.01157531818192286
(INFO) 2023-04-09 19:31:32,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,395 [api1.py:131] throughput: 0.011575342209205787
(INFO) 2023-04-09 19:31:32,516 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:32,516 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,517 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:31:32,521 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,559 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:32,559 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,569 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,570 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:32,600 [api1.py:131] throughput: 0.011575335385612713
(INFO) 2023-04-09 19:31:32,604 [api1.py:131] throughput: 0.008681805107381292
(INFO) 2023-04-09 19:31:32,653 [api1.py:131] throughput: 0.01736179591178423
(INFO) 2023-04-09 19:31:32,695 [api1.py:131] throughput: 0.005788072001640405
(INFO) 2023-04-09 19:31:32,696 [api1.py:131] throughput: 0.008681800349418331
(INFO) 2023-04-09 19:31:32,747 [api1.py:131] throughput: 0.004341130125335089
(INFO) 2023-04-09 19:31:32,747 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:32,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,749 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:32,749 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,751 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:32,752 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,752 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,757 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,758 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,775 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:32,775 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,780 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,793 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:32,794 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,858 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:32,859 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:32,863 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:32,893 [api1.py:131] throughput: 0.005788072458448197
(INFO) 2023-04-09 19:31:32,943 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:32,958 [api1.py:131] throughput: 0.011575313164189619
(INFO) 2023-04-09 19:31:33,009 [api1.py:131] throughput: 0.006945586711555101
(INFO) 2023-04-09 19:31:33,047 [api1.py:131] throughput: 0.008681793729397062
(INFO) 2023-04-09 19:31:33,056 [api1.py:131] throughput: 0.011575338409704105
(INFO) 2023-04-09 19:31:33,152 [api1.py:131] throughput: 0.006945590043768928
(INFO) 2023-04-09 19:31:33,221 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:33,221 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,228 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,249 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:31:33,249 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,260 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:33,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,260 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:33,261 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,266 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,436 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 19:31:33,452 [api1.py:131] throughput: 0.004961254621254395
(INFO) 2023-04-09 19:31:33,462 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,502 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 19:31:33,567 [api1.py:131] throughput: 0.008681802924315286
(INFO) 2023-04-09 19:31:33,572 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 11, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:33,572 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,577 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:33,577 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,577 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,590 [api1.py:131] throughput: 0.00496125378551189
(INFO) 2023-04-09 19:31:33,615 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:33,616 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,621 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,678 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 64, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:33,678 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,682 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,701 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:33,701 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,705 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:33,705 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,714 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,766 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:33,781 [api1.py:131] throughput: 0.004961253570431144
(INFO) 2023-04-09 19:31:33,793 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:33,793 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,803 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,839 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:31:33,869 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:33,945 [api1.py:131] throughput: 0.006945588480245695
(INFO) 2023-04-09 19:31:33,959 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:33,960 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:33,967 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:33,998 [api1.py:131] throughput: 0.00694558833653955
(INFO) 2023-04-09 19:31:34,027 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:31:34,028 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,050 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:31:34,050 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,056 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,260 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 19:31:34,314 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 19:31:34,558 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 19:31:34,564 [api1.py:131] throughput: 0.006945590779544812
(INFO) 2023-04-09 19:31:34,576 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:34,577 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,582 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,589 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:31:34,590 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,590 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:34,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,591 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:34,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,594 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,595 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,598 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,624 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:34,625 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,625 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:34,625 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,626 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:34,626 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,630 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,637 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,641 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,672 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:34,693 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:34,694 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,699 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 72, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:34,700 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,704 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,705 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,756 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:34,838 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:31:34,882 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 256, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:34,883 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,888 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,910 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 19:31:34,919 [api1.py:131] throughput: 0.006945589085324262
(INFO) 2023-04-09 19:31:34,946 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:34,947 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,950 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:34,950 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,951 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,955 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:34,974 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:34,974 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:34,981 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:35,040 [api1.py:131] throughput: 0.003472940274200892
(INFO) 2023-04-09 19:31:35,046 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:35,047 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:35,051 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:35,082 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:35,083 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:35,090 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:35,115 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:35,154 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:35,224 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:35,243 [api1.py:131] throughput: 0.008681801131083317
(INFO) 2023-04-09 19:31:35,245 [api1.py:131] throughput: 0.011575320497801208
(INFO) 2023-04-09 19:31:35,268 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 56}
(INFO) 2023-04-09 19:31:35,269 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:35,306 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 19:31:35,418 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 19:31:35,545 [api1.py:131] throughput: 0.003858804391723718
(INFO) 2023-04-09 19:31:35,610 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:35,610 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:35,627 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:35,687 [api1.py:131] throughput: 0.0038588045482519048
(INFO) 2023-04-09 19:31:35,722 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:35,724 [api1.py:131] throughput: 0.003858804356504878
(INFO) 2023-04-09 19:31:36,653 [api1.py:131] throughput: 0.017361805318763017
(INFO) 2023-04-09 19:31:37,031 [api1.py:131] throughput: 0.004341130141940841
(INFO) 2023-04-09 19:31:37,202 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:31:37,202 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,204 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:37,205 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,207 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,210 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:37,211 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,211 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:37,211 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,212 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:37,212 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,278 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:37,279 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,284 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,299 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:37,313 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:37,319 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:37,320 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,325 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:37,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,461 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:37,463 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,475 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,475 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:37,479 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,487 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,531 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 19:31:37,581 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:37,601 [api1.py:131] throughput: 0.011575333837278531
(INFO) 2023-04-09 19:31:37,703 [api1.py:131] throughput: 0.0023153256627700737
(INFO) 2023-04-09 19:31:37,704 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:37,875 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:31:37,875 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:37,898 [api1.py:131] throughput: 0.01736179591178423
(INFO) 2023-04-09 19:31:37,990 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 41}
(INFO) 2023-04-09 19:31:37,991 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:37,998 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:38,105 [api1.py:131] throughput: 0.005788071882782347
(INFO) 2023-04-09 19:31:38,183 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:38,184 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:38,190 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:38,304 [api1.py:131] throughput: 0.01736179166347457
(INFO) 2023-04-09 19:31:38,648 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:38,648 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:38,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:38,701 [api1.py:131] throughput: 0.01736179912392218
(INFO) 2023-04-09 19:31:38,732 [api1.py:131] throughput: 0.008681798120226368
(INFO) 2023-04-09 19:31:38,777 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:38,826 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:38,826 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:38,831 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,035 [api1.py:131] throughput: 0.011575315444976917
(INFO) 2023-04-09 19:31:39,041 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:31:39,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,050 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,065 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:39,065 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,154 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:39,154 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,162 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,216 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:39,216 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,223 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,238 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:39,238 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,539 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:39,539 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,569 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:39,586 [api1.py:131] throughput: 0.011575335116804667
(INFO) 2023-04-09 19:31:39,607 [api1.py:131] throughput: 0.0019294469839792995
(INFO) 2023-04-09 19:31:39,609 [api1.py:131] throughput: 0.01736180469163078
(INFO) 2023-04-09 19:31:39,626 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:39,626 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,627 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:39,627 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,633 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,706 [api1.py:131] throughput: 0.0069455901226020515
(INFO) 2023-04-09 19:31:39,765 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:39,765 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,769 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,788 [api1.py:131] throughput: 0.011575334393090756
(INFO) 2023-04-09 19:31:39,916 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:39,916 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,921 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,924 [api1.py:131] throughput: 0.01157532248284053
(INFO) 2023-04-09 19:31:39,952 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:39,952 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,958 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:39,987 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:31:39,987 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:39,988 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:39,996 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:40,016 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 19:31:40,059 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:40,059 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:40,065 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:40,076 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:40,105 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:40,105 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:40,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:40,128 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 19:31:40,188 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:31:40,322 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 19:31:40,541 [api1.py:131] throughput: 0.008681800754726084
(INFO) 2023-04-09 19:31:40,714 [api1.py:131] throughput: 0.00694558872227711
(INFO) 2023-04-09 19:31:41,129 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:41,129 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,129 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:41,130 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,130 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:41,131 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,131 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:41,131 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,134 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,136 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,138 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,142 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,220 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 33, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:31:41,220 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,222 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:41,235 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:41,235 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,369 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:41,462 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 19:31:41,514 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:41,514 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,519 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,662 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:31:41,692 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:31:41,723 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:41,755 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 18, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:41,757 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,766 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,845 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:41,845 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:41,850 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:41,976 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:42,147 [api1.py:131] throughput: 0.011575327213853657
(INFO) 2023-04-09 19:31:42,161 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:42,161 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:42,167 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:42,423 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:31:42,466 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:42,467 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:42,472 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:42,536 [api1.py:131] throughput: 0.011575330726274995
(INFO) 2023-04-09 19:31:42,860 [api1.py:131] throughput: 0.005788070518376598
(INFO) 2023-04-09 19:31:42,893 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:31:42,893 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,067 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 37}
(INFO) 2023-04-09 19:31:43,068 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,075 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,108 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:43,108 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,116 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,140 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:31:43,141 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,146 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,443 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,460 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:43,461 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,598 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:31:43,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,606 [api1.py:131] throughput: 0.01736179662366335
(INFO) 2023-04-09 19:31:43,607 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,674 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:43,714 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 19:31:43,771 [api1.py:131] throughput: 0.01736178432240038
(INFO) 2023-04-09 19:31:43,820 [api1.py:131] throughput: 0.00496125387607221
(INFO) 2023-04-09 19:31:43,905 [api1.py:131] throughput: 0.011575333919387153
(INFO) 2023-04-09 19:31:43,962 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:43,963 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:43,969 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:43,983 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 50}
(INFO) 2023-04-09 19:31:43,983 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:44,012 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:44,435 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 19:31:44,506 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 19:31:44,507 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:44,515 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:44,537 [api1.py:131] throughput: 0.0031572303891872246
(INFO) 2023-04-09 19:31:44,794 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:44,795 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:44,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:44,988 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:45,001 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:45,002 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:45,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:45,225 [api1.py:131] throughput: 0.011575323971620467
(INFO) 2023-04-09 19:31:45,444 [api1.py:131] throughput: 0.008681808447474406
(INFO) 2023-04-09 19:31:45,743 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:45,744 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:45,753 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:45,888 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:45,888 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:45,893 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:46,072 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:46,073 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:46,091 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:46,146 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:46,181 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 19:31:46,236 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:46,238 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:46,239 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:46,242 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:46,255 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:46,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:46,260 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:46,272 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 45}
(INFO) 2023-04-09 19:31:46,272 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:46,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:46,369 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:46,550 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:31:46,551 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:46,558 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:46,626 [api1.py:131] throughput: 0.017361785781203086
(INFO) 2023-04-09 19:31:47,008 [api1.py:131] throughput: 0.006945591028117781
(INFO) 2023-04-09 19:31:47,091 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:47,093 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:47,101 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:47,169 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:47,169 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:47,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:47,195 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:47,196 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:47,269 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:47,290 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:47,290 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:47,301 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:47,360 [api1.py:131] throughput: 0.01157533699846126
(INFO) 2023-04-09 19:31:47,361 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:47,611 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:47,872 [api1.py:131] throughput: 0.017361801179691095
(INFO) 2023-04-09 19:31:47,987 [api1.py:131] throughput: 0.0086818033892274
(INFO) 2023-04-09 19:31:48,282 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:48,283 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:48,287 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:48,464 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:48,464 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:48,468 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:48,475 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 19:31:48,476 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:48,484 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:48,493 [api1.py:131] throughput: 0.008681795485728252
(INFO) 2023-04-09 19:31:48,520 [api1.py:131] throughput: 0.011575339507337669
(INFO) 2023-04-09 19:31:48,724 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 64, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 26}
(INFO) 2023-04-09 19:31:48,725 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:48,740 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:48,801 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:48,971 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 19:31:49,278 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:49,279 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:49,283 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:49,507 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:49,540 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:31:49,541 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:49,545 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:49,663 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:31:49,793 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:49,794 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:49,798 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:50,086 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:31:50,246 [api1.py:131] throughput: 0.004341129918401875
(INFO) 2023-04-09 19:31:50,530 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:50,530 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:50,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:50,558 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:50,558 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:50,566 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:50,618 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:50,778 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:31:50,778 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:50,790 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:50,866 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:50,866 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:50,871 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:50,891 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:31:50,891 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:50,899 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:50,980 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:50,980 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:50,983 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,025 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 39}
(INFO) 2023-04-09 19:31:51,026 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,035 [api1.py:131] throughput: 0.006945587659067808
(INFO) 2023-04-09 19:31:51,063 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:31:51,067 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:51,153 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:51,153 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,158 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,202 [api1.py:131] throughput: 0.017361787131946558
(INFO) 2023-04-09 19:31:51,212 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,251 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:51,295 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:51,510 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:51,511 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,517 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,637 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:31:51,638 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,643 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,697 [api1.py:131] throughput: 0.01157532853101141
(INFO) 2023-04-09 19:31:51,740 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:51,740 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,749 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,841 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 20, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:51,842 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,849 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:51,850 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,851 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,857 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:51,916 [api1.py:131] throughput: 0.017361797937901876
(INFO) 2023-04-09 19:31:51,940 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:31:51,941 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:51,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:52,007 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:52,043 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:31:52,140 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:31:52,664 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:52,664 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:52,669 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:52,737 [api1.py:131] throughput: 0.006945588940105396
(INFO) 2023-04-09 19:31:53,074 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:53,085 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 16, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:53,086 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:53,093 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:53,189 [api1.py:131] throughput: 0.011575339744663871
(INFO) 2023-04-09 19:31:53,253 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 14, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:31:53,254 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:53,265 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:53,683 [api1.py:131] throughput: 0.004961253691414062
(INFO) 2023-04-09 19:31:53,936 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 51, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:53,937 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:53,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:54,007 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:54,008 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:54,017 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:54,128 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:31:54,129 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:54,150 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:54,217 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:54,250 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 37}
(INFO) 2023-04-09 19:31:54,250 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:54,386 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:54,387 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:54,397 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:54,416 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:31:54,417 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:54,422 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:54,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:54,513 [api1.py:131] throughput: 0.005788072969484766
(INFO) 2023-04-09 19:31:54,513 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:31:54,529 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:31:54,601 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:31:54,997 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:31:54,997 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:55,003 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:55,045 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 40}
(INFO) 2023-04-09 19:31:55,046 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:55,054 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:55,172 [api1.py:131] throughput: 0.011575338524129215
(INFO) 2023-04-09 19:31:55,213 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 48, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:55,213 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:55,217 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:55,363 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 19:31:55,380 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:55,380 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:55,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:55,669 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:31:55,726 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:56,115 [api1.py:131] throughput: 0.017361798545737262
(INFO) 2023-04-09 19:31:56,585 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:56,586 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:56,591 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:57,001 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:31:57,067 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:31:57,068 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:57,072 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:57,441 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:31:57,623 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:31:57,623 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:57,630 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:57,817 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:31:57,818 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:57,823 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:57,885 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:31:57,886 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:57,890 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:57,994 [api1.py:131] throughput: 0.004341130335674631
(INFO) 2023-04-09 19:31:58,078 [api1.py:131] throughput: 0.017361795159226368
(INFO) 2023-04-09 19:31:58,098 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:31:58,217 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:58,218 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:58,221 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:58,250 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:31:58,355 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:31:58,374 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:31:58,374 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:58,381 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:58,517 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:31:58,518 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:58,533 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:58,595 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 54}
(INFO) 2023-04-09 19:31:58,595 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:58,620 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:58,656 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 43}
(INFO) 2023-04-09 19:31:58,656 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:58,664 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:58,738 [api1.py:131] throughput: 0.006945587560526473
(INFO) 2023-04-09 19:31:58,850 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:31:58,851 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:58,855 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:59,031 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:31:59,033 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:59,034 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:59,038 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:59,123 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:31:59,305 [api1.py:131] throughput: 0.017361800199615158
(INFO) 2023-04-09 19:31:59,306 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:31:59,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:59,311 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:59,338 [api1.py:131] throughput: 0.00578807258268928
(INFO) 2023-04-09 19:31:59,358 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:31:59,359 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:59,367 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:31:59,559 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:31:59,622 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 31, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:31:59,622 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:31:59,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:00,035 [api1.py:131] throughput: 0.01736180469163078
(INFO) 2023-04-09 19:32:00,430 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 29}
(INFO) 2023-04-09 19:32:00,430 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:00,445 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:00,509 [api1.py:131] throughput: 0.011575338709058693
(INFO) 2023-04-09 19:32:00,577 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:00,578 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:00,583 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:00,779 [api1.py:131] throughput: 0.008681806703602293
(INFO) 2023-04-09 19:32:00,894 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:01,036 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:01,037 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:01,043 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:01,159 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:32:01,598 [api1.py:131] throughput: 0.00496125443817082
(INFO) 2023-04-09 19:32:02,114 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:32:02,114 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:02,118 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:32:02,118 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:02,119 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:02,125 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:02,286 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:32:02,375 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:02,375 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:02,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:02,481 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:32:02,481 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:02,485 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:02,518 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:32:02,654 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 19:32:02,658 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 45}
(INFO) 2023-04-09 19:32:02,659 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:02,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:02,684 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:32:02,938 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:32:02,938 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:02,943 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:03,125 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:32:03,941 [api1.py:131] throughput: 0.017361801179691095
(INFO) 2023-04-09 19:32:04,383 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 31}
(INFO) 2023-04-09 19:32:04,384 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:04,392 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:05,004 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 256, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:05,004 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:05,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:05,026 [api1.py:131] throughput: 0.011575336634269613
(INFO) 2023-04-09 19:32:05,162 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:32:05,273 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:05,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:05,279 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:05,307 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 54, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:32:05,307 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:05,328 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:05,530 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:05,787 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:05,788 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:05,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:05,815 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:05,816 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:05,820 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:05,961 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:32:05,981 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:05,994 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:32:05,994 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:05,999 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:06,305 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:32:06,305 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:06,327 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:06,415 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 19:32:06,604 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:32:06,605 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:06,610 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:06,844 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:32:06,989 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 46}
(INFO) 2023-04-09 19:32:06,989 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:06,997 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:07,131 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 19:32:07,468 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 56, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:32:07,469 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:07,481 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:07,767 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:32:08,198 [api1.py:131] throughput: 0.017361801637770104
(INFO) 2023-04-09 19:32:08,416 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:08,417 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:08,420 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:08,547 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:08,587 [api1.py:131] throughput: 0.005788073359196167
(INFO) 2023-04-09 19:32:09,146 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:09,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:09,152 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:09,186 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:32:09,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:09,200 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:09,345 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:09,879 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:09,880 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:09,885 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:09,968 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:32:10,048 [api1.py:131] throughput: 0.011575337662575496
(INFO) 2023-04-09 19:32:10,376 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:10,378 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:10,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:10,700 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:10,909 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 47}
(INFO) 2023-04-09 19:32:10,909 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:10,922 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:11,090 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:11,091 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:11,096 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:11,513 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:12,127 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 38}
(INFO) 2023-04-09 19:32:12,128 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:12,137 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:12,332 [api1.py:131] throughput: 0.01736180207635641
(INFO) 2023-04-09 19:32:12,553 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 28, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:12,554 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:12,562 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:12,778 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:12,779 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:12,782 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:12,782 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:12,782 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:12,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:12,843 [api1.py:131] throughput: 0.017361797298075198
(INFO) 2023-04-09 19:32:12,852 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:12,882 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:13,100 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:13,101 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:13,106 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:13,127 [api1.py:131] throughput: 0.008681805647372656
(INFO) 2023-04-09 19:32:13,484 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:32:13,485 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:13,490 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:13,511 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:32:13,582 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:13,583 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:13,586 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:13,628 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:32:13,629 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:13,633 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:32:13,633 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:13,634 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:13,640 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:13,721 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 19:32:13,722 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:13,722 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:13,729 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:13,780 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:32:13,862 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:13,978 [api1.py:131] throughput: 0.011575336245798548
(INFO) 2023-04-09 19:32:13,985 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:32:14,111 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:14,112 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:14,118 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:14,426 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:14,426 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:14,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:14,468 [api1.py:131] throughput: 0.01157531667660243
(INFO) 2023-04-09 19:32:14,677 [api1.py:131] throughput: 0.011575312160643492
(INFO) 2023-04-09 19:32:14,906 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:14,906 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:14,911 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:15,050 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:16,168 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:32:16,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:16,178 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:16,951 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 19:32:17,254 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:32:17,255 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,264 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:17,574 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:17,575 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,581 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:17,659 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:17,747 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 11}
(INFO) 2023-04-09 19:32:17,747 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,747 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:17,748 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,751 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:17,753 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:17,758 [api1.py:131] throughput: 0.004961253441382704
(INFO) 2023-04-09 19:32:17,791 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:17,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,796 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:17,816 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:32:17,817 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,826 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:17,858 [api1.py:131] throughput: 0.017361736049430775
(INFO) 2023-04-09 19:32:17,918 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:17,922 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:17,951 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 17, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:17,952 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:17,959 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:18,029 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 72, 'core_array_w': 80, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:18,030 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:18,033 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:18,088 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 37, 'reticle_array_w': 25}
(INFO) 2023-04-09 19:32:18,089 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:18,106 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:32:18,107 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:18,107 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:18,114 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:18,174 [api1.py:131] throughput: 0.011575303128736187
(INFO) 2023-04-09 19:32:18,177 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 48}
(INFO) 2023-04-09 19:32:18,177 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:18,185 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:18,212 [api1.py:131] throughput: 0.011575329693209674
(INFO) 2023-04-09 19:32:18,252 [api1.py:131] throughput: 0.011575336245798548
(INFO) 2023-04-09 19:32:18,273 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:32:18,495 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:18,495 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:18,507 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:19,006 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:32:19,007 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:19,011 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:19,058 [api1.py:131] throughput: 0.008681802973253402
(INFO) 2023-04-09 19:32:19,344 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:32:19,362 [api1.py:131] throughput: 0.017361802496668313
(INFO) 2023-04-09 19:32:20,841 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:20,841 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:20,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:21,012 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:32:21,013 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:21,020 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:21,109 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:21,242 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:21,242 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:21,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:21,273 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:21,274 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:21,277 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:21,333 [api1.py:131] throughput: 0.017361790643880562
(INFO) 2023-04-09 19:32:21,365 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:21,378 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:21,467 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:21,467 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:21,471 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:21,517 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:21,518 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:21,539 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:21,640 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:32:21,685 [api1.py:131] throughput: 0.002894136998369269
(INFO) 2023-04-09 19:32:21,701 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:22,073 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:32:22,074 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:22,079 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:22,539 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:32:22,910 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:22,912 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:22,917 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:23,216 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:23,584 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 50}
(INFO) 2023-04-09 19:32:23,585 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:23,600 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:24,185 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 28}
(INFO) 2023-04-09 19:32:24,186 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:24,192 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:24,743 [api1.py:131] throughput: 0.011575335385612713
(INFO) 2023-04-09 19:32:25,081 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 46}
(INFO) 2023-04-09 19:32:25,081 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:25,094 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:25,138 [api1.py:131] throughput: 0.017361803286854736
(INFO) 2023-04-09 19:32:25,387 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 24}
(INFO) 2023-04-09 19:32:25,388 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:25,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:25,442 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:25,443 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:25,448 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:25,602 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:25,620 [api1.py:116] Design point: {'core_buffer_size': 512, 'core_buffer_bw': 2048, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:25,620 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:25,626 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:25,654 [api1.py:131] throughput: 0.01736178274203105
(INFO) 2023-04-09 19:32:25,707 [api1.py:131] throughput: 0.0115753087736766
(INFO) 2023-04-09 19:32:25,998 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 128, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:25,998 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:26,003 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:26,056 [api1.py:131] throughput: 0.017361801637770104
(INFO) 2023-04-09 19:32:26,199 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 17}
(INFO) 2023-04-09 19:32:26,199 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:26,204 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:26,222 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:26,456 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:26,456 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:26,460 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:26,673 [api1.py:131] throughput: 0.017361766473540003
(INFO) 2023-04-09 19:32:27,065 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:27,565 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:32:27,567 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:27,573 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:27,896 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:32:29,662 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 27}
(INFO) 2023-04-09 19:32:29,662 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:32:29,662 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,662 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,662 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:32:29,663 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:29,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:29,666 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:29,734 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:32:29,775 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:32:29,803 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:32:29,804 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:29,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,804 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,805 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 14}
(INFO) 2023-04-09 19:32:29,806 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,808 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:29,812 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:29,813 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:29,820 [api1.py:131] throughput: 0.01157533490773175
(INFO) 2023-04-09 19:32:29,924 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 21, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:29,924 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:29,925 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:29,926 [api1.py:131] throughput: 0.01736175452119866
(INFO) 2023-04-09 19:32:29,930 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:30,161 [api1.py:131] throughput: 0.017361795159226368
(INFO) 2023-04-09 19:32:30,315 [api1.py:131] throughput: 0.011575333235148685
(INFO) 2023-04-09 19:32:30,681 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:32:30,681 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:30,686 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:30,951 [api1.py:131] throughput: 0.011575331084685456
(INFO) 2023-04-09 19:32:31,940 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 16}
(INFO) 2023-04-09 19:32:31,941 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:31,945 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:32,097 [api1.py:131] throughput: 0.017361762987438745
(INFO) 2023-04-09 19:32:32,291 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:32,292 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:32,297 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:32,369 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:32,557 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 18}
(INFO) 2023-04-09 19:32:32,558 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:32,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:32,598 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 23}
(INFO) 2023-04-09 19:32:32,599 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:32,604 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:32,710 [api1.py:131] throughput: 0.017361769572297854
(INFO) 2023-04-09 19:32:32,760 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 51}
(INFO) 2023-04-09 19:32:32,760 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:32,769 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:32,797 [api1.py:131] throughput: 0.01736178102423863
(INFO) 2023-04-09 19:32:33,459 [api1.py:131] throughput: 0.0173618036587072
(INFO) 2023-04-09 19:32:33,526 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:32:33,526 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:33,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:33,715 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:32:33,715 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:33,721 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:34,092 [api1.py:131] throughput: 0.01736177915028365
(INFO) 2023-04-09 19:32:34,145 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 19:32:35,002 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:35,002 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:35,007 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:35,030 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 16, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 19}
(INFO) 2023-04-09 19:32:35,031 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:35,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:35,093 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:35,193 [api1.py:131] throughput: 0.017361772344871605
(INFO) 2023-04-09 19:32:35,944 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 21}
(INFO) 2023-04-09 19:32:35,945 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:35,951 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:36,126 [api1.py:131] throughput: 0.01736177709785723
(INFO) 2023-04-09 19:32:36,303 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 8, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 39}
(INFO) 2023-04-09 19:32:36,304 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:36,313 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:36,322 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 33}
(INFO) 2023-04-09 19:32:36,323 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:36,329 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:36,635 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 2048, 'core_mac_num': 32, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:32:36,635 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:36,639 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:36,681 [api1.py:131] throughput: 0.017361793517282162
(INFO) 2023-04-09 19:32:36,781 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 4096, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:36,781 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:36,786 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:36,802 [api1.py:131] throughput: 0.011575331866671992
(INFO) 2023-04-09 19:32:36,992 [api1.py:131] throughput: 0.008681799133495302
(INFO) 2023-04-09 19:32:37,056 [api1.py:131] throughput: 0.008681807239655275
(INFO) 2023-04-09 19:32:37,189 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 30}
(INFO) 2023-04-09 19:32:37,190 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:37,198 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:37,898 [api1.py:131] throughput: 0.011575336245798548
(INFO) 2023-04-09 19:32:38,116 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 64, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:38,117 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:38,121 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:38,402 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:38,768 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 80, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 61}
(INFO) 2023-04-09 19:32:38,769 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:38,781 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:38,938 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 15}
(INFO) 2023-04-09 19:32:38,939 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:38,946 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:39,210 [api1.py:131] throughput: 0.017361759036525676
(INFO) 2023-04-09 19:32:39,382 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 32, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 40, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:32:39,382 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:39,388 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:39,677 [api1.py:131] throughput: 0.017361806706678782
(INFO) 2023-04-09 19:32:39,703 [api1.py:131] throughput: 0.006945587242162185
(INFO) 2023-04-09 19:32:40,000 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:32:40,001 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:40,009 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:40,063 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 40, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 12}
(INFO) 2023-04-09 19:32:40,064 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:40,069 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:40,267 [api1.py:131] throughput: 0.017361743232891393
(INFO) 2023-04-09 19:32:40,325 [api1.py:131] throughput: 0.017361794362400467
(INFO) 2023-04-09 19:32:40,992 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 2048, 'core_mac_num': 16, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 22}
(INFO) 2023-04-09 19:32:40,993 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:41,001 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:41,467 [api1.py:131] throughput: 0.011575331866671992
(INFO) 2023-04-09 19:32:41,682 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 35}
(INFO) 2023-04-09 19:32:41,683 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:41,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:42,420 [api1.py:131] throughput: 0.011575339256449978
(INFO) 2023-04-09 19:32:42,571 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 128, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:42,571 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:42,579 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:42,667 [api1.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:42,668 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:42,672 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:42,765 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:32:42,904 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:32:42,905 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:42,909 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:42,926 [api1.py:131] throughput: 0.008681806776445726
(INFO) 2023-04-09 19:32:43,017 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:32:43,409 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 16, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 20}
(INFO) 2023-04-09 19:32:43,410 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:43,415 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:43,431 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 37}
(INFO) 2023-04-09 19:32:43,432 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:43,443 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:43,722 [api1.py:131] throughput: 0.017361774840188735
(INFO) 2023-04-09 19:32:44,056 [api1.py:131] throughput: 0.01736179662366335
(INFO) 2023-04-09 19:32:45,345 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:45,346 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:45,349 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:45,435 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:45,534 [api1.py:116] Design point: {'core_buffer_size': 256, 'core_buffer_bw': 1024, 'core_mac_num': 64, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 13}
(INFO) 2023-04-09 19:32:45,535 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:45,539 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:45,694 [api1.py:131] throughput: 0.017361749311208867
(INFO) 2023-04-09 19:32:45,953 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 64, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 10}
(INFO) 2023-04-09 19:32:45,953 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:45,958 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:46,019 [api1.py:131] throughput: 0.01736172742928588
(INFO) 2023-04-09 19:32:46,952 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 1024, 'core_mac_num': 8, 'core_noc_bw': 128, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 32}
(INFO) 2023-04-09 19:32:46,953 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:46,956 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 48, 'core_array_w': 24, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 9}
(INFO) 2023-04-09 19:32:46,956 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 36}
(INFO) 2023-04-09 19:32:46,957 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:46,958 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:46,959 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:46,965 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:46,971 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:47,056 [api1.py:131] throughput: 0.017361716893564853
(INFO) 2023-04-09 19:32:47,171 [api1.py:131] throughput: 0.01736179261934406
(INFO) 2023-04-09 19:32:47,261 [api1.py:131] throughput: 0.011575339507337669
(INFO) 2023-04-09 19:32:47,815 [api1.py:116] Design point: {'core_buffer_size': 48, 'core_buffer_bw': 512, 'core_mac_num': 32, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 6, 'reticle_array_w': 8}
(INFO) 2023-04-09 19:32:47,816 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:47,820 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:47,895 [api1.py:131] throughput: 0.017361703723931553
(INFO) 2023-04-09 19:32:48,675 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 38}
(INFO) 2023-04-09 19:32:48,676 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:48,687 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:48,907 [api1.py:131] throughput: 0.011575339969499231
(INFO) 2023-04-09 19:32:50,173 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 256, 'core_mac_num': 8, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 64, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-09 19:32:50,174 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 512, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 1}
(INFO) 2023-04-09 19:32:50,184 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-09 19:32:50,399 [api1.py:131] throughput: 0.0115753389908042
