(DEBUG) 2023-04-10 17:17:15,590 [logger.py:40] logger init.
(INFO) 2023-04-10 17:17:15,590 [logger.py:42] Logfile /home/chenyiqi/wafer_scale_chip_dse_framework/log/DSE4WSE_2023-04-10-17-17-15-590390.log
(INFO) 2023-04-10 17:17:19,691 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:19,691 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 86, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:17:19,697 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:20,141 [api1.py:131] throughput: 1.9273652041922678
(INFO) 2023-04-10 17:17:20,142 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:20,143 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 73, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 517}
(INFO) 2023-04-10 17:17:20,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:20,595 [api1.py:131] throughput: 2.197573744148917
(INFO) 2023-04-10 17:17:20,596 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:20,596 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:20,601 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:21,031 [api1.py:131] throughput: 0.08717007046057175
(INFO) 2023-04-10 17:17:21,032 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:21,032 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 266}
(INFO) 2023-04-10 17:17:21,037 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:21,350 [api1.py:131] throughput: 7.031349813446976
(INFO) 2023-04-10 17:17:21,351 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:21,351 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 84, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 391}
(INFO) 2023-04-10 17:17:21,356 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:21,741 [api1.py:131] throughput: 2.854030908316717
(INFO) 2023-04-10 17:17:21,742 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:21,742 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 169, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 12}
(INFO) 2023-04-10 17:17:21,747 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:22,176 [api1.py:131] throughput: 0.23416697636154735
(INFO) 2023-04-10 17:17:22,177 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:22,177 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 70}
(INFO) 2023-04-10 17:17:22,182 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:22,590 [api1.py:131] throughput: 2.3293397195171286
(INFO) 2023-04-10 17:17:22,591 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:22,591 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:22,596 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:22,999 [api1.py:131] throughput: 0.3761667162797463
(INFO) 2023-04-10 17:17:23,000 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:23,000 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 89, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:17:23,112 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:23,489 [api1.py:131] throughput: 0.25163098380803123
(INFO) 2023-04-10 17:17:23,490 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:23,490 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 362, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:17:23,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:23,793 [api1.py:131] throughput: 1.4849153103845145
(INFO) 2023-04-10 17:17:23,795 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:23,795 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 265, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:17:23,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:24,141 [api1.py:131] throughput: 0.3149965380015694
(INFO) 2023-04-10 17:17:24,142 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:24,142 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 382, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 125}
(INFO) 2023-04-10 17:17:24,147 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:24,542 [api1.py:131] throughput: 1.5631762639950602
(INFO) 2023-04-10 17:17:24,544 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:24,544 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 112}
(INFO) 2023-04-10 17:17:24,548 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:24,914 [api1.py:131] throughput: 0.500646063432934
(INFO) 2023-04-10 17:17:24,915 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:24,915 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 17:17:24,920 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:25,261 [api1.py:131] throughput: 3.758727273699312
(INFO) 2023-04-10 17:17:25,262 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:25,262 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 65, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 105}
(INFO) 2023-04-10 17:17:25,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:25,782 [api1.py:131] throughput: 0.9298140023107915
(INFO) 2023-04-10 17:17:25,784 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:25,784 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 2, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:17:25,788 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:26,188 [api1.py:131] throughput: 0.10455389185389438
(INFO) 2023-04-10 17:17:26,189 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:26,189 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 229, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:17:26,194 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:26,603 [api1.py:131] throughput: 0.978792958139091
(INFO) 2023-04-10 17:17:26,604 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:26,604 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 121, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:26,609 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:27,166 [api1.py:131] throughput: 0.114502891636501
(INFO) 2023-04-10 17:17:27,168 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:27,168 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:27,173 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:27,619 [api1.py:131] throughput: 0.5735269727771541
(INFO) 2023-04-10 17:17:27,621 [api1.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:27,621 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:17:27,626 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,024 [api1.py:131] throughput: 0.23339289549900757
(INFO) 2023-04-10 17:17:28,029 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:17:28,029 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 277, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:17:28,031 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,111 [api1.py:131] throughput: 0.3501535581154874
(INFO) 2023-04-10 17:17:28,112 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 2048, 'core_mac_num': 128, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 9, 'reticle_array_w': 21}
(INFO) 2023-04-10 17:17:28,112 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 260, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:28,114 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,206 [api1.py:131] throughput: 0.04843002992341037
(INFO) 2023-04-10 17:17:28,217 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,217 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:17:28,221 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,221 [api2.py:131] throughput: 25.861375931600655
(INFO) 2023-04-10 17:17:28,222 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,222 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 73}
(INFO) 2023-04-10 17:17:28,226 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,226 [api2.py:131] throughput: 236.25833493514327
(INFO) 2023-04-10 17:17:28,226 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,227 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 438, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 68}
(INFO) 2023-04-10 17:17:28,230 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,231 [api2.py:131] throughput: 0.3841434552971858
(INFO) 2023-04-10 17:17:28,231 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,231 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 161, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 117}
(INFO) 2023-04-10 17:17:28,235 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,236 [api2.py:131] throughput: 0.9918729195903679
(INFO) 2023-04-10 17:17:28,236 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,236 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 491, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:17:28,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,241 [api2.py:131] throughput: 0.31725130050553685
(INFO) 2023-04-10 17:17:28,241 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,241 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 32}
(INFO) 2023-04-10 17:17:28,246 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,246 [api2.py:131] throughput: 4.044770384625768
(INFO) 2023-04-10 17:17:28,247 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,247 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 212, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:17:28,251 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,251 [api2.py:131] throughput: 0.7081414008461931
(INFO) 2023-04-10 17:17:28,252 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,252 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 94, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 166}
(INFO) 2023-04-10 17:17:28,256 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,257 [api2.py:131] throughput: 1.4798628989380924
(INFO) 2023-04-10 17:17:28,258 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,258 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 132}
(INFO) 2023-04-10 17:17:28,262 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,262 [api2.py:131] throughput: 36.362336764275646
(INFO) 2023-04-10 17:17:28,263 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,263 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:17:28,380 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,381 [api2.py:131] throughput: 0.3757900323927581
(INFO) 2023-04-10 17:17:28,382 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,382 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:28,386 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,386 [api2.py:131] throughput: 4.536594211674935
(INFO) 2023-04-10 17:17:28,387 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,387 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 122, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:17:28,391 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,391 [api2.py:131] throughput: 0.8181487420219745
(INFO) 2023-04-10 17:17:28,392 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,392 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 87, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:17:28,396 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,396 [api2.py:131] throughput: 1.0345763014423615
(INFO) 2023-04-10 17:17:28,397 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 64, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 32, 'wafer_mem_bw': 900, 'reticle_array_h': 36, 'reticle_array_w': 14}
(INFO) 2023-04-10 17:17:28,397 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 97, 'data_parallel_size': 3, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:28,401 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:28,401 [api2.py:131] throughput: 0.9356160008496719
(INFO) 2023-04-10 17:17:33,425 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,425 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 77}
(INFO) 2023-04-10 17:17:33,430 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,430 [api2.py:131] throughput: 1.4454086982936523
(INFO) 2023-04-10 17:17:33,431 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,431 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:33,436 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,436 [api2.py:131] throughput: 2.317692461775355
(INFO) 2023-04-10 17:17:33,436 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,436 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 387, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:33,441 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,441 [api2.py:131] throughput: 0.38812655616836506
(INFO) 2023-04-10 17:17:33,442 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,442 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:17:33,447 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,447 [api2.py:131] throughput: 2.002915774341365
(INFO) 2023-04-10 17:17:33,448 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,448 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 160, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:17:33,453 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,453 [api2.py:131] throughput: 0.9977735987801244
(INFO) 2023-04-10 17:17:33,454 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,454 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 118, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 63}
(INFO) 2023-04-10 17:17:33,459 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,459 [api2.py:131] throughput: 0.6034363969044735
(INFO) 2023-04-10 17:17:33,460 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,460 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 111, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:33,465 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,465 [api2.py:131] throughput: 1.3198544571058437
(INFO) 2023-04-10 17:17:33,466 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,466 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 30, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 68}
(INFO) 2023-04-10 17:17:33,471 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,471 [api2.py:131] throughput: 0.7851981207861263
(INFO) 2023-04-10 17:17:33,472 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,472 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 173, 'data_parallel_size': 1, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 104}
(INFO) 2023-04-10 17:17:33,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,477 [api2.py:131] throughput: 21.87579264410516
(INFO) 2023-04-10 17:17:33,478 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,478 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 95, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:17:33,483 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,483 [api2.py:131] throughput: 1.118698576548263
(INFO) 2023-04-10 17:17:33,484 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,484 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:33,489 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,489 [api2.py:131] throughput: 5.326974076506521
(INFO) 2023-04-10 17:17:33,490 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,490 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:17:33,495 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,495 [api2.py:131] throughput: 4.300990482618831
(INFO) 2023-04-10 17:17:33,496 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,496 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 5, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 76}
(INFO) 2023-04-10 17:17:33,500 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,501 [api2.py:131] throughput: 0.3765058119243269
(INFO) 2023-04-10 17:17:33,501 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,501 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 101, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:17:33,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,506 [api2.py:131] throughput: 1.4153552374326703
(INFO) 2023-04-10 17:17:33,507 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,507 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 84}
(INFO) 2023-04-10 17:17:33,512 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,512 [api2.py:131] throughput: 1.2229025443427277
(INFO) 2023-04-10 17:17:33,513 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,513 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 250, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 110}
(INFO) 2023-04-10 17:17:33,518 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,518 [api2.py:131] throughput: 18.004867483576717
(INFO) 2023-04-10 17:17:33,519 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,519 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 91, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:33,523 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,523 [api2.py:131] throughput: 1.6514764953319025
(INFO) 2023-04-10 17:17:33,524 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,524 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 82, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 36}
(INFO) 2023-04-10 17:17:33,529 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,529 [api2.py:131] throughput: 1.2421836147335807
(INFO) 2023-04-10 17:17:33,530 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,530 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 17:17:33,535 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,535 [api2.py:131] throughput: 1.7593940310237808
(INFO) 2023-04-10 17:17:33,536 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 512, 'core_mac_num': 8, 'core_noc_bw': 512, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 15, 'reticle_array_w': 42}
(INFO) 2023-04-10 17:17:33,536 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:17:33,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:33,541 [api2.py:131] throughput: 0.9669203518339811
(INFO) 2023-04-10 17:17:38,140 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,140 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 193, 'data_parallel_size': 2, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:17:38,145 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,145 [api2.py:131] throughput: 0.6269562846895966
(INFO) 2023-04-10 17:17:38,146 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,146 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 169, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:17:38,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,151 [api2.py:131] throughput: 0.8797925696856377
(INFO) 2023-04-10 17:17:38,152 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,152 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:17:38,157 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,157 [api2.py:131] throughput: 0.3795479928576257
(INFO) 2023-04-10 17:17:38,158 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,158 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:17:38,163 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,163 [api2.py:131] throughput: 2.183080638726674
(INFO) 2023-04-10 17:17:38,164 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,164 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:17:38,168 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,169 [api2.py:131] throughput: 1.0140386887026809
(INFO) 2023-04-10 17:17:38,169 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,169 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 5, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 26}
(INFO) 2023-04-10 17:17:38,174 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,175 [api2.py:131] throughput: 1.7479949101810937
(INFO) 2023-04-10 17:17:38,175 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,175 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 133, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:17:38,180 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,181 [api2.py:131] throughput: 0.5343493077333175
(INFO) 2023-04-10 17:17:38,181 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,181 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 163, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 48}
(INFO) 2023-04-10 17:17:38,186 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,186 [api2.py:131] throughput: 0.8627868257118708
(INFO) 2023-04-10 17:17:38,187 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,187 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:17:38,192 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,192 [api2.py:131] throughput: 0.7466059590412516
(INFO) 2023-04-10 17:17:38,193 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,193 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:17:38,198 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,198 [api2.py:131] throughput: 12.748072890859047
(INFO) 2023-04-10 17:17:38,199 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,199 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 9, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 104}
(INFO) 2023-04-10 17:17:38,204 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,204 [api2.py:131] throughput: 2.064878058623251
(INFO) 2023-04-10 17:17:38,205 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,205 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 331}
(INFO) 2023-04-10 17:17:38,210 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,210 [api2.py:131] throughput: 0.698990652959208
(INFO) 2023-04-10 17:17:38,211 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,211 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 140, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 330}
(INFO) 2023-04-10 17:17:38,216 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,216 [api2.py:131] throughput: 16.097546867527985
(INFO) 2023-04-10 17:17:38,217 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,217 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 46, 'data_parallel_size': 1, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 17:17:38,222 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,222 [api2.py:131] throughput: 0.3976026563178201
(INFO) 2023-04-10 17:17:38,223 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,223 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 116, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 65}
(INFO) 2023-04-10 17:17:38,227 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,228 [api2.py:131] throughput: 0.3656496859292507
(INFO) 2023-04-10 17:17:38,228 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,228 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 9, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:17:38,233 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,233 [api2.py:131] throughput: 0.44541175323582344
(INFO) 2023-04-10 17:17:38,234 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,234 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 4, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 84}
(INFO) 2023-04-10 17:17:38,239 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,239 [api2.py:131] throughput: 2.3382197146760704
(INFO) 2023-04-10 17:17:38,240 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,240 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 8, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:17:38,244 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,245 [api2.py:131] throughput: 5.351879918522123
(INFO) 2023-04-10 17:17:38,245 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,245 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 7, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 114}
(INFO) 2023-04-10 17:17:38,250 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,251 [api2.py:131] throughput: 4.227293210481613
(INFO) 2023-04-10 17:17:38,251 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 32, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 13, 'reticle_array_w': 53}
(INFO) 2023-04-10 17:17:38,251 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 203, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 348}
(INFO) 2023-04-10 17:17:38,256 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:38,256 [api2.py:131] throughput: 35.04787160398191
(INFO) 2023-04-10 17:17:46,831 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,832 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 233, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 42}
(INFO) 2023-04-10 17:17:46,834 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,834 [api2.py:131] throughput: 0.839245074621932
(INFO) 2023-04-10 17:17:46,835 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,835 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:17:46,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,838 [api2.py:131] throughput: 1.2822870593113425
(INFO) 2023-04-10 17:17:46,838 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,838 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 4, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:17:46,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,841 [api2.py:131] throughput: 4.075197245284549
(INFO) 2023-04-10 17:17:46,841 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,841 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 93}
(INFO) 2023-04-10 17:17:46,844 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,844 [api2.py:131] throughput: 0.5355543543721669
(INFO) 2023-04-10 17:17:46,845 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,845 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 6, 'model_parallel_size': 8, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 70}
(INFO) 2023-04-10 17:17:46,847 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,847 [api2.py:131] throughput: 4.2497788584101865
(INFO) 2023-04-10 17:17:46,848 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,848 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 71, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:46,850 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,851 [api2.py:131] throughput: 0.5752702454303374
(INFO) 2023-04-10 17:17:46,851 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,851 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 11, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:46,854 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,854 [api2.py:131] throughput: 5.134741398057746
(INFO) 2023-04-10 17:17:46,855 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,855 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 10, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 185}
(INFO) 2023-04-10 17:17:46,857 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,857 [api2.py:131] throughput: 62.39698736325367
(INFO) 2023-04-10 17:17:46,858 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,858 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:17:46,860 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,861 [api2.py:131] throughput: 2.2973083855728764
(INFO) 2023-04-10 17:17:46,861 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,861 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 8, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:17:46,864 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,864 [api2.py:131] throughput: 0.96635402348789
(INFO) 2023-04-10 17:17:46,864 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,864 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 54, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:17:46,867 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,867 [api2.py:131] throughput: 0.7123213093980848
(INFO) 2023-04-10 17:17:46,868 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,868 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 42, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:17:46,870 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,870 [api2.py:131] throughput: 0.6481710467348872
(INFO) 2023-04-10 17:17:46,871 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,871 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 317, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:46,874 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,874 [api2.py:131] throughput: 0.37741901214575296
(INFO) 2023-04-10 17:17:46,875 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,875 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 5, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:17:46,877 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,877 [api2.py:131] throughput: 0.6835944335944335
(INFO) 2023-04-10 17:17:46,878 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,878 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 68, 'data_parallel_size': 5, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 78}
(INFO) 2023-04-10 17:17:46,880 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,881 [api2.py:131] throughput: 1.4782044600775177
(INFO) 2023-04-10 17:17:46,881 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,881 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:46,883 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,884 [api2.py:131] throughput: 1.8981556278374296
(INFO) 2023-04-10 17:17:46,884 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,884 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:17:46,887 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,887 [api2.py:131] throughput: 0.7134104843663157
(INFO) 2023-04-10 17:17:46,888 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,888 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 495, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:46,890 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,890 [api2.py:131] throughput: 0.2594779791293683
(INFO) 2023-04-10 17:17:46,891 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,891 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:17:46,894 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,894 [api2.py:131] throughput: 0.701113020616827
(INFO) 2023-04-10 17:17:46,894 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 16, 'wafer_mem_bw': 900, 'reticle_array_h': 12, 'reticle_array_w': 25}
(INFO) 2023-04-10 17:17:46,895 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 60, 'data_parallel_size': 7, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 298}
(INFO) 2023-04-10 17:17:46,897 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:46,897 [api2.py:131] throughput: 73.56712311427505
(INFO) 2023-04-10 17:17:51,033 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,033 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 2, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 178}
(INFO) 2023-04-10 17:17:51,036 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,036 [api2.py:131] throughput: 23.96746345102048
(INFO) 2023-04-10 17:17:51,037 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,037 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:17:51,039 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,039 [api2.py:131] throughput: 0.7354120083143704
(INFO) 2023-04-10 17:17:51,040 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,040 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 111, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:17:51,042 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,043 [api2.py:131] throughput: 0.611092163515596
(INFO) 2023-04-10 17:17:51,043 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,043 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 20, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 17:17:51,045 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,046 [api2.py:131] throughput: 1.8445103321423535
(INFO) 2023-04-10 17:17:51,046 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,046 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 219, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:17:51,049 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,049 [api2.py:131] throughput: 0.6754684851572306
(INFO) 2023-04-10 17:17:51,049 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,050 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 14, 'model_parallel_size': 3, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 54}
(INFO) 2023-04-10 17:17:51,052 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,052 [api2.py:131] throughput: 1.3527654248093555
(INFO) 2023-04-10 17:17:51,052 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,053 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 488, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:17:51,055 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,055 [api2.py:131] throughput: 0.34420909731464155
(INFO) 2023-04-10 17:17:51,056 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,056 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:17:51,058 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,058 [api2.py:131] throughput: 1.370852906661433
(INFO) 2023-04-10 17:17:51,059 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,059 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 17, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 24}
(INFO) 2023-04-10 17:17:51,061 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,062 [api2.py:131] throughput: 0.765911601958879
(INFO) 2023-04-10 17:17:51,062 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,062 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 11, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:17:51,064 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,065 [api2.py:131] throughput: 0.35722712740817236
(INFO) 2023-04-10 17:17:51,065 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,065 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:17:51,068 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,068 [api2.py:131] throughput: 1.3872938600141658
(INFO) 2023-04-10 17:17:51,069 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,069 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 34, 'data_parallel_size': 10, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:17:51,071 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,071 [api2.py:131] throughput: 1.9317555708604794
(INFO) 2023-04-10 17:17:51,072 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,072 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 50, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:17:51,074 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,075 [api2.py:131] throughput: 2.055770111279888
(INFO) 2023-04-10 17:17:51,075 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,075 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 14, 'model_parallel_size': 24, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:17:51,077 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,078 [api2.py:131] throughput: 4.118804798129644
(INFO) 2023-04-10 17:17:51,078 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,078 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 22, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 38}
(INFO) 2023-04-10 17:17:51,080 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,081 [api2.py:131] throughput: 46.43047862260036
(INFO) 2023-04-10 17:17:51,081 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,081 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 14, 'model_parallel_size': 6, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:51,084 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,084 [api2.py:131] throughput: 1.178973279097145
(INFO) 2023-04-10 17:17:51,085 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,085 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 20, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 29}
(INFO) 2023-04-10 17:17:51,087 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,087 [api2.py:131] throughput: 2.7035747351384365
(INFO) 2023-04-10 17:17:51,088 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,088 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 55, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:17:51,091 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,091 [api2.py:131] throughput: 0.32217252273157143
(INFO) 2023-04-10 17:17:51,092 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,092 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 121, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:17:51,094 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,094 [api2.py:131] throughput: 0.45695410308813156
(INFO) 2023-04-10 17:17:51,095 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 256, 'core_mac_num': 4, 'core_noc_bw': 2048, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 8, 'reticle_array_w': 34}
(INFO) 2023-04-10 17:17:51,095 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 9, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:17:51,097 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:51,098 [api2.py:131] throughput: 2.317521922775851
(INFO) 2023-04-10 17:17:55,251 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:55,251 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 19, 'data_parallel_size': 16, 'model_parallel_size': 4, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:17:55,254 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:55,433 [api1.py:131] throughput: 1.2464110109174205
(INFO) 2023-04-10 17:17:55,434 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:55,434 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 17, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:55,438 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:55,619 [api1.py:131] throughput: 0.9213523008134831
(INFO) 2023-04-10 17:17:55,620 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:55,620 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 18, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:55,624 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:55,808 [api1.py:131] throughput: 0.4442746813512099
(INFO) 2023-04-10 17:17:55,809 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:55,809 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 16, 'model_parallel_size': 2, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 23}
(INFO) 2023-04-10 17:17:55,812 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:55,931 [api1.py:131] throughput: 1.676831545674296
(INFO) 2023-04-10 17:17:55,932 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:55,932 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 36, 'data_parallel_size': 14, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 113}
(INFO) 2023-04-10 17:17:55,934 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:56,085 [api1.py:131] throughput: 11.937775242532814
(INFO) 2023-04-10 17:17:56,086 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:56,086 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 19, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:17:56,089 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:56,255 [api1.py:131] throughput: 1.8364230439565143
(INFO) 2023-04-10 17:17:56,256 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:56,256 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:17:56,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:56,422 [api1.py:131] throughput: 0.8257002888448255
(INFO) 2023-04-10 17:17:56,423 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:56,424 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 10, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 28}
(INFO) 2023-04-10 17:17:56,428 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:56,572 [api1.py:131] throughput: 7.186058511155744
(INFO) 2023-04-10 17:17:56,573 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:56,573 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 11, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 281}
(INFO) 2023-04-10 17:17:56,577 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:56,694 [api1.py:131] throughput: 21.94531087421819
(INFO) 2023-04-10 17:17:56,695 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:56,695 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 474, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:17:56,699 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:56,829 [api1.py:131] throughput: 1.1448038356425059
(INFO) 2023-04-10 17:17:56,830 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:56,830 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 18, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 81}
(INFO) 2023-04-10 17:17:56,969 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:57,146 [api1.py:131] throughput: 1.3202666593759058
(INFO) 2023-04-10 17:17:57,147 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:57,147 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 101, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:17:57,151 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:57,353 [api1.py:131] throughput: 0.16318720622835478
(INFO) 2023-04-10 17:17:57,355 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:57,355 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 455, 'data_parallel_size': 1, 'model_parallel_size': 3, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:17:57,358 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:57,536 [api1.py:131] throughput: 0.4429858322141164
(INFO) 2023-04-10 17:17:57,537 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:57,537 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 80}
(INFO) 2023-04-10 17:17:57,541 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:57,678 [api1.py:131] throughput: 5.780876054617837
(INFO) 2023-04-10 17:17:57,679 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:57,679 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 19, 'model_parallel_size': 8, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 25}
(INFO) 2023-04-10 17:17:57,683 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:57,797 [api1.py:131] throughput: 10.30755128219511
(INFO) 2023-04-10 17:17:57,799 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:57,799 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 5, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 31}
(INFO) 2023-04-10 17:17:57,802 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:57,924 [api1.py:131] throughput: 9.637535035595283
(INFO) 2023-04-10 17:17:57,925 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:57,926 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 31, 'data_parallel_size': 7, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 342}
(INFO) 2023-04-10 17:17:57,929 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:58,038 [api1.py:131] throughput: 21.118876292633253
(INFO) 2023-04-10 17:17:58,039 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:58,039 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 41, 'data_parallel_size': 4, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:17:58,042 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:58,342 [api1.py:131] throughput: 0.3963913645251981
(INFO) 2023-04-10 17:17:58,343 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:58,343 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 77, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 33}
(INFO) 2023-04-10 17:17:58,347 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:58,535 [api1.py:131] throughput: 0.7387553086905824
(INFO) 2023-04-10 17:17:58,536 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 1024, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 24, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 10, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:17:58,536 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 18, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 151}
(INFO) 2023-04-10 17:17:58,540 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:17:58,669 [api1.py:131] throughput: 19.44422443574912
(INFO) 2023-04-10 17:18:02,877 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,877 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 15, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:18:02,879 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,879 [api2.py:131] throughput: 0.49078146606724155
(INFO) 2023-04-10 17:18:02,880 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,880 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 199}
(INFO) 2023-04-10 17:18:02,882 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,883 [api2.py:131] throughput: 31.168406705001996
(INFO) 2023-04-10 17:18:02,883 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,883 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 9, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:18:02,886 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,886 [api2.py:131] throughput: 2.5862852608971267
(INFO) 2023-04-10 17:18:02,887 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,887 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:18:02,889 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,889 [api2.py:131] throughput: 1.0452865809462388
(INFO) 2023-04-10 17:18:02,890 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,890 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 11, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:18:02,893 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,893 [api2.py:131] throughput: 0.3840867893500681
(INFO) 2023-04-10 17:18:02,894 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,894 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 14, 'data_parallel_size': 11, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 93}
(INFO) 2023-04-10 17:18:02,896 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,896 [api2.py:131] throughput: 1.9607900082551275
(INFO) 2023-04-10 17:18:02,897 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,897 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 99, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 52}
(INFO) 2023-04-10 17:18:02,900 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,900 [api2.py:131] throughput: 13.470683094538325
(INFO) 2023-04-10 17:18:02,901 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,901 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 18, 'model_parallel_size': 1, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 105}
(INFO) 2023-04-10 17:18:02,903 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,903 [api2.py:131] throughput: 7.409960461060714
(INFO) 2023-04-10 17:18:02,904 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,904 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 19, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:18:02,906 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,906 [api2.py:131] throughput: 1.0334189207340334
(INFO) 2023-04-10 17:18:02,907 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,907 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 19, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:18:02,910 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,910 [api2.py:131] throughput: 4.087997706011912
(INFO) 2023-04-10 17:18:02,911 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,911 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 17, 'model_parallel_size': 4, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 20}
(INFO) 2023-04-10 17:18:02,913 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,913 [api2.py:131] throughput: 1.3979824531965572
(INFO) 2023-04-10 17:18:02,914 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,914 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 18, 'model_parallel_size': 3, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 57}
(INFO) 2023-04-10 17:18:02,916 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,916 [api2.py:131] throughput: 11.993009015707194
(INFO) 2023-04-10 17:18:02,917 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,917 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 3, 'data_parallel_size': 18, 'model_parallel_size': 1, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:18:02,919 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,919 [api2.py:131] throughput: 0.4759275112051112
(INFO) 2023-04-10 17:18:02,920 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,920 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 162, 'data_parallel_size': 3, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 112}
(INFO) 2023-04-10 17:18:02,923 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,923 [api2.py:131] throughput: 14.364985552928815
(INFO) 2023-04-10 17:18:02,924 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,924 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 20, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:18:02,926 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,926 [api2.py:131] throughput: 0.7067566825406468
(INFO) 2023-04-10 17:18:02,927 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,927 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 40}
(INFO) 2023-04-10 17:18:02,929 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,930 [api2.py:131] throughput: 3.9303124960472293
(INFO) 2023-04-10 17:18:02,930 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,930 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 16, 'model_parallel_size': 4, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:18:02,932 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,933 [api2.py:131] throughput: 1.4018358352380784
(INFO) 2023-04-10 17:18:02,933 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,933 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 25, 'data_parallel_size': 20, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:18:02,936 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,936 [api2.py:131] throughput: 2.3444538720367816
(INFO) 2023-04-10 17:18:02,937 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,937 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 11, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 181}
(INFO) 2023-04-10 17:18:02,939 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,939 [api2.py:131] throughput: 24.201784957519994
(INFO) 2023-04-10 17:18:02,940 [api2.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 32, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:02,940 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 49, 'data_parallel_size': 10, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 13}
(INFO) 2023-04-10 17:18:02,942 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:02,943 [api2.py:131] throughput: 0.36795965108404355
(INFO) 2023-04-10 17:18:07,202 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,202 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 104}
(INFO) 2023-04-10 17:18:07,211 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,211 [api2.py:131] throughput: 0.4141199439942028
(INFO) 2023-04-10 17:18:07,212 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,212 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 5, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 17:18:07,221 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,221 [api2.py:131] throughput: 6.803326241612703
(INFO) 2023-04-10 17:18:07,222 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,222 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 70, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 34}
(INFO) 2023-04-10 17:18:07,231 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,231 [api2.py:131] throughput: 0.8345969843534569
(INFO) 2023-04-10 17:18:07,232 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,232 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 16, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:18:07,240 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,241 [api2.py:131] throughput: 0.7065022276467537
(INFO) 2023-04-10 17:18:07,241 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,241 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 79, 'data_parallel_size': 1, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 175}
(INFO) 2023-04-10 17:18:07,250 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,250 [api2.py:131] throughput: 0.6988494105989335
(INFO) 2023-04-10 17:18:07,251 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,251 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 27, 'data_parallel_size': 8, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 393}
(INFO) 2023-04-10 17:18:07,259 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,260 [api2.py:131] throughput: 0.5366998924280214
(INFO) 2023-04-10 17:18:07,260 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,260 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 15, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 482}
(INFO) 2023-04-10 17:18:07,269 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,269 [api2.py:131] throughput: 17.02178261132235
(INFO) 2023-04-10 17:18:07,270 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,270 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 124, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 146}
(INFO) 2023-04-10 17:18:07,278 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,279 [api2.py:131] throughput: 0.3403584325673897
(INFO) 2023-04-10 17:18:07,279 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,279 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 4, 'data_parallel_size': 6, 'model_parallel_size': 12, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 106}
(INFO) 2023-04-10 17:18:07,288 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,288 [api2.py:131] throughput: 5.556332125866428
(INFO) 2023-04-10 17:18:07,289 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,289 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 16, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 140}
(INFO) 2023-04-10 17:18:07,298 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,298 [api2.py:131] throughput: 2.0437780797396528
(INFO) 2023-04-10 17:18:07,299 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,299 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 33, 'data_parallel_size': 14, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 583}
(INFO) 2023-04-10 17:18:07,307 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,307 [api2.py:131] throughput: 3.834816127026221
(INFO) 2023-04-10 17:18:07,308 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,308 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 45, 'data_parallel_size': 7, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 69}
(INFO) 2023-04-10 17:18:07,316 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,316 [api2.py:131] throughput: 2.4726105752753993
(INFO) 2023-04-10 17:18:07,317 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,317 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 4, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:18:07,325 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,325 [api2.py:131] throughput: 3.438445004373157
(INFO) 2023-04-10 17:18:07,326 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,326 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 43, 'data_parallel_size': 4, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:18:07,334 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,334 [api2.py:131] throughput: 1.0371271601272705
(INFO) 2023-04-10 17:18:07,335 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,335 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 24, 'data_parallel_size': 7, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 21}
(INFO) 2023-04-10 17:18:07,343 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,343 [api2.py:131] throughput: 4.218926169682848
(INFO) 2023-04-10 17:18:07,344 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,344 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 38, 'data_parallel_size': 3, 'model_parallel_size': 12, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 444}
(INFO) 2023-04-10 17:18:07,353 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,353 [api2.py:131] throughput: 4.546802368452652
(INFO) 2023-04-10 17:18:07,354 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,354 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 39}
(INFO) 2023-04-10 17:18:07,506 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,507 [api2.py:131] throughput: 5.922895716034359
(INFO) 2023-04-10 17:18:07,508 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,508 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 9, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 107}
(INFO) 2023-04-10 17:18:07,516 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,516 [api2.py:131] throughput: 2.4940412155311193
(INFO) 2023-04-10 17:18:07,517 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,517 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 13, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 486}
(INFO) 2023-04-10 17:18:07,525 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,525 [api2.py:131] throughput: 28.74668519123162
(INFO) 2023-04-10 17:18:07,526 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 35, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:07,526 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 13, 'model_parallel_size': 2, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 290}
(INFO) 2023-04-10 17:18:07,534 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:07,534 [api2.py:131] throughput: 0.9206027444277777
(INFO) 2023-04-10 17:18:11,556 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:11,556 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 4, 'model_parallel_size': 4, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 144}
(INFO) 2023-04-10 17:18:11,563 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:12,170 [api1.py:131] throughput: 4.771708783780757
(INFO) 2023-04-10 17:18:12,171 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:12,171 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 8, 'data_parallel_size': 20, 'model_parallel_size': 24, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 14}
(INFO) 2023-04-10 17:18:12,296 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:13,041 [api1.py:131] throughput: 1.1664882019211738
(INFO) 2023-04-10 17:18:13,042 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:13,042 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 15, 'model_parallel_size': 1, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 197}
(INFO) 2023-04-10 17:18:13,048 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:13,655 [api1.py:131] throughput: 0.9365347290682202
(INFO) 2023-04-10 17:18:13,656 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:13,656 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 9, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 8}
(INFO) 2023-04-10 17:18:13,663 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:14,530 [api1.py:131] throughput: 0.32004708143484817
(INFO) 2023-04-10 17:18:14,531 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:14,531 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 40, 'data_parallel_size': 12, 'model_parallel_size': 2, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 19}
(INFO) 2023-04-10 17:18:14,538 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:15,258 [api1.py:131] throughput: 0.6648132837755585
(INFO) 2023-04-10 17:18:15,260 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:15,260 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 20, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 80}
(INFO) 2023-04-10 17:18:15,267 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:16,000 [api1.py:131] throughput: 8.482883085190446
(INFO) 2023-04-10 17:18:16,001 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:16,001 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 16, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:18:16,008 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:16,644 [api1.py:131] throughput: 0.8119391340978755
(INFO) 2023-04-10 17:18:16,645 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:16,645 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 6, 'data_parallel_size': 17, 'model_parallel_size': 24, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:18:16,653 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:17,430 [api1.py:131] throughput: 0.7469516405581065
(INFO) 2023-04-10 17:18:17,431 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:17,431 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 157, 'data_parallel_size': 3, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:18:17,438 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:18,311 [api1.py:131] throughput: 0.132587249113528
(INFO) 2023-04-10 17:18:18,313 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:18,313 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 2, 'data_parallel_size': 20, 'model_parallel_size': 12, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:18:18,320 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:18,790 [api1.py:131] throughput: 10.543851968566281
(INFO) 2023-04-10 17:18:18,791 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:18,791 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 13, 'model_parallel_size': 1, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 139}
(INFO) 2023-04-10 17:18:18,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:19,611 [api1.py:131] throughput: 1.4142071746977707
(INFO) 2023-04-10 17:18:19,612 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:19,612 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 20, 'model_parallel_size': 24, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:18:19,619 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:20,385 [api1.py:131] throughput: 0.5864374949792261
(INFO) 2023-04-10 17:18:20,386 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:20,387 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 100, 'data_parallel_size': 5, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 108}
(INFO) 2023-04-10 17:18:20,394 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:20,931 [api1.py:131] throughput: 5.960058937866145
(INFO) 2023-04-10 17:18:20,932 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:20,932 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:18:20,940 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:21,755 [api1.py:131] throughput: 0.632585811609238
(INFO) 2023-04-10 17:18:21,756 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:21,756 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 13, 'data_parallel_size': 21, 'model_parallel_size': 12, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 7}
(INFO) 2023-04-10 17:18:21,763 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:22,575 [api1.py:131] throughput: 0.8158946453476987
(INFO) 2023-04-10 17:18:22,576 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:22,576 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 5, 'data_parallel_size': 20, 'model_parallel_size': 2, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 69}
(INFO) 2023-04-10 17:18:22,585 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:23,348 [api1.py:131] throughput: 2.042746327385207
(INFO) 2023-04-10 17:18:23,350 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:23,350 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 3, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 774}
(INFO) 2023-04-10 17:18:23,498 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:24,048 [api1.py:131] throughput: 4.493816446144375
(INFO) 2023-04-10 17:18:24,049 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:24,049 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 16, 'model_parallel_size': 8, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 142}
(INFO) 2023-04-10 17:18:24,057 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:24,468 [api1.py:131] throughput: 11.757431138858472
(INFO) 2023-04-10 17:18:24,469 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:24,469 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 19, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 137}
(INFO) 2023-04-10 17:18:24,477 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:25,076 [api1.py:131] throughput: 14.637124534113122
(INFO) 2023-04-10 17:18:25,077 [api1.py:116] Design point: {'core_buffer_size': 128, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 8, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 27, 'reticle_array_w': 36}
(INFO) 2023-04-10 17:18:25,077 [api1.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 21, 'data_parallel_size': 12, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 111}
(INFO) 2023-04-10 17:18:25,085 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:25,674 [api1.py:131] throughput: 10.046625339489768
(INFO) 2023-04-10 17:18:30,192 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,192 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 1, 'data_parallel_size': 9, 'model_parallel_size': 6, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 83}
(INFO) 2023-04-10 17:18:30,196 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,197 [api2.py:131] throughput: 2.7985520442748903
(INFO) 2023-04-10 17:18:30,197 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,197 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 15, 'data_parallel_size': 15, 'model_parallel_size': 4, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 127}
(INFO) 2023-04-10 17:18:30,202 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,202 [api2.py:131] throughput: 1.7159718589178268
(INFO) 2023-04-10 17:18:30,203 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,203 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 9, 'data_parallel_size': 11, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 49}
(INFO) 2023-04-10 17:18:30,207 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,207 [api2.py:131] throughput: 0.8310313975992539
(INFO) 2023-04-10 17:18:30,208 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,208 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 81, 'data_parallel_size': 6, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:18:30,213 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,213 [api2.py:131] throughput: 0.3529638893127734
(INFO) 2023-04-10 17:18:30,214 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,214 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 16, 'data_parallel_size': 11, 'model_parallel_size': 24, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:18:30,218 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,219 [api2.py:131] throughput: 2.5156927853685827
(INFO) 2023-04-10 17:18:30,219 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,219 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 15, 'model_parallel_size': 12, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:18:30,335 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,335 [api2.py:131] throughput: 2.694389212120082
(INFO) 2023-04-10 17:18:30,336 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,336 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 9, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:18:30,341 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,341 [api2.py:131] throughput: 2.1383929608367707
(INFO) 2023-04-10 17:18:30,341 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,342 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 1, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 121}
(INFO) 2023-04-10 17:18:30,346 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,346 [api2.py:131] throughput: 2.8954225071939805
(INFO) 2023-04-10 17:18:30,347 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,347 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 44}
(INFO) 2023-04-10 17:18:30,352 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,352 [api2.py:131] throughput: 1.7317936653010166
(INFO) 2023-04-10 17:18:30,352 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,352 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:18:30,357 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,357 [api2.py:131] throughput: 1.1464107276992213
(INFO) 2023-04-10 17:18:30,358 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,358 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 479, 'data_parallel_size': 1, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 17:18:30,362 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,363 [api2.py:131] throughput: 0.36035628707452283
(INFO) 2023-04-10 17:18:30,363 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,363 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 22, 'data_parallel_size': 13, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:18:30,368 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,368 [api2.py:131] throughput: 3.4586004610288077
(INFO) 2023-04-10 17:18:30,369 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,369 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 8, 'model_parallel_size': 3, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 46}
(INFO) 2023-04-10 17:18:30,373 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,373 [api2.py:131] throughput: 1.7089588211772975
(INFO) 2023-04-10 17:18:30,374 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,374 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 7, 'data_parallel_size': 12, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 35}
(INFO) 2023-04-10 17:18:30,379 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,379 [api2.py:131] throughput: 0.47530308009985367
(INFO) 2023-04-10 17:18:30,380 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,380 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 28, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 9}
(INFO) 2023-04-10 17:18:30,384 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,385 [api2.py:131] throughput: 0.9378715553188742
(INFO) 2023-04-10 17:18:30,385 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,385 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 10, 'data_parallel_size': 14, 'model_parallel_size': 2, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 78}
(INFO) 2023-04-10 17:18:30,390 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,391 [api2.py:131] throughput: 5.362996509900822
(INFO) 2023-04-10 17:18:30,391 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,391 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 37, 'data_parallel_size': 6, 'model_parallel_size': 4, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 130}
(INFO) 2023-04-10 17:18:30,397 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,397 [api2.py:131] throughput: 15.058173519348173
(INFO) 2023-04-10 17:18:30,397 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,398 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 18, 'data_parallel_size': 4, 'model_parallel_size': 2, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 106}
(INFO) 2023-04-10 17:18:30,403 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,403 [api2.py:131] throughput: 1.2713688600949429
(INFO) 2023-04-10 17:18:30,404 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,404 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 29, 'data_parallel_size': 13, 'model_parallel_size': 8, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:18:30,408 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,409 [api2.py:131] throughput: 2.1747213988681224
(INFO) 2023-04-10 17:18:30,409 [api2.py:116] Design point: {'core_buffer_size': 64, 'core_buffer_bw': 512, 'core_mac_num': 4, 'core_noc_bw': 1024, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 16, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 19, 'reticle_array_w': 35}
(INFO) 2023-04-10 17:18:30,409 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 11, 'data_parallel_size': 14, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 50}
(INFO) 2023-04-10 17:18:30,414 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:30,415 [api2.py:131] throughput: 0.8222535040742961
(INFO) 2023-04-10 17:18:34,775 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,775 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 48, 'data_parallel_size': 6, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:18:34,778 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,779 [api2.py:131] throughput: 2.56528091696497
(INFO) 2023-04-10 17:18:34,779 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,779 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 90, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 1, 'num_reticle_per_pipeline_stage': 30}
(INFO) 2023-04-10 17:18:34,783 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,783 [api2.py:131] throughput: 13.060836810650523
(INFO) 2023-04-10 17:18:34,784 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,784 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 44, 'data_parallel_size': 5, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:18:34,787 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,787 [api2.py:131] throughput: 1.0394071552169593
(INFO) 2023-04-10 17:18:34,788 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,788 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 35, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 90}
(INFO) 2023-04-10 17:18:34,791 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,791 [api2.py:131] throughput: 0.5295200440971221
(INFO) 2023-04-10 17:18:34,792 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,792 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 2, 'model_parallel_size': 1, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 16}
(INFO) 2023-04-10 17:18:34,795 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,795 [api2.py:131] throughput: 0.3886704834856367
(INFO) 2023-04-10 17:18:34,796 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,796 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 493, 'data_parallel_size': 1, 'model_parallel_size': 24, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 1}
(INFO) 2023-04-10 17:18:34,799 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,800 [api2.py:131] throughput: 0.31537120189559076
(INFO) 2023-04-10 17:18:34,800 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,800 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 66, 'data_parallel_size': 6, 'model_parallel_size': 2, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 58}
(INFO) 2023-04-10 17:18:34,803 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,804 [api2.py:131] throughput: 0.7534192081134632
(INFO) 2023-04-10 17:18:34,804 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,804 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 75, 'data_parallel_size': 3, 'model_parallel_size': 1, 'tensor_parallel_size': 8, 'num_reticle_per_pipeline_stage': 3}
(INFO) 2023-04-10 17:18:34,807 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,807 [api2.py:131] throughput: 0.3594647470498748
(INFO) 2023-04-10 17:18:34,808 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,808 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 413, 'data_parallel_size': 1, 'model_parallel_size': 8, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 15}
(INFO) 2023-04-10 17:18:34,811 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,811 [api2.py:131] throughput: 0.4344591253482474
(INFO) 2023-04-10 17:18:34,812 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,812 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 185, 'data_parallel_size': 2, 'model_parallel_size': 12, 'tensor_parallel_size': 4, 'num_reticle_per_pipeline_stage': 4}
(INFO) 2023-04-10 17:18:34,815 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,816 [api2.py:131] throughput: 0.933682380822604
(INFO) 2023-04-10 17:18:34,816 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,816 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 56, 'data_parallel_size': 6, 'model_parallel_size': 6, 'tensor_parallel_size': 12, 'num_reticle_per_pipeline_stage': 10}
(INFO) 2023-04-10 17:18:34,820 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,820 [api2.py:131] throughput: 1.4293290079496177
(INFO) 2023-04-10 17:18:34,821 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,821 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 32, 'data_parallel_size': 2, 'model_parallel_size': 24, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 2}
(INFO) 2023-04-10 17:18:34,824 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,824 [api2.py:131] throughput: 3.354490057398814
(INFO) 2023-04-10 17:18:34,825 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,825 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 126, 'data_parallel_size': 2, 'model_parallel_size': 8, 'tensor_parallel_size': 2, 'num_reticle_per_pipeline_stage': 177}
(INFO) 2023-04-10 17:18:34,828 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,828 [api2.py:131] throughput: 1.9601531409522177
(INFO) 2023-04-10 17:18:34,829 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,829 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 110, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 17}
(INFO) 2023-04-10 17:18:34,832 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,833 [api2.py:131] throughput: 1.4178575474563822
(INFO) 2023-04-10 17:18:34,833 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,833 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 17, 'data_parallel_size': 4, 'model_parallel_size': 3, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 6}
(INFO) 2023-04-10 17:18:34,837 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,837 [api2.py:131] throughput: 1.029081218975612
(INFO) 2023-04-10 17:18:34,837 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,838 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 63, 'data_parallel_size': 8, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 60}
(INFO) 2023-04-10 17:18:34,841 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,841 [api2.py:131] throughput: 1.8757485501505549
(INFO) 2023-04-10 17:18:34,842 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,842 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 197, 'data_parallel_size': 2, 'model_parallel_size': 6, 'tensor_parallel_size': 3, 'num_reticle_per_pipeline_stage': 93}
(INFO) 2023-04-10 17:18:34,845 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,845 [api2.py:131] throughput: 1.0587875367569084
(INFO) 2023-04-10 17:18:34,846 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,846 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 52, 'data_parallel_size': 7, 'model_parallel_size': 8, 'tensor_parallel_size': 24, 'num_reticle_per_pipeline_stage': 11}
(INFO) 2023-04-10 17:18:34,849 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,849 [api2.py:131] throughput: 1.7581206643033485
(INFO) 2023-04-10 17:18:34,850 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,850 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 85, 'data_parallel_size': 3, 'model_parallel_size': 3, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 18}
(INFO) 2023-04-10 17:18:34,853 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,853 [api2.py:131] throughput: 0.8582593021419265
(INFO) 2023-04-10 17:18:34,854 [api2.py:116] Design point: {'core_buffer_size': 32, 'core_buffer_bw': 128, 'core_mac_num': 4, 'core_noc_bw': 256, 'core_noc_vc': 4, 'core_noc_buffer_size': 4, 'reticle_bw': 1, 'core_array_h': 56, 'core_array_w': 8, 'wafer_mem_bw': 900, 'reticle_array_h': 7, 'reticle_array_w': 54}
(INFO) 2023-04-10 17:18:34,854 [api2.py:117] Model parameters: {'attention_heads': 24, 'hidden_size': 2304, 'sequence_length': 2048, 'number_of_layers': 24, 'mini_batch_size': 512, 'micro_batch_size': 59, 'data_parallel_size': 8, 'model_parallel_size': 12, 'tensor_parallel_size': 6, 'num_reticle_per_pipeline_stage': 5}
(INFO) 2023-04-10 17:18:34,858 [wse_attn.py:412] Calculating training throughput of attention module
(INFO) 2023-04-10 17:18:34,858 [api2.py:131] throughput: 2.0554446428055972
